"""
Veri Analizi ve Ã–n Ä°ÅŸleme Rehberi
Deep Learning iÃ§in veri hazÄ±rlama sÃ¼reÃ§lerini Ã¶ÄŸretir
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
import os

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

class DataAnalysisGuide:
    def __init__(self, dataset_path=None):
        self.dataset_path = dataset_path
        self.data = None
        self.analysis_results = {}
        
    def load_and_explore_data(self, file_path):
        """Veriyi yÃ¼kle ve temel keÅŸif yap"""
        print("ğŸ“‚ VERÄ° YÃœKLENÄ°YOR VE KEÅF EDÄ°LÄ°YOR...")
        print("="*60)
        
        try:
            self.data = pd.read_csv(file_path)
            print(f"âœ… Veri baÅŸarÄ±yla yÃ¼klendi: {file_path}")
        except Exception as e:
            print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
            return False
            
        # Temel bilgiler
        print(f"\nğŸ“Š VERÄ° SETÄ° GENEL BÄ°LGÄ°LERÄ°:")
        print("-"*40)
        print(f"ğŸ”¢ SatÄ±r sayÄ±sÄ±: {len(self.data):,}")
        print(f"ğŸ”¢ SÃ¼tun sayÄ±sÄ±: {len(self.data.columns)}")
        print(f"ğŸ’¾ HafÄ±za kullanÄ±mÄ±: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # SÃ¼tun tÃ¼rleri
        print(f"\nğŸ“‹ SÃœTUN TÃœRLERÄ°:")
        print("-"*40)
        dtype_counts = self.data.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} sÃ¼tun")
            
        # Ä°lk 5 satÄ±r
        print(f"\nğŸ‘€ VERÄ°NÄ°N Ä°LK 5 SATIRI:")
        print("-"*40)
        print(self.data.head())
        
        # Eksik deÄŸerler
        missing_data = self.data.isnull().sum()
        if missing_data.any():
            print(f"\nâš ï¸  EKSÄ°K DEÄERLER:")
            print("-"*40)
            for col, missing_count in missing_data[missing_data > 0].items():
                missing_percent = (missing_count / len(self.data)) * 100
                print(f"  {col}: {missing_count} ({missing_percent:.1f}%)")
        else:
            print(f"\nâœ… Eksik deÄŸer bulunmuyor!")
            
        return True
    
    def analyze_data_quality(self):
        """Veri kalitesini analiz et"""
        if self.data is None:
            print("âŒ Ã–nce veri yÃ¼klemelisiniz!")
            return
            
        print("\nğŸ” VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°")
        print("="*60)
        
        quality_issues = []
        
        # 1. Duplicate kontrolÃ¼
        duplicates = self.data.duplicated().sum()
        if duplicates > 0:
            quality_issues.append(f"Tekrarlanan satÄ±rlar: {duplicates}")
            print(f"âš ï¸  {duplicates} tekrarlanan satÄ±r bulundu")
        else:
            print("âœ… Tekrarlanan satÄ±r yok")
            
        # 2. AykÄ±rÄ± deÄŸer analizi (sayÄ±sal sÃ¼tunlar iÃ§in)
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        outlier_cols = []
        
        if len(numeric_cols) > 0:
            print(f"\nğŸ“ˆ AYKIRI DEÄER ANALÄ°ZÄ° (SayÄ±sal SÃ¼tunlar):")
            print("-"*50)
            
            for col in numeric_cols:
                Q1 = self.data[col].quantile(0.25)
                Q3 = self.data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = ((self.data[col] < lower_bound) | (self.data[col] > upper_bound)).sum()
                outlier_percent = (outliers / len(self.data)) * 100
                
                print(f"  {col}: {outliers} aykÄ±rÄ± deÄŸer (%{outlier_percent:.1f})")
                if outlier_percent > 5:  # %5'ten fazla aykÄ±rÄ± deÄŸer
                    outlier_cols.append(col)
                    quality_issues.append(f"{col} sÃ¼tununda Ã§ok aykÄ±rÄ± deÄŸer")
        
        # 3. Kategorik sÃ¼tun analizi
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            print(f"\nğŸ“Š KATEGORÄ°K SÃœTUN ANALÄ°ZÄ°:")
            print("-"*50)
            
            for col in categorical_cols:
                unique_count = self.data[col].nunique()
                unique_percent = (unique_count / len(self.data)) * 100
                
                print(f"  {col}: {unique_count} benzersiz deÄŸer (%{unique_percent:.1f})")
                
                if unique_percent > 50:
                    quality_issues.append(f"{col} sÃ¼tununda Ã§ok fazla kategori")
                elif unique_count == 1:
                    quality_issues.append(f"{col} sÃ¼tunu sabit (tek deÄŸer)")
        
        # SonuÃ§
        if quality_issues:
            print(f"\nâš ï¸  VERÄ° KALÄ°TESÄ° UYARILARI:")
            print("-"*50)
            for i, issue in enumerate(quality_issues, 1):
                print(f"  {i}. {issue}")
        else:
            print(f"\nâœ… Veri kalitesi genel olarak iyi gÃ¶rÃ¼nÃ¼yor!")
            
        return quality_issues
    
    def create_data_visualization_guide(self):
        """Veri gÃ¶rselleÅŸtirme rehberi"""
        if self.data is None:
            print("âŒ Ã–nce veri yÃ¼klemelisiniz!")
            return
            
        print("\nğŸ“Š VERÄ° GÃ–RSELLEÅTÄ°RME REHBERÄ°")
        print("="*60)
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        
        if len(numeric_cols) == 0 and len(categorical_cols) == 0:
            print("âŒ GÃ¶rselleÅŸtirilebilecek sÃ¼tun bulunamadÄ±.")
            return
            
        # Subplot sayÄ±sÄ±nÄ± hesapla
        total_plots = min(len(numeric_cols), 4) + min(len(categorical_cols), 2)
        if total_plots == 0:
            return
            
        rows = (total_plots + 2) // 3  # 3 sÃ¼tunlu layout
        cols = min(total_plots, 3)
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))
        if total_plots == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.flatten()
        else:
            axes = axes.flatten()
        
        plot_idx = 0
        
        # SayÄ±sal sÃ¼tunlar iÃ§in histogram
        for i, col in enumerate(numeric_cols[:4]):  # Ä°lk 4 sayÄ±sal sÃ¼tun
            axes[plot_idx].hist(self.data[col].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')
            axes[plot_idx].set_title(f' {col} DaÄŸÄ±lÄ±mÄ±')
            axes[plot_idx].set_xlabel(col)
            axes[plot_idx].set_ylabel('Frekans')
            
            # Ä°statistikler ekle
            mean_val = self.data[col].mean()
            median_val = self.data[col].median()
            axes[plot_idx].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Ortalama: {mean_val:.2f}')
            axes[plot_idx].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Medyan: {median_val:.2f}')
            axes[plot_idx].legend()
            
            plot_idx += 1
        
        # Kategorik sÃ¼tunlar iÃ§in bar chart
        for i, col in enumerate(categorical_cols[:2]):  # Ä°lk 2 kategorik sÃ¼tun
            if plot_idx >= total_plots:
                break
                
            value_counts = self.data[col].value_counts().head(10)  # En Ã§ok 10 kategori
            axes[plot_idx].bar(range(len(value_counts)), value_counts.values, color='lightcoral')
            axes[plot_idx].set_title(f' {col} Kategorileri')
            axes[plot_idx].set_xlabel(col)
            axes[plot_idx].set_ylabel('SayÄ±')
            axes[plot_idx].set_xticks(range(len(value_counts)))
            axes[plot_idx].set_xticklabels(value_counts.index, rotation=45, ha='right')
            
            plot_idx += 1
        
        # KullanÄ±lmayan subplot'larÄ± gizle
        for i in range(plot_idx, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.show()
        
        # Korelasyon analizi (sayÄ±sal sÃ¼tunlar iÃ§in)
        if len(numeric_cols) > 1:
            print("\nğŸ”— KORELASYON ANALÄ°ZÄ°:")
            print("-"*50)
            
            plt.figure(figsize=(10, 8))
            correlation_matrix = self.data[numeric_cols].corr()
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, fmt='.2f', cbar_kws={'label': 'Korelasyon KatsayÄ±sÄ±'})
            plt.title(' SÃ¼tunlar ArasÄ± Korelasyon Matrisi')
            plt.tight_layout()
            plt.show()
            
            # YÃ¼ksek korelasyonlarÄ± bul
            high_corr_pairs = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_val = abs(correlation_matrix.iloc[i, j])
                    if corr_val > 0.8:  # YÃ¼ksek korelasyon eÅŸiÄŸi
                        high_corr_pairs.append((
                            correlation_matrix.columns[i], 
                            correlation_matrix.columns[j], 
                            correlation_matrix.iloc[i, j]
                        ))
            
            if high_corr_pairs:
                print("âš ï¸  YÃœKSEK KORELASYON UYARISI:")
                for col1, col2, corr in high_corr_pairs:
                    print(f"   {col1} â†” {col2}: {corr:.3f}")
                print("   Not: YÃ¼ksek korelasyonlu sÃ¼tunlardan birini Ã§Ä±karmayÄ± dÃ¼ÅŸÃ¼nÃ¼n.")
    
    def preprocessing_recommendations(self):
        """Ã–n iÅŸleme Ã¶nerilerini ver"""
        if self.data is None:
            print("âŒ Ã–nce veri yÃ¼klemelisiniz!")
            return
            
        print("\nğŸ”§ VERÄ° Ã–N Ä°ÅLEME Ã–NERÄ°LERÄ°")
        print("="*60)
        
        recommendations = []
        
        # 1. Eksik deÄŸer Ã¶nerileri
        missing_data = self.data.isnull().sum()
        if missing_data.any():
            print("ğŸ“‹ EKSÄ°K DEÄER Ã–NERÄ°LERÄ°:")
            for col, missing_count in missing_data[missing_data > 0].items():
                missing_percent = (missing_count / len(self.data)) * 100
                
                if missing_percent < 5:
                    recommendation = f"   {col}: Az eksik (%{missing_percent:.1f}) â†’ Silme veya ortalama ile doldurma"
                elif missing_percent < 30:
                    if self.data[col].dtype in ['int64', 'float64']:
                        recommendation = f"   {col}: Orta eksik (%{missing_percent:.1f}) â†’ Medyan ile doldurma Ã¶nerilir"
                    else:
                        recommendation = f"   {col}: Orta eksik (%{missing_percent:.1f}) â†’ Mod ile doldurma Ã¶nerilir"
                else:
                    recommendation = f"   {col}: Ã‡ok eksik (%{missing_percent:.1f}) â†’ SÃ¼tunu Ã§Ä±karmayÄ± dÃ¼ÅŸÃ¼nÃ¼n"
                    
                print(recommendation)
                recommendations.append(recommendation)
        
        # 2. Normalizasyon Ã¶nerileri
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\nğŸ“ NORMALÄ°ZASYON Ã–NERÄ°LERÄ°:")
            
            for col in numeric_cols:
                col_min = self.data[col].min()
                col_max = self.data[col].max()
                col_range = col_max - col_min
                
                if col_range > 1000:
                    recommendation = f"   {col}: GeniÅŸ aralÄ±k ({col_min:.1f} - {col_max:.1f}) â†’ StandardScaler Ã¶nerilir"
                    print(recommendation)
                    recommendations.append(recommendation)
                elif col_min >= 0 and col_max <= 1:
                    print(f"   {col}: Zaten normalize ({col_min:.3f} - {col_max:.3f})")
                else:
                    recommendation = f"   {col}: Normal aralÄ±k â†’ MinMaxScaler kullanabilirsiniz"
                    print(recommendation)
        
        # 3. Kategorik deÄŸiÅŸken Ã¶nerileri  
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            print(f"\nğŸ·ï¸  KATEGORÄ°K DEÄÄ°ÅKEN Ã–NERÄ°LERÄ°:")
            
            for col in categorical_cols:
                unique_count = self.data[col].nunique()
                
                if unique_count == 2:
                    recommendation = f"   {col}: Ä°kili kategori â†’ LabelEncoder kullanÄ±n"
                    print(recommendation)
                elif unique_count <= 10:
                    recommendation = f"   {col}: Az kategori ({unique_count}) â†’ OneHotEncoder kullanÄ±n"
                    print(recommendation)
                else:
                    recommendation = f"   {col}: Ã‡ok kategori ({unique_count}) â†’ Target Encoding veya Embedding dÃ¼ÅŸÃ¼nÃ¼n"
                    print(recommendation)
                    
                recommendations.append(recommendation)
        
        # 4. Ã–rnek kod Ã¼retimi
        self.generate_preprocessing_code(recommendations)
        
        return recommendations
    
    def generate_preprocessing_code(self, recommendations):
        """Ã–neriler temelinde Ã¶rnek kod Ã¼ret"""
        print(f"\nğŸ’» Ã–NERÄ°LER DOÄRULTUSUNDA Ã–RNEK KOD:")
        print("="*60)
        
        # Hedef sÃ¼tunu belirleme (son sÃ¼tun varsayÄ±lan)
        target_column = self.data.columns[-1]
        
        code_lines = [
            "import pandas as pd",
            "import numpy as np", 
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder",
            "from sklearn.model_selection import train_test_split",
            "import os",
            "",
            "# Veriyi yÃ¼kle",
            f"data = pd.read_csv(r'{self.dataset_path if self.dataset_path else 'your_dataset.csv'}')",
            "",
            "# Veri setinin temel bilgilerini gÃ¶ster",
            "print('ğŸ“Š Veri Seti Bilgileri:')",
            "print(f'SatÄ±r sayÄ±sÄ±: {len(data)}')",
            "print(f'SÃ¼tun sayÄ±sÄ±: {len(data.columns)}')",
            "print(f'SÃ¼tunlar: {list(data.columns)}')",
            "print('\\nÄ°lk 5 satÄ±r:')",
            "print(data.head())",
            ""
        ]
        
        # Eksik deÄŸer doldurma
        missing_data = self.data.isnull().sum()
        if missing_data.any():
            code_lines.extend([
                "# Eksik deÄŸerleri kontrol et",
                "missing_data = data.isnull().sum()",
                "if missing_data.any():",
                "    print('\\nâš ï¸ Eksik DeÄŸerler:')",
                "    for col, missing_count in missing_data[missing_data > 0].items():",
                "        print(f'  {col}: {missing_count}')",
                ""
            ])
            
            code_lines.append("    # Eksik deÄŸerleri doldur")
            for col, missing_count in missing_data[missing_data > 0].items():
                if self.data[col].dtype in ['int64', 'float64']:
                    code_lines.append(f"    data.loc[data['{col}'].isnull(), '{col}'] = data['{col}'].median()")
                else:
                    code_lines.append(f"    data.loc[data['{col}'].isnull(), '{col}'] = data['{col}'].mode()[0]")
            code_lines.extend(["", "else:", "    print('âœ… Eksik deÄŸer yok')", ""])
        
        # Kategorik encoding
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            code_lines.extend([
                "# Kategorik sÃ¼tunlarÄ± encode et",
                "categorical_encoders = {}",
                ""
            ])
            
            for col in categorical_cols:
                # Hedef sÃ¼tunu kategorik encoding'den hariÃ§ tut
                if col == target_column:
                    continue
                    
                unique_count = self.data[col].nunique()
                if unique_count == 2:
                    code_lines.extend([
                        f"# {col} sÃ¼tunu iÃ§in Label Encoding (2 kategori)",
                        f"categorical_encoders['{col}'] = LabelEncoder()",
                        f"data['{col}'] = categorical_encoders['{col}'].fit_transform(data['{col}'])"
                    ])
                elif unique_count <= 10:
                    code_lines.extend([
                        f"# {col} sÃ¼tunu iÃ§in One-Hot Encoding ({unique_count} kategori)",
                        f"data = pd.get_dummies(data, columns=['{col}'], prefix='{col}')"
                    ])
                else:
                    code_lines.extend([
                        f"# {col} sÃ¼tunu Ã§ok fazla kategori iÃ§eriyor ({unique_count}), Target Encoding veya diÄŸer yÃ¶ntemler dÃ¼ÅŸÃ¼nÃ¼lmeli",
                        f"# Åimdilik One-Hot Encoding kullanÄ±lÄ±yor (dikkatli olun!)",
                        f"data = pd.get_dummies(data, columns=['{col}'], prefix='{col}')"
                    ])
                code_lines.append("")
        
        # Normalizasyon 
        numeric_cols = [col for col in self.data.select_dtypes(include=[np.number]).columns if col != target_column]
        if len(numeric_cols) > 0:
            code_lines.extend([
                "# SayÄ±sal sÃ¼tunlarÄ± normalize et (hedef sÃ¼tun hariÃ§)",
                "scaler = StandardScaler()",
                f"numeric_features = {numeric_cols}",
                "print(f'\\nğŸ“ Normalize edilecek sayÄ±sal sÃ¼tunlar: {numeric_features}')",
                "",
                "# Normalizasyon Ã¶ncesi istatistikler",
                "print('\\nğŸ“Š Normalizasyon Ã–ncesi Ä°statistikler:')",
                "print(data[numeric_features].describe())",
                "",
                "# Normalizasyonu uygula",
                "data[numeric_features] = scaler.fit_transform(data[numeric_features])",
                "",
                "# Normalizasyon sonrasÄ± istatistikler",
                "print('\\nğŸ“Š Normalizasyon SonrasÄ± Ä°statistikler:')",
                "print(data[numeric_features].describe())",
                ""
            ])
        
        # Hedef sÃ¼tunu iÅŸleme
        if target_column in self.data.columns:
            if self.data[target_column].dtype == 'object':
                code_lines.extend([
                    f"# Hedef sÃ¼tunu ({target_column}) kategorik - Label Encoding uygula",
                    f"target_encoder = LabelEncoder()",
                    f"data['{target_column}'] = target_encoder.fit_transform(data['{target_column}'])",
                    f"print(f'\\nğŸ¯ Hedef sÃ¼tunu encode edildi. SÄ±nÄ±flar: {{list(target_encoder.classes_)}}')",
                    ""
                ])
            else:
                code_lines.extend([
                    f"# Hedef sÃ¼tunu ({target_column}) sayÄ±sal - ek iÅŸlem gerekmiyor",
                    f"print(f'\\nğŸ¯ Hedef sÃ¼tunu: {target_column} (sayÄ±sal)')",
                    ""
                ])
        
        # Final steps
        code_lines.extend([
            "# Son veri seti durumu",
            "print(f'\\nğŸ“‹ Ä°ÅŸlenmiÅŸ Veri Seti:')",
            "print(f'Boyut: {data.shape}')",
            "print(f'SÃ¼tunlar: {list(data.columns)}')",
            "",
            "# Ã–zellik ve hedef deÄŸiÅŸkenleri ayÄ±r",
            f"if '{target_column}' in data.columns:",
            f"    X = data.drop('{target_column}', axis=1)",
            f"    y = data['{target_column}']",
            "else:",
            "    print('âŒ Hedef sÃ¼tunu bulunamadÄ±!')",
            "    print('Mevcut sÃ¼tunlar:', list(data.columns))",
            "    # En son sÃ¼tunu hedef olarak kabul et",
            "    X = data.iloc[:, :-1]",
            "    y = data.iloc[:, -1]",
            "    print(f'Son sÃ¼tun hedef olarak seÃ§ildi: {y.name}')",
            "",
            "# EÄŸitim ve test setlerine ayÄ±r", 
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
            "",
            "# SonuÃ§larÄ± gÃ¶ster",
            "print('\\nâœ… Veri Ã¶n iÅŸleme tamamlandÄ±!')",
            "print(f'ğŸ“Š EÄŸitim seti boyutu: {X_train.shape}')",
            "print(f'ğŸ“Š Test seti boyutu: {X_test.shape}')",
            "print(f'ğŸ¯ Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ±:')",
            "if hasattr(y, 'value_counts'):",
            "    print(y.value_counts())",
            "else:",
            "    print(f'Min: {y.min()}, Max: {y.max()}, Ortalama: {y.mean():.2f}')",
            "",
            "# Opsiyonel: SonuÃ§larÄ± kaydet",
            "save_choice = input('\\nÄ°ÅŸlenmiÅŸ veriyi kaydetmek istiyor musunuz? (y/n): ')",
            "if save_choice.lower() == 'y':",
            "    processed_filename = 'processed_data.csv'",
            "    data.to_csv(processed_filename, index=False)",
            "    print(f'âœ… Ä°ÅŸlenmiÅŸ veri {processed_filename} dosyasÄ±na kaydedildi.')",
        ])
        
        # Kodu yazdÄ±r
        for line in code_lines:
            print(line)
        
        # Dosyaya kaydet
        out_file = os.path.join(CURRENT_DIR, "preprocessing_code.py")
        with open(out_file, "w", encoding="utf-8") as f:
            f.write("\n".join(code_lines))

        print(f"\nâœ… Kod '{out_file}' dosyasÄ±na kaydedildi!")
        print(f"ğŸ’¡ Ä°pucu: Kodu Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce hedef sÃ¼tun adÄ±nÄ± kontrol edin!")
        
        # OluÅŸturulan kodu test et
        self.test_generated_code(out_file)
    
    def test_generated_code(self, code_file):
        """OluÅŸturulan preprocessing kodunu test et"""
        print(f"\nğŸ§ª OLUÅTURULAN KOD TEST EDÄ°LÄ°YOR...")
        print("="*60)
        
        try:
            # Test amaÃ§lÄ± kodu Ã§alÄ±ÅŸtÄ±r (gÃ¼venlik iÃ§in sadece syntax check)
            with open(code_file, 'r', encoding='utf-8') as f:
                code_content = f.read()
            
            # Syntax kontrolÃ¼
            try:
                compile(code_content, code_file, 'exec')
                print("âœ… Syntax kontrolÃ¼ baÅŸarÄ±lÄ± - kod hatalarÄ±z!")
            except SyntaxError as e:
                print(f"âŒ Syntax hatasÄ± bulundu: {e}")
                return False
            
            # Import kontrolÃ¼
            required_imports = ['pandas', 'numpy', 'sklearn']
            for imp in required_imports:
                if imp not in code_content:
                    print(f"âš ï¸  {imp} import eksik olabilir")
            
            # Temel kod yapÄ±sÄ± kontrolÃ¼
            essential_parts = [
                'pd.read_csv',
                'train_test_split', 
                'print',
                'shape'
            ]
            
            missing_parts = []
            for part in essential_parts:
                if part not in code_content:
                    missing_parts.append(part)
            
            if missing_parts:
                print(f"âš ï¸  Eksik kod parÃ§alarÄ±: {missing_parts}")
            else:
                print("âœ… TÃ¼m temel kod parÃ§alarÄ± mevcut")
            
            # Dosya boyutu kontrolÃ¼
            lines = len(code_content.split('\n'))
            print(f"ğŸ“„ OluÅŸturulan kod: {lines} satÄ±r")
            
            if lines < 10:
                print("âš ï¸  Kod Ã§ok kÄ±sa gÃ¶rÃ¼nÃ¼yor, eksiklik olabilir")
            elif lines > 200:
                print("âš ï¸  Kod Ã§ok uzun, optimizasyon gerekebilir") 
            else:
                print("âœ… Kod boyutu uygun")
            
            print(f"ğŸš€ Kodu Ã§alÄ±ÅŸtÄ±rmak iÃ§in: python {os.path.basename(code_file)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Test sÄ±rasÄ±nda hata: {e}")
            return False

# Demo fonksiyonu
def demo_data_analysis():
    """Demo veri analizi"""
    print("ğŸ® DEMO: Veri Analizi Rehberi")
    
    # Sahte veri oluÅŸtur
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'age': np.random.normal(35, 12, n_samples).astype(int),
        'salary': np.random.normal(50000, 15000, n_samples),
        'experience': np.random.normal(8, 4, n_samples).clip(0, None),
        'gender': np.random.choice(['Male', 'Female'], n_samples),
        'education': np.random.choice(['Bachelor', 'Master', 'PhD', 'High School'], n_samples),
        'score': np.random.normal(75, 10, n_samples).clip(0, 100)
    }
    
    # Hedef deÄŸiÅŸken oluÅŸtur (Ã¶rnek: yÃ¼ksek maaÅŸlÄ± = 1, dÃ¼ÅŸÃ¼k = 0)
    salary_threshold = np.median([s for s in data['salary'] if not np.isnan(s)])
    target = []
    for i in range(n_samples):
        if np.isnan(data['salary'][i]):
            # Eksik maaÅŸ deÄŸeri iÃ§in, yaÅŸ ve deneyime gÃ¶re tahmin et
            prob = 0.5 + (data['age'][i] - 35) * 0.01 + (data['experience'][i] - 8) * 0.02
            target.append(1 if np.random.random() < prob else 0)
        else:
            target.append(1 if data['salary'][i] > salary_threshold else 0)
    
    data['high_earner'] = target  # Hedef deÄŸiÅŸken
    
    # BazÄ± eksik deÄŸerler ekle
    missing_indices = np.random.choice(n_samples, 50, replace=False)
    data['salary'] = [val if i not in missing_indices else np.nan 
                     for i, val in enumerate(data['salary'])]
    
    df = pd.DataFrame(data)
    DEMOFILE = os.path.join(CURRENT_DIR, 'demo_dataset.csv')
    df.to_csv(DEMOFILE, index=False)

    # Analizi baÅŸlat
    analyzer = DataAnalysisGuide(DEMOFILE)
    analyzer.data = df  # Demo iÃ§in direkt set et
    
    print("\n1. Veri keÅŸfi...")
    analyzer.load_and_explore_data(DEMOFILE)

    input("\nDevam etmek iÃ§in ENTER tuÅŸuna basÄ±nÄ±z...")
    
    print("\n2. Veri kalitesi analizi...")
    analyzer.analyze_data_quality()
    
    input("\nGÃ¶rselleÅŸtirme iÃ§in ENTER tuÅŸuna basÄ±nÄ±z...")
    
    print("\n3. Veri gÃ¶rselleÅŸtirme...")
    analyzer.create_data_visualization_guide()
    
    input("\nÃ–n iÅŸleme Ã¶nerileri iÃ§in ENTER tuÅŸuna basÄ±nÄ±z...")
    
    print("\n4. Ã–n iÅŸleme Ã¶nerileri...")
    analyzer.preprocessing_recommendations()

if __name__ == "__main__":
    choice = input("""
ğŸ” Veri Analizi Rehberi
1. Demo Ã§alÄ±ÅŸtÄ±r
2. Kendi veri dosyanÄ±zÄ± analiz edin
SeÃ§iminiz: """)
    
    if choice == "1":
        demo_data_analysis()
    elif choice == "2":
        file_path = input("Veri dosyasÄ± yolunu girin (.csv): ")
        analyzer = DataAnalysisGuide(file_path)
        if analyzer.load_and_explore_data(file_path):
            analyzer.analyze_data_quality()
            analyzer.create_data_visualization_guide()
            analyzer.preprocessing_recommendations()