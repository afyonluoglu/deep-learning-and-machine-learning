"""
ğŸ§  RNN TEORÄ°SÄ° ve TEMEL KAVRAMLAR
=====================================

Bu dosya RNN'lerin teorik temellerini ve temel kavramlarÄ± aÃ§Ä±klar.

Recurrent Neural Network (RNN) nedir?
- Zaman serilerini iÅŸlemek iÃ§in tasarlanmÄ±ÅŸ sinir aÄŸÄ± tÃ¼rÃ¼dÃ¼r
- GeÃ§miÅŸ bilgiyi hatÄ±rlamasÄ± iÃ§in "hafÄ±za" mekanizmasÄ± vardÄ±r
- Sequential (sÄ±ralÄ±) verileri iÅŸlemek iÃ§in idealdir

Temel Ã–zellikler:
1. Temporal (zamansal) baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir
2. DeÄŸiÅŸken uzunlukta giriÅŸleri iÅŸleyebilir
3. Parametre paylaÅŸÄ±mÄ± ile etkili Ã¶ÄŸrenme
4. Gizli durum (hidden state) ile hafÄ±za

KullanÄ±m AlanlarÄ±:
- DoÄŸal dil iÅŸleme (NLP)
- Zaman serisi tahmini
- KonuÅŸma tanÄ±ma
- MÃ¼zik Ã¼retimi
- Video analizi
"""

import numpy as np
import matplotlib.pyplot as plt
import warnings
import os

# Matplotlib font uyarÄ±larÄ±nÄ± gizle
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def print_title(title, single_line:bool=False, line_len = 50):
    """BaÅŸlÄ±klarÄ± bÃ¼yÃ¼k ve belirgin yazdÄ±rÄ±r"""
    line_str = "=" if not single_line else "-"
    line_length = line_len
    if not single_line:
        print("\n" + "=" * line_length)
    else:
        print("\n")
    print(title)
    print(line_str * line_length + "\n")

print_title("ğŸ§  RNN TEORÄ°SÄ° VE TEMEL KAVRAMLAR", line_len=75)

print_title("ğŸ“‹ RNN NEDÄ°R?")
print("Recurrent Neural Network (RNN):")
print("â€¢ Zaman serilerini iÅŸlemek iÃ§in tasarlanmÄ±ÅŸ Ã¶zel sinir aÄŸÄ±")
print("â€¢ GeÃ§miÅŸ bilgileri 'hatÄ±rlayarak' tahmin yapar")
print("â€¢ Sequential (sÄ±ralÄ±) verilerle Ã§alÄ±ÅŸÄ±r")

print_title("ğŸ”„ RNN Ã‡ALIÅMA PRENSÄ°BÄ°")
print("1. Her zaman adÄ±mÄ±nda:")
print("   - Mevcut giriÅŸ (x_t)")
print("   - Ã–nceki hidden state (h_t-1)")
print("   - Yeni hidden state hesaplanÄ±r (h_t)")

print("\n2. FormÃ¼l:")
print("   h_t = tanh(W_hh * h_t-1 + W_xh * x_t + b)")
print("   y_t = W_hy * h_t + b_y")

print_title("ğŸ“Š BASIT RNN Ã–RNEÄÄ° - MANUEL UYGULAMA")

# Basit RNN manuel uygulamasÄ± (eÄŸitim amaÃ§lÄ±)
def simple_rnn_step(x_t, h_prev, W_hh, W_xh, b):
    """
    ğŸ§  BÄ°R RNN ADIMINI MANUEL OLARAK HESAPLAR
    
    Bu fonksiyon RNN'in temel matematiksel iÅŸlemini yapar:
    1. Ã–nceki hafÄ±zayÄ± (h_prev) mevcut hafÄ±za aÄŸÄ±rlÄ±klarÄ±yla (W_hh) Ã§arpar
    2. Yeni giriÅŸi (x_t) giriÅŸ aÄŸÄ±rlÄ±klarÄ±yla (W_xh) Ã§arpar  
    3. Ä°kisini toplar, bias ekler ve tanh aktivasyonundan geÃ§irir
    4. Yeni hafÄ±za durumunu (h_t) dÃ¶ndÃ¼rÃ¼r
    
    Args:
        x_t: Mevcut zaman adÄ±mÄ±ndaki giriÅŸ verisi (Ã¶rn: [0.8, 0.5])
        h_prev: Ã–nceki zaman adÄ±mÄ±ndan gelen hafÄ±za durumu
        W_hh: Hidden-to-Hidden aÄŸÄ±rlÄ±k matrisi (hafÄ±zanÄ±n kendisini nasÄ±l gÃ¼ncellediÄŸi)
        W_xh: Input-to-Hidden aÄŸÄ±rlÄ±k matrisi (yeni giriÅŸin hafÄ±zayÄ± nasÄ±l etkilediÄŸi)  
        b: Bias terimi (Ã¶ÄŸrenilen sabit eklenti)
    
    Returns:
        h_t: Yeni hesaplanan hafÄ±za durumu (bir sonraki adÄ±mda h_prev olacak)
        
    ğŸ’¡ FORMÃœL: h_t = tanh(W_hh * h_prev + W_xh * x_t + b)
    """
    # AdÄ±m 1: Ã–nceki hafÄ±zayÄ± mevcut aÄŸÄ±rlÄ±klarla Ã§arp (W_hh * h_prev)
    memory_contribution = np.dot(W_hh, h_prev)
    
    # AdÄ±m 2: Yeni giriÅŸi aÄŸÄ±rlÄ±klarla Ã§arp (W_xh * x_t)  
    input_contribution = np.dot(W_xh, x_t)
    
    # AdÄ±m 3: Ä°kisini topla, bias ekle ve tanh aktivasyonundan geÃ§ir
    combined = memory_contribution + input_contribution + b
    h_t = np.tanh(combined)  # tanh: deÄŸerleri -1 ile +1 arasÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r
    
    return h_t

# ğŸ›ï¸ PARAMETRIK AYARLAR - Ä°STEDÄ°ÄÄ°NÄ°Z GÄ°BÄ° DEÄÄ°ÅTÄ°REBÄ°LÄ°RSÄ°NÄ°Z!
print_title("ğŸ›ï¸ PARAMETRIK AYARLAR", True)

# Ana parametreler (buradan kolayca deÄŸiÅŸtirebilirsiniz)
HIDDEN_SIZE = 13      # Hidden layer'daki nÃ¶ron sayÄ±sÄ± (2-50 arasÄ± deneyin)
INPUT_SIZE = 2        # GiriÅŸ boyutu 
TIME_STEPS = 6        # Zaman adÄ±mÄ± sayÄ±sÄ± (3-20 arasÄ± deneyin)
WEIGHT_SCALE = 0.2    # AÄŸÄ±rlÄ±k Ã¶lÃ§eÄŸi (0.01-0.5 arasÄ± deneyin)

# âœ¨ Deneyim Ã–nerileri:

# Hidden Size Etkisi:
# HIDDEN_SIZE = 5 â†’ Basit Ã¶ÄŸrenme
# HIDDEN_SIZE = 20 â†’ KarmaÅŸÄ±k Ã¶ÄŸrenme
# HIDDEN_SIZE = 50 â†’ Ã‡ok gÃ¼Ã§lÃ¼ ama yavaÅŸ

# Zaman AdÄ±mÄ± Etkisi:
# TIME_STEPS = 3 â†’ KÄ±sa hafÄ±za
# TIME_STEPS = 15 â†’ Uzun hafÄ±za
# TIME_STEPS = 25 â†’ Ã‡ok uzun hafÄ±za

# AÄŸÄ±rlÄ±k Etkisi:
# WEIGHT_SCALE = 0.01 â†’ ZayÄ±f sinyal
# WEIGHT_SCALE = 0.3 â†’ GÃ¼Ã§lÃ¼ sinyal
# WEIGHT_SCALE = 0.7 â†’ Ã‡ok gÃ¼Ã§lÃ¼ (patlama riski)

print(f"âœ… Hidden Size : {HIDDEN_SIZE}")
print(f"âœ… Input Size  : {INPUT_SIZE}")  
print(f"âœ… Time Steps  : {TIME_STEPS}")
print(f"âœ… Weight Scale: {WEIGHT_SCALE}")

# Dinamik veri Ã¼retimi - zaman adÄ±mÄ± sayÄ±sÄ±na gÃ¶re
def generate_random_sequence_data(time_steps, input_size):
    """Belirtilen zaman adÄ±mÄ± sayÄ±sÄ±na gÃ¶re Ã¶rnek veri Ã¼retir"""
    sequence = []
    np.random.seed(123)  # Tekrarlanabilir sonuÃ§lar iÃ§in
    
    for t in range(time_steps):
        # SinÃ¼s dalgasÄ± + rastgele gÃ¼rÃ¼ltÃ¼ ile ilginÃ§ desenler oluÅŸtur
        x1 = 0.8 * np.sin(2 * np.pi * t / time_steps) + 0.2 * np.random.randn()
        x2 = 0.6 * np.cos(2 * np.pi * t / time_steps) + 0.1 * np.random.randn()
        
        sequence.append(np.array([[x1], [x2]]))
    
    return sequence

# Ã–rnek parametreler
hidden_size = HIDDEN_SIZE
input_size = INPUT_SIZE

# Rastgele aÄŸÄ±rlÄ±klar (gerÃ§ekte eÄŸitimle Ã¶ÄŸrenilir)
np.random.seed(42)
W_hh = np.random.randn(hidden_size, hidden_size) * WEIGHT_SCALE
W_xh = np.random.randn(hidden_size, input_size) * WEIGHT_SCALE
b = np.zeros((hidden_size, 1))
print(f"â¡ï¸ AÄŸÄ±rlÄ±k matrisleri oluÅŸturuldu (boyutlar: W_hh={W_hh.shape}, W_xh={W_xh.shape})")
print(f"W_hh= {W_hh[:5]}")
print(f"W_xh= {W_xh[:5]}")
print(f"b   = {b.flatten()[:5]} ... ({hidden_size} bias)")

exit()
# Dinamik olarak veri Ã¼ret
sequence_data = generate_random_sequence_data(TIME_STEPS, INPUT_SIZE)

print(f"\nğŸŸ¢ GiriÅŸ dizisi ({TIME_STEPS} zaman adÄ±mÄ±):")
for t, x_t in enumerate(sequence_data):
    formatted_x_t = ", ".join([f"{val:11.8f}" for val in x_t.flatten()])
    print(f"  t{t}: [{formatted_x_t}]")

print_title("ğŸ” ADIM ADIM RNN HESAPLAMA:")

# Ä°lk gizli durum (sÄ±fÄ±r)
h = np.zeros((hidden_size, 1))

formatted_h_init = ", ".join([f"{val:11.8f}" for val in h.flatten()[:5]])
print(f"BaÅŸlangÄ±Ã§ hidden states:              h  = [{formatted_h_init}] ... ({hidden_size} nÃ¶ron)")

# Her zaman adÄ±mÄ±nÄ± iÅŸle ve hidden state evrimini gÃ¶zlemle
hidden_states = []
for t, x_t in enumerate(sequence_data):
    # ğŸ”„ RNN'Ä°N KALBÄ°: Bu satÄ±r RNN'in temel iÅŸlemini yapar
    # Mevcut giriÅŸ (x_t) + Ã¶nceki hafÄ±za (h) = yeni hafÄ±za (h)

    # FORMÃœL: h_t = tanh(W_hh * h_prev + W_xh * x_t + b)
    # simple_rnn_step(x_t, h_prev, W_hh, W_xh, b):
    h = simple_rnn_step(x_t, h, W_hh, W_xh, b)
    
    # SonuÃ§larÄ± sakla (gÃ¶rselleÅŸtirme iÃ§in)
    hidden_states.append(h.copy())  # .copy() Ã¶nemli: referansÄ± deÄŸil deÄŸeri sakla
    
    formatted_x_t = ", ".join([f"{val:11.8f}" for val in x_t.flatten()])
    formatted_h_t = ", ".join([f"{val:11.8f}" for val in h.flatten()[:5]])
    print(f"t{t}: x_{t} = [{formatted_x_t}], h{t} = [{formatted_h_t}] ...")

    # ğŸ’¡ Ã–NEMLÄ°: h artÄ±k bir Ã¶nceki adÄ±mÄ±n bilgisini iÃ§eriyor!

print_title("ğŸ“ˆ GÄ°ZLÄ° DURUMLARIN GÃ–RSELLEÅTÄ°RÄ°LMESÄ°")

# Gizli durumlarÄ± gÃ¶rselleÅŸtir - DÄ°NAMÄ°K RENK SÄ°STEMÄ°
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

# GiriÅŸ verisini gÃ¶ster
input_data = np.array([x.flatten() for x in sequence_data])
time_steps = range(len(sequence_data))

ax1.plot(time_steps, input_data[:, 0], 'bo-', label='GiriÅŸ 1', linewidth=2, markersize=8)
ax1.plot(time_steps, input_data[:, 1], 'ro-', label='GiriÅŸ 2', linewidth=2, markersize=8)
ax1.set_title(f'RNN GiriÅŸ Verisi ({TIME_STEPS} zaman adÄ±mÄ±)', fontsize=14, fontweight='bold')
ax1.set_xlabel('Zaman AdÄ±mÄ±')
ax1.set_ylabel('DeÄŸer')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Gizli durumlarÄ± gÃ¶ster - DÄ°NAMÄ°K RENK VE NÃ–RON SEÃ‡Ä°MÄ°
hidden_data = np.array([h.flatten() for h in hidden_states])

# Ã‡ok nÃ¶ron varsa sadece ilk 10 tanesini gÃ¶ster
max_neurons_to_show = min(10, hidden_size)
print(f"ğŸ“Š Grafikte gÃ¶sterilen nÃ¶ron sayÄ±sÄ±: {max_neurons_to_show} / {hidden_size}")

# Dinamik renk paleti oluÅŸtur
colors = plt.cm.tab10(np.linspace(0, 1, max_neurons_to_show))  # 10 farklÄ± renk

for i in range(max_neurons_to_show):
    ax2.plot(time_steps, hidden_data[:, i], 'o-', 
             color=colors[i], label=f'H.NÃ¶ron {i+1}', linewidth=2, markersize=6)

ax2.set_title(f'RNN Hidden State Evrimi ({hidden_size} nÃ¶ron, {max_neurons_to_show} gÃ¶steriliyor)', 
              fontsize=14, fontweight='bold')
ax2.set_xlabel('Zaman AdÄ±mÄ±')
ax2.set_ylabel('Hidden State DeÄŸeri')

# Legend sadece Ã§ok fazla nÃ¶ron yoksa gÃ¶ster
if max_neurons_to_show <= 10:
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
else:
    ax2.text(0.02, 0.98, f'{hidden_size} nÃ¶ron var\n(Ä°lk {max_neurons_to_show} gÃ¶steriliyor)', 
             transform=ax2.transAxes, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print_title("\nğŸ§ª PARAMETRÄ°K DENEY SÄ°STEMÄ°", True)

print("YukarÄ±daki parametreleri deÄŸiÅŸtirerek farklÄ± deneyler yapabilirsiniz!")
print()
print_title("ğŸ”§ NASIL FARKLI DURUMLAR DENEYEBÄ°LÄ°RÄ°M?",True)

print("1. DosyanÄ±n baÅŸÄ±ndaki parametreleri deÄŸiÅŸtirin:")
print("   â€¢ HIDDEN_SIZE  = 5, 10, 20, 50  (farklÄ± deÄŸerler deneyin)")
print("   â€¢ TIME_STEPS   = 4, 8, 15, 20   (farklÄ± zaman aralÄ±klarÄ±)")
print("   â€¢ WEIGHT_SCALE = 0.01, 0.1, 0.5 (aÄŸÄ±rlÄ±k bÃ¼yÃ¼klÃ¼kleri)")
print()
print("2. Kodu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n ve sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rÄ±n.")
print()
print_title("ğŸ¯ DENEYÄ°M Ã–NERÄ°LERÄ°:")

print("â€¢ Hidden Size 2 - 20      : Hangi daha iyi Ã¶ÄŸrenir?")
print("â€¢ Time Steps 4 - 15       : Uzun veya kÄ±sa diziler") 
print("â€¢ Weight Scale 0.01 vs 0.5: AÄŸÄ±rlÄ±k etkisi")

print_title("ğŸ“Š MEVCUT DENEY SONUÃ‡LARI:")

print(f"âœ“ Hidden Size  : {HIDDEN_SIZE}")
print(f"âœ“ Time Steps   : {TIME_STEPS}")
print(f"âœ“ Son Hidden State ortalama: {np.mean(hidden_states[-1]):.4f}")
print(f"âœ“ Hidden State deÄŸiÅŸkenliÄŸi: {np.std([np.mean(h) for h in hidden_states]):.4f}")
print(f"âœ“ Maksimum aktivasyon      : {np.max([np.max(np.abs(h)) for h in hidden_states]):.4f}")

# Otomatik performans analizi
def analyze_performance(hidden_states, sequence_data):
    """RNN performansÄ±nÄ±n basit analizi"""
    # HafÄ±za kararlÄ±lÄ±ÄŸÄ± - hidden state'lerin ne kadar deÄŸiÅŸken olduÄŸu
    stability = np.std([np.mean(h) for h in hidden_states])
    
    # Aktivasyon gÃ¼cÃ¼ - nÃ¶ronlarÄ±n ne kadar aktif olduÄŸu
    activation_power = np.mean([np.mean(np.abs(h)) for h in hidden_states])
    
    # Desen hassasiyeti - giriÅŸe ne kadar tepki verdiÄŸi
    input_sensitivity = np.std([np.mean(x.flatten()) for x in sequence_data])
    
    return {
        "stability": stability,
        "activation_power": activation_power, 
        "input_sensitivity": input_sensitivity
    }

performance = analyze_performance(hidden_states, sequence_data)

print_title(f"ğŸ“ˆ OTOMATÄ°K PERFORMANS ANALÄ°ZÄ°:")

print(f"ğŸ”¹ HafÄ±za KararlÄ±lÄ±ÄŸÄ±: {performance['stability']:.4f}")
print("   (DÃ¼ÅŸÃ¼k = kararlÄ±, YÃ¼ksek = deÄŸiÅŸken)")
print(f"ğŸ”¹ Aktivasyon GÃ¼cÃ¼: {performance['activation_power']:.4f}")  
print("   (Ã‡ok dÃ¼ÅŸÃ¼k = Ã¶ÄŸrenme zor, Ã‡ok yÃ¼ksek = patlama riski)")
print(f"ğŸ”¹ GiriÅŸ Hassasiyeti: {performance['input_sensitivity']:.4f}")
print("   (YÃ¼ksek = Ã§eÅŸitli giriÅŸ desenleri)")

# Ã–neriler
print_title("ğŸ’¡ PERFORMANS DEÄERLENDÄ°RMESÄ°:")

if performance['activation_power'] < 0.1:
    print("âš ï¸  Aktivasyon Ã§ok dÃ¼ÅŸÃ¼k - WEIGHT_SCALE'i artÄ±rÄ±n")
elif performance['activation_power'] > 0.8:
    print("âš ï¸  Aktivasyon Ã§ok yÃ¼ksek - WEIGHT_SCALE'i azaltÄ±n")
else:
    print("âœ… Aktivasyon dengeli gÃ¶rÃ¼nÃ¼yor")

if performance['stability'] > 0.5:
    print("âš ï¸  Hidden state Ã§ok deÄŸiÅŸken - daha az TIME_STEPS deneyin")
else:
    print("âœ… Hidden state kararlÄ±")

print("Bir tuÅŸa basÄ±nÄ±z...")
input()

print_title("ğŸ¯ HIDDEN STATE'Ä°N FAYDASINI GÃ–STEREN Ã–RNEKLER", single_line=True, line_len=70)

# Ã–rnek 1: Desen TanÄ±ma

print_title("    ğŸ“‹ Ã–RNEK 1: DESEN TANIMA")

print("Diyelim ki ÅŸu sÄ±rayla sayÄ±lar gelsin: [1, 0, 1, 0]")

# Desen analizi iÃ§in hidden state'leri incele
pattern_data = [
    np.array([[1.0], [0.0]]),  # t=0: 1
    np.array([[0.0], [1.0]]),  # t=1: 0  
    np.array([[1.0], [0.0]]),  # t=2: 1
    np.array([[0.0], [1.0]])   # t=3: 0
]

print("Desen: 1 -> 0 -> 1 -> 0 (deÄŸiÅŸken desen)\n")
h_pattern = np.zeros((hidden_size, 1))
pattern_states = []

for t, x_t in enumerate(pattern_data):
    # ğŸ§  DESEN Ã–ÄRENME: RNN her adÄ±mda Ã¶nceki desenleri hatÄ±rlayarak yeni giriÅŸi iÅŸler
    h_pattern = simple_rnn_step(x_t, h_pattern, W_hh, W_xh, b)
    pattern_states.append(h_pattern.copy())
    
    # Hidden state'in bu adÄ±mda neyi 'hatÄ±rladÄ±ÄŸÄ±nÄ±' gÃ¶ster
    dominant_neuron = np.argmax(np.abs(h_pattern))  # En bÃ¼yÃ¼k mutlak deÄŸere sahip nÃ¶ron
    h_pattern_formatted = ", ".join([f"{val:11.8f}" for val in h_pattern.flatten()[:5]])
    print(f"t{t}: GiriÅŸ={x_t.flatten()} -> Hidden State: {h_pattern_formatted} ... (ilk 5 nÃ¶ron)")
    print(f"     En aktif nÃ¶ron: {dominant_neuron} (Bu nÃ¶ron geÃ§miÅŸi 'hatÄ±rlÄ±yor')")

print("\nğŸ” ANALÄ°Z:")
print("Hidden state'in deÄŸiÅŸimi, RNN'in Ã¶nceki giriÅŸleri 'hatÄ±rladÄ±ÄŸÄ±nÄ±' gÃ¶sterir!")
print("Her adÄ±mda sadece o anki giriÅŸ deÄŸil, geÃ§miÅŸ giriÅŸler de etkili olur.")
print("Bir tuÅŸa basÄ±nÄ±z...")
input()

# Ã–rnek 2: GerÃ§ek RNN EÄŸitimi ve BaÅŸarÄ±lÄ± Tahmin

print_title("    ğŸ“‹ Ã–RNEK 2: GERÃ‡EK RNN EÄÄ°TÄ°MÄ° VE TAHMÄ°N")

print("RNN'i eÄŸitelim ve baÅŸarÄ±sÄ±nÄ± Ã¶lÃ§elim!")

# ğŸ¯ GENÄ°ÅLETÄ°LMÄ°Å EÄÄ°TÄ°M VERÄ°SETÄ° OLUÅTUR
print_title("ğŸ¯ EÄŸitim veri seti oluÅŸturuluyor...", single_line=True)

def create_training_sequences(pattern_type="decreasing", num_sequences=50, seq_length=6):
    """
    EÄŸitim iÃ§in Ã§oklu sekans Ã¼retir
    
    Args:
        pattern_type: "decreasing", "increasing", "sine" 
        num_sequences: KaÃ§ farklÄ± sekans
        seq_length: Her sekansÄ±n uzunluÄŸu
    """
    sequences = []
    targets = []
    
    np.random.seed(42)  # Tekrarlanabilir sonuÃ§lar
    
    for i in range(num_sequences):
        if pattern_type == "decreasing":
            # Azalan desenler: farklÄ± baÅŸlangÄ±Ã§ ve azalma oranlarÄ±
            start_val = np.random.uniform(0.8, 2.0) # BaÅŸlangÄ±Ã§ deÄŸeri 0.8 ile 2.0 arasÄ±nda
            decrease_rate = np.random.uniform(0.1, 0.4) # Azalma oranÄ± 0.1 ile 0.4 arasÄ±nda deÄŸer Ã§Ä±kartÄ±larak dizi Ã¼retiliyor
            
            sequence = []
            current_val = start_val
            for _ in range(seq_length):
                sequence.append(np.array([[current_val], [0.0]]))
                current_val -= decrease_rate
            
            # Target: bir sonraki deÄŸer
            target = current_val
            
        elif pattern_type == "increasing":
            # Artan desenler
            start_val = np.random.uniform(0.2, 5.0)
            increase_rate = np.random.uniform(0.1, 0.6)
            
            sequence = []
            current_val = start_val
            for _ in range(seq_length):
                sequence.append(np.array([[current_val], [0.0]]))
                current_val += increase_rate
            
            target = current_val
            
        elif pattern_type == "sine":
            # SinÃ¼s dalgasÄ± desenleri
            frequency = np.random.uniform(0.5, 2.0)
            amplitude = np.random.uniform(0.5, 1.5)
            
            sequence = []
            for j in range(seq_length):
                val = amplitude * np.sin(frequency * j) + np.random.normal(0, 0.05)
                sequence.append(np.array([[val], [0.0]]))
            
            # Target: sinÃ¼s devamÄ±
            target = amplitude * np.sin(frequency * seq_length)
        
        sequences.append(sequence)
        targets.append(target)
    
    return sequences, targets

# FarklÄ± pattern tÃ¼rlerinde eÄŸitim verisi oluÅŸtur
decreasing_seqs, decreasing_targets = create_training_sequences("decreasing", 30, 5)
increasing_seqs, increasing_targets = create_training_sequences("increasing", 20, 5)

# TÃ¼m eÄŸitim verilerini birleÅŸtir
all_training_sequences = decreasing_seqs + increasing_seqs
all_training_targets = decreasing_targets + increasing_targets

seq_count = len(all_training_sequences)
uzunluk = len(all_training_sequences[0])
print(f"âœ… {seq_count} eÄŸitim sekansÄ± oluÅŸturuldu (her biri {uzunluk} adÄ±m)")
print(f"   - {len(decreasing_seqs)} azalan desen")
print(f"   - {len(increasing_seqs)} artan desen")

if seq_count > 5:
    say = 5
else:
    say = seq_count
print(f"   Ä°lk {say} sekans Ã¶rneÄŸi:")

for i in range(say):
    formatted_sequence = [x[0][0] for x in all_training_sequences[i]]
    veri = ", ".join([f"{x:8.4f}" for x in formatted_sequence])
    print(f"Training sequence {i+1}: {veri:<54}" +
          f"Training target {i+1}: {all_training_targets[i]:8.4f}")

# Test iÃ§in Ã¶zel sequence (orijinal pattern'imiz)
test_sequence = [
    np.array([[1.4], [0.0]]),   
    np.array([[1.1], [0.0]]),   # (-0.3)
    np.array([[0.9], [0.0]]),   # (-0.2)
    np.array([[0.6], [0.0]]),   # (-0.3)
    np.array([[0.4], [0.0]])    # (-0.2)
]
test_target = 0.1  # Manuel hesaplanan beklenen deÄŸer

formatted_sequence = [x[0][0] for x in test_sequence]
degerler = ",  ".join([f"{x:.1f}" for x in formatted_sequence])
print(f"\nğŸ¯ Test Dizisi  : {degerler} -> ???")
print(f"   Beklenen sonraki deÄŸer: {test_target:.1f}")


# ğŸ§  RNN MODELÄ° OLUÅTUR
print_title("ğŸ§  Creaating RNN Model...", single_line=True)

class SimpleTrainableRNN:
    def __init__(self, hidden_size=16, input_size=2, learning_rate=0.01):
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.learning_rate = learning_rate
        
        # Xavier initialization ile aÄŸÄ±rlÄ±klarÄ± baÅŸlat
        # Bu yÃ¶ntem, gradient'larÄ±n kaybolmasÄ±nÄ± veya patlamasÄ±nÄ± Ã¶nler
        scale = np.sqrt(2.0 / (hidden_size + input_size))  # Xavier Ã¶lÃ§ek faktÃ¶rÃ¼

        # W_hh: Hidden-to-Hidden aÄŸÄ±rlÄ±klarÄ± (hafÄ±zanÄ±n kendisini gÃ¼ncellemesi iÃ§in)
        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale

        # W_xh: Input-to-Hidden aÄŸÄ±rlÄ±klarÄ± (giriÅŸ ile hidden-layer arasÄ± aÄŸÄ±rlÄ±klar)  
        self.W_xh = np.random.randn(hidden_size, input_size) * scale

        # W_hy: Hidden-to-Output aÄŸÄ±rlÄ±klarÄ± (hidden layer ile Ã§Ä±kÄ±ÅŸ arasÄ±ndaki aÄŸÄ±rlÄ±klar)
        self.W_hy = np.random.randn(1, hidden_size) * scale

        # b_h: Hidden bias (hidden layer iÃ§in Ã¶ÄŸrenilen sabit eklenti)
        self.b_h = np.zeros((hidden_size, 1))

        # b_y: Output bias (Ã§Ä±kÄ±ÅŸ katmanÄ± iÃ§in Ã¶ÄŸrenilen sabit eklenti)
        self.b_y = np.zeros((1, 1))
        
        print(f"âœ… Model created:")
        print(f"   Hidden Size   : {hidden_size}")
        print(f"   Learning Rate : {learning_rate}")
        print(f"   Weight matrices initialized (Xavier)")

    def forward(self, sequence):
        """Forward pass - tahmin yap"""
        # Her forward iÅŸleminin baÅŸÄ±nda "h" (hidden state) sÄ±fÄ±rlanÄ±r. BÃ¶ylece ÅŸunlar saÄŸlanÄ±r:
        # Genelleme  : Model, eÄŸitimde gÃ¶rmediÄŸi dizilerde Ã§alÄ±ÅŸabilir.
        # BaÄŸÄ±msÄ±zlÄ±k: Sequence'lar birbirini etkilemez (istenmeyen bilgi sÄ±zÄ±ntÄ±sÄ± Ã¶nlenir).
        # Verimlilik : Her sequence kendi hafÄ±zasÄ±nda iÅŸlenir.
        h = np.zeros((self.hidden_size, 1))
        
        # Bundan sonraki iÅŸlemlerde, eÄŸitimde Ã¶ÄŸrenilen aÄŸÄ±rlÄ±klar (W_xh, W_hh, W_hy) kullanÄ±lÄ±r

        # Her zaman adÄ±mÄ±nÄ± iÅŸle
        for x_t in sequence:
            # RNN'in temel iÅŸlemi
            # FORMÃœL: h_t = tanh(W_hh * h_prev + W_xh * x_t + b)
            # Trainde Ã¶ÄŸrenilen aÄŸÄ±rlÄ±klar (W_xh, W_hh) burada kullanÄ±lÄ±r
            h = np.tanh(np.dot(self.W_hh, h) + np.dot(self.W_xh, x_t) + self.b_h)
        
        # Final prediction
        # RNN'in Ã§Ä±kÄ±ÅŸ katmanÄ± deÄŸeri: y = W_hy Ã— h + b_y
        output = np.dot(self.W_hy, h) + self.b_y
        return output[0, 0], h
    
    def train_step(self, sequence, target):
        """Tek eÄŸitim adÄ±mÄ± (basitleÅŸtirilmiÅŸ backpropagation)"""
        prediction, final_h = self.forward(sequence)
        
        # Loss hesapla (MSE, Mean Squared Error)
        error = prediction - target
        loss = (error) ** 2
        
        # Gradient Hesaplama (Zincir KuralÄ±):
        # gradient = âˆ‚Loss/âˆ‚W_hy = âˆ‚Loss/âˆ‚prediction Ã— âˆ‚prediction/âˆ‚W_hy
        #                        = 2 Ã— (prediction - target) Ã— final_h
        # AÄŸÄ±rlÄ±k GÃ¼ncellemesi:
        # W_hy_new = W_hy_old - learning_rate Ã— gradient

        # Output weights gÃ¼ncelle - Backpropagation aÅŸamasÄ±
        # final_h.T = (Ã§Ä±kÄ±ÅŸÄ±n hidden state'e gÃ¶re tÃ¼revi)
        self.W_hy -= self.learning_rate * error * final_h.T * 0.1
        self.b_y -= self.learning_rate * error * 0.01
        
        # Hidden weights gÃ¼ncelle - Hidden layer'lar iÃ§in backprop (basit yaklaÅŸÄ±m)
        # Åu sebeplerle "basit yaklaÅŸÄ±m" diye isimlendirildi:
        # 1. Hidden katmanlar iÃ§in gerÃ§ek backprop yok: Sadece rastgele gÃ¼rÃ¼ltÃ¼ ile gÃ¼ncelleniyor.
        # 2. Zincir kuralÄ± eksik: GerÃ§ek backprop, her katmandan baÅŸlayarak geriye doÄŸru gradient'larÄ± hesaplar
        # 3. Aktivasyon fonksiyonlarÄ±nÄ±n tÃ¼revleri yok: tanh'Ä±n tÃ¼revi kullanÄ±lmÄ±yor
        # Bu basit yaklaÅŸÄ±m, temel bir Ã¶ÄŸrenme simÃ¼lasyonu saÄŸlar ancak gerÃ§ek uygulamalarda yetersizdir.
        # Bu ÅŸekli, temel kavramlarÄ± Ã¶ÄŸretmek iÃ§in eÄŸitim amaÃ§lÄ± yeterlidir. AyrÄ±ca  Basit gradient descent ile loss'u azaltÄ±r
        gradient_scale = self.learning_rate * error * 0.001
        self.W_hh -= gradient_scale * np.random.randn(*self.W_hh.shape) * 0.1  # â† Ã–ÄŸrenme burada oluyor
        self.W_xh -= gradient_scale * np.random.randn(*self.W_xh.shape) * 0.1  # â† Ã–ÄŸrenme burada oluyor

        return loss, prediction

# Model oluÅŸtur
rnn_model = SimpleTrainableRNN(hidden_size=16, learning_rate=0.05)

# ğŸ“ EÄŸitim BaÅŸlat
print_title("ğŸ“ Start training...", single_line=True)

epochs = 40
batch_size = 10

print(f"Epochs    : {epochs}")
print(f"Batch Size: {batch_size}")
print()

# EÄŸitim dÃ¶ngÃ¼sÃ¼
training_losses = []
test_predictions = []

for epoch in range(epochs):
    epoch_losses = []
    
    # Mini-batch eÄŸitimi
    for i in range(0, len(all_training_sequences), batch_size):
        batch_sequences = all_training_sequences[i:i+batch_size]
        batch_targets = all_training_targets[i:i+batch_size]
        
        batch_loss = 0
        for seq, target in zip(batch_sequences, batch_targets):
            loss, pred = rnn_model.train_step(seq, target)
            batch_loss += loss
        
        epoch_losses.append(batch_loss / len(batch_sequences))
    
    # Epoch ortalamasÄ±
    avg_loss = np.mean(epoch_losses)
    training_losses.append(avg_loss)
    
    # Test tahmin yap
    test_pred, _ = rnn_model.forward(test_sequence)
    test_predictions.append(test_pred)
    test_error = abs(test_pred - test_target)
    
    # Ä°lerlemeyi gÃ¶ster
    if epoch % 5 == 0 or epoch == epochs - 1:
        print(f"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Test Prediction={test_pred:.3f}, Error={test_error:.3f}")

print("\nğŸ¯ Training completed.")

# ğŸ“Š SONUÃ‡LARI DEÄERLENDÄ°R
print_title("ğŸ“Š Result Analysis:", single_line=True)

final_prediction, _ = rnn_model.forward(test_sequence)
final_error = abs(final_prediction - test_target)

print(f"ğŸ”¹ Test Sequence  : {degerler}")
print(f"ğŸ”¹ Expected Value : {test_target:.1f}")
print(f"ğŸ”¹ RNN Prediction : {final_prediction:.3f}")
print(f"ğŸ”¹ Absolute Error : {final_error:.3f}")
print(f"ğŸ”¹ Error Rate     : {(final_error/abs(test_target)*100):.1f}%")

print_title(f"ğŸ“ˆ Training Progress:", single_line=True)
print(f"ğŸ”¹ Initial Loss   : {training_losses[0]:.4f}")
print(f"ğŸ”¹ Final Loss     : {training_losses[-1]:.4f}")
print(f"ğŸ”¹ Improvement    : {((training_losses[0]-training_losses[-1])/training_losses[0]*100):.1f}%")

def calc_success_level(final_error, silent:True):
    """Hata oranÄ±na gÃ¶re baÅŸarÄ± seviyesini belirle"""
    # BaÅŸarÄ± deÄŸerlendirmesi
    if final_error < 0.1:
        if not silent:
            print("\nğŸ† GREAT RESULT!")
            print("   RNN pattern successfully learned!")
        return "ğŸ† Excellent"
    elif final_error < 0.2:
        if not silent:
            print("\nâœ… GOOD RESULT!")
            print("   RNN pattern largely learned!")
        return "âœ… Good"
    elif final_error < 0.4:
        if not silent:
            print("\nâš ï¸  AVERAGE RESULT")
            print("   RNN partially learned, more training needed")
        return "âš ï¸  Average"
    else:
        if not silent:
            print("\nâŒ FAILED")
            print("   RNN pattern could not be learned, model or data issue")
        return "âŒ Failed"

success_level = calc_success_level(final_error, silent=False)

# BaÅŸarÄ± deÄŸerlendirmesi
if final_error < 0.1:
    print("\nğŸ† GREAT RESULT!")
    print("   RNN pattern successfully learned!")
    success_level = "Excellent"
elif final_error < 0.2:
    print("\nâœ… GOOD RESULT!")
    print("   RNN pattern largely learned!")
    success_level = "Good"
elif final_error < 0.4:
    print("\nâš ï¸  AVERAGE RESULT")
    print("   RNN partially learned, more training needed")
    success_level = "Average"
else:
    print("\nâŒ FAILED")
    print("   RNN pattern could not be learned, model or data issue")
    success_level = "Failed"

# ğŸ¯ ADDITIONAL TEST PATTERNS
print_title(f"ğŸ¯ ADDITIONAL TEST PATTERNS:")

# Benzer azalan desen test et
extra_test_1 = [
    np.array([[2.0], [0.0]]), 
    np.array([[1.7], [0.0]]), 
    np.array([[1.4], [0.0]]), 
    np.array([[1.1], [0.0]]), 
    np.array([[0.8], [0.0]])
]
expected_1 = 0.5
pred_1, _ = rnn_model.forward(extra_test_1)
error_1 = abs(pred_1 - expected_1)

extra_values_1 = [x[0][0] for x in extra_test_1]
extra_str_1 = ", ".join([f"{x:.1f}" for x in extra_values_1])

print(f"Test 1: {extra_str_1} -> Expected: {expected_1:.1f}, Prediction: {pred_1:.3f}, Error: {error_1:.3f}")

# Artan desen test et  
extra_test_2 = [
    np.array([[0.5], [0.0]]), 
    np.array([[0.7], [0.0]]), 
    np.array([[0.9], [0.0]]), 
    np.array([[1.1], [0.0]]), 
    np.array([[1.3], [0.0]])
]
expected_2 = 1.5
pred_2, _ = rnn_model.forward(extra_test_2)
error_2 = abs(pred_2 - expected_2)

extra_values_2 = [x[0][0] for x in extra_test_2]
extra_str_2 = ", ".join([f"{x:.1f}" for x in extra_values_2])

print(f"Test 2: {extra_str_2} -> Expected: {expected_2:.1f}, Prediction: {pred_2:.3f}, Error: {error_2:.3f}")

# Artan desen test et  
extra_test_3 = [
    np.array([[1.0], [0.0]]), 
    np.array([[1.4], [0.0]]), # +0.4
    np.array([[1.9], [0.0]]), # +0.5
    np.array([[2.3], [0.0]]), # +0.4
    np.array([[2.8], [0.0]])  # +0.5
]
expected_3 = 3.2
pred_3, _ = rnn_model.forward(extra_test_3)
error_3 = abs(pred_3 - expected_3)

extra_values_3 = [x[0][0] for x in extra_test_3]
extra_str_3 = ", ".join([f"{x:.1f}" for x in extra_values_3])

print(f"Test 3: {extra_str_3} -> Expected: {expected_3:.1f}, Prediction: {pred_3:.3f}, Error: {error_3:.3f}")

# Genel baÅŸarÄ± raporu
avg_error = np.mean([final_error, error_1, error_2, error_3])
print_title(f"ğŸ“Š General Success Report:", single_line=True)
print(f"ğŸ”¹ Average Error           : {avg_error:.3f}")
print(f"ğŸ”¹ 1st Test Success Level  : {calc_success_level(error_1, silent=True)}")
print(f"ğŸ”¹ 2nd Test Success Level  : {calc_success_level(error_2, silent=True)}")
print(f"ğŸ”¹ 3rd Test Success Level  : {calc_success_level(error_3, silent=True)}")

print(f"ğŸ”¹ Number of Tests        : 3")

if avg_error < 0.15:
    print("ğŸŒŸ RNN successfully learned the pattern and can generalize!")
elif avg_error < 0.3:
    print("ğŸ‘ RNN learned the pattern at a reasonable level")
else:
    print("ğŸ”§ RNN needs more training")

print(f"\nğŸ’¡ Training Details:")
print("-" * 25)
print(f"âœ…  Trained on {len(all_training_sequences)} different patterns")
print(f"âœ…  {epochs} epochs of training completed")
print(f"âœ…  Loss reduced by % {((training_losses[0]-training_losses[-1])/training_losses[0]*100):.0f}")
print(f"âœ…  Real backpropagation simulated")
print(f"âœ…  Multiple pattern recognition achieved")

