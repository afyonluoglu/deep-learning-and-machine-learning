"""
ğŸ”¤ VANILLA RNN - TEMEL RNN DETAYILI AÃ‡IKLAMA
===========================================

Bu dosya temel Vanilla RNN'lerin detaylÄ± implementasyonunu ve
limitasyonlarÄ±nÄ± aÃ§Ä±klar. LSTM ve GRU ile karÅŸÄ±laÅŸtÄ±rma yapar.

Vanilla RNN Ã–zellikleri:
1. En basit RNN tÃ¼rÃ¼
2. Hidden state sadece tanh aktivasyonu kullanÄ±r
3. Vanishing gradient problemi yaÅŸar
4. KÄ±sa vadeli baÄŸÄ±mlÄ±lÄ±klar iÃ§in uygun

Ã–ÄŸreneceÄŸiniz konular:
- Vanilla RNN mathematiksel formÃ¼lasyonu
- Manual implementation
- Limitasyonlar ve problemler
- LSTM/GRU ile performans karÅŸÄ±laÅŸtÄ±rmasÄ±
"""

from calendar import EPOCH
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import seaborn as sns
from datetime import datetime



def print_section(title, char="=", single_line:bool=False, width=55):
    title = title
    if not single_line:
        print(f"{char*width}")
    if char == "=":
        title = "ğŸ“‹ "+ title
    print(title)
    print(f"{char*width}")

print_section("ğŸ”¤ VANILLA RNN - TEMEL RNN DETAYILI AÃ‡IKLAMA", char="#", width=80)

print_section("VANILLA RNN MATEMATIKSEL TEMELLER")

print_section("ğŸ“ VANILLA RNN FORMÃœLLERI:", char="-", single_line=True, width=35)

print("h_t = tanh(W_hh * h_t-1 + W_xh * x_t + b_h)")
print("y_t = W_hy * h_t + b_y")
print("")
print("Burada:")
print("â€¢ h_t     : t anÄ±ndaki hidden state")
print("â€¢ x_t     : t anÄ±ndaki input")
print("â€¢ W_hh    : hidden-to-hidden weight matrix")
print("â€¢ W_xh    : input-to-hidden weight matrix") 
print("â€¢ W_hy    : hidden-to-output weight matrix")
print("â€¢ b_h, b_y: bias vektÃ¶rleri")

def manual_vanilla_rnn_step(x_t, h_prev, W_hh, W_xh, b_h):
    """
    Vanilla RNN adÄ±mÄ±nÄ± manuel olarak uygular
    """
    h_t = np.tanh(np.dot(W_hh, h_prev) + np.dot(W_xh, x_t) + b_h)
    return h_t

def demonstrate_manual_rnn():
    """Manuel RNN implementasyonunu gÃ¶sterir"""
    
    print_section("MANUEL RNN IMPLEMENTASYONU")
    
    # Parametreler
    hidden_size = 4
    input_size = 2
    sequence_length = 8
    
    # Rastgele aÄŸÄ±rlÄ±klar
    np.random.seed(42)
    W_hh = np.random.randn(hidden_size, hidden_size) * 0.1
    W_xh = np.random.randn(hidden_size, input_size) * 0.1
    W_hy = np.random.randn(1, hidden_size) * 0.1
    b_h = np.zeros((hidden_size, 1))
    b_y = np.zeros((1, 1))
    
    # Ã–rnek sequence
    sequence = []
    for i in range(sequence_length):
        x = np.array([[np.sin(i * 0.5)], [np.cos(i * 0.5)]])
        sequence.append(x)
    
    print(f"ğŸ”§ Parametreler:")
    print(f"   Hidden size     : {hidden_size}")
    print(f"   Input size      : {input_size}")
    print(f"   Sequence length : {sequence_length}")
    
    # RNN forward pass
    hidden_states = []
    outputs = []
    h = np.zeros((hidden_size, 1))
    
    print(f"\nğŸ”„ RNN Forward Pass:")
    for t, x_t in enumerate(sequence):
        h = manual_vanilla_rnn_step(x_t, h, W_hh, W_xh, b_h)
        y_t = np.dot(W_hy, h) + b_y
        
        hidden_states.append(h.copy())
        outputs.append(y_t.copy())
        
        x_t_formatted = ", ".join([f"{x:11.8f}" for x in x_t.flatten()])
        h_t_formatted = ", ".join([f"{h:11.8f}" for h in h.flatten()])
        y_t_formatted = f"{y_t[0,0]:.3f}"
        print(f"   t={t}: x_t=[{x_t_formatted}]   h_t=[{h_t_formatted}]    y_t=[{y_t_formatted}]")

    return np.array(hidden_states), np.array(outputs)

hidden_states, outputs = demonstrate_manual_rnn()

print_section("VANILLA RNN PROBLEMLERI")

def demonstrate_vanishing_gradient():
    """Vanishing gradient problemini gÃ¶sterir"""

    print_section("âš ï¸  VANISHING GRADIENT PROBLEM:", char="-", single_line=True, width=35)

    # FarklÄ± sequence uzunluklarÄ± test et
    sequence_lengths = [5, 10, 20, 50, 100]
    final_gradients = []
    
    for seq_len in sequence_lengths:
        # Basit toy problem
        np.random.seed(42)
        X = np.random.randn(1, seq_len, 1)
        y = np.array([[1.0]])  # Hedef
        
        # Vanilla RNN modeli
        model = Sequential([
            Input(shape=(seq_len, 1)),  # GiriÅŸ katmanÄ±
            SimpleRNN(10, activation='tanh'),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        print(f"Model compiled... {seq_len} sequence length")

        # Ä°lk aÄŸÄ±rlÄ±klarÄ± kaydet
        initial_weights = model.get_weights()
        
        # Bir adÄ±m gradient hesapla
        with tf.GradientTape() as tape:
            pred = model(X, training=True)
            loss = tf.keras.losses.mse(y, pred)
        
        gradients = tape.gradient(loss, model.trainable_variables)
        
        # Ä°lk katmanÄ±n gradientini al
        if gradients[0] is not None:
            grad_norm = tf.norm(gradients[0]).numpy()
            final_gradients.append(grad_norm)
        else:
            final_gradients.append(0.0)
    
    # GÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(sequence_lengths, final_gradients, 'ro-', linewidth=2, markersize=8)
    plt.title('Gradient Norm vs Sequence Length', fontweight='bold')
    plt.xlabel('Sequence Length')
    plt.ylabel('Gradient Norm')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)
    
    # Hidden state evrimi
    plt.subplot(2, 2, 2)
    for i in range(min(4, hidden_states.shape[2])):
        plt.plot(hidden_states[:, 0, i], label=f'Hidden {i+1}', linewidth=2)
    plt.title('Hidden State Evolution', fontweight='bold')
    plt.xlabel('Time Step')
    plt.ylabel('Hidden State Value')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Aktivasyon saturation analizi
    plt.subplot(2, 2, 3)
    tanh_input = np.linspace(-5, 5, 1000)
    tanh_output = np.tanh(tanh_input)
    tanh_derivative = 1 - tanh_output**2
    
    plt.plot(tanh_input, tanh_output, 'b-', label='tanh(x)', linewidth=2)
    plt.plot(tanh_input, tanh_derivative, 'r-', label="tanh'(x)", linewidth=2)
    plt.title('Tanh Activation & Derivative', fontweight='bold')
    plt.xlabel('Input')
    plt.ylabel('Output')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Problem illustration
    plt.subplot(2, 2, 4)
    problems = ['Vanishing\nGradient', 'Limited\nMemory', 'Slow\nTraining', 'Poor Long\nDependencies']
    severity = [0.9, 0.8, 0.6, 0.95]
    colors = ['red', 'orange', 'yellow', 'darkred']
    
    bars = plt.bar(problems, severity, color=colors, alpha=0.7)
    plt.title('Vanilla RNN Problems', fontweight='bold')
    plt.ylabel('Severity (0-1)')
    plt.ylim(0, 1)
    
    for bar, sev in zip(bars, severity):
        plt.text(bar.get_x() + bar.get_width()/2, sev + 0.02, 
                f'{sev:.1f}', ha='center', fontweight='bold')
    
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print("ğŸ“Š SonuÃ§lar:")
    print(f"   â€¢ KÄ±sa sequence (5): Gradient norm = {final_gradients[0]:.6f}")
    print(f"   â€¢ Uzun sequence (100): Gradient norm = {final_gradients[-1]:.6f}")
    print(f"   â€¢ Gradient azalma oranÄ±: {final_gradients[-1]/final_gradients[0]:.2e}")

demonstrate_vanishing_gradient()

print_section("VANILLA RNN vs LSTM vs GRU KARÅILAÅTIRMA")

def compare_rnn_types():
    """FarklÄ± RNN tÃ¼rlerini karÅŸÄ±laÅŸtÄ±rÄ±r"""
    
    # Veri oluÅŸtur
    print("ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma verisi oluÅŸturuluyor...")
    
    np.random.seed(42)
    seq_length = 50
    n_features = 1
    n_samples = 1000
    
    # Uzun vadeli baÄŸÄ±mlÄ±lÄ±k gerektiren veri
    X = []
    y = []
    
    for i in range(n_samples):
        # Ä°lk 10 deÄŸer Ã¶nemli sinyal iÃ§eriyor
        seq = np.random.randn(seq_length, n_features)
        important_signal = np.random.choice([1, -1]) * 2
        seq[:5] += important_signal  # Ä°lk 5 deÄŸere sinyal ekle
        
        # Hedef: Ä°lk 5 deÄŸerin ortalamasÄ±nÄ±n iÅŸareti
        target = 1 if np.mean(seq[:5]) > 0 else 0
        
        X.append(seq)
        y.append(target)
    
    X = np.array(X)
    y = np.array(y)
    
    # Train/test split
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"âœ… Veri hazÄ±rlandÄ±: {X.shape}, Hedef: {len(np.unique(y))} sÄ±nÄ±f")
    
    # Modeller
    models = {}
    histories = {}
    
    print("\nğŸ—ï¸ Modeller oluÅŸturuluyor ve eÄŸitiliyor...")
    EPOCHS_FOR_MODELS = 30
    LAYER_1_NEURONS = 32
    LAYER_2_NEURONS = 16
    ADAM_PARAMETER = 0.001
    BATCH_SIZE = 32
    ACTIVATION = "relu"
    OUT_ACTIVATION = "sigmoid"
    LOSS_ALGORITHM = "binary_crossentropy"
    METRICS = ["accuracy"]

    print(f"   â€¢ Epochs         : {EPOCHS_FOR_MODELS}")
    print(f"   â€¢ Layer 1 Neurons: {LAYER_1_NEURONS}")
    print(f"   â€¢ Layer 2 Neurons: {LAYER_2_NEURONS}")
    print(f"   â€¢ Adam Parameter : {ADAM_PARAMETER}")
    print(f"   â€¢ Batch Size     : {BATCH_SIZE}")
    print(f"   â€¢ Sequence Length: {seq_length}")
    print(f"   â€¢ Features       : {n_features}")
    print(f"   â€¢ Loss Algorithm : {LOSS_ALGORITHM}")
    print(f"   â€¢ Metrics        : {METRICS}")

    # Vanilla RNN
    print("   ğŸ“Š Vanilla RNN...")
    start = tf.timestamp()
    vanilla_rnn = Sequential([
        Input(shape=(seq_length, n_features)),         
        SimpleRNN(LAYER_1_NEURONS),
        Dense(LAYER_2_NEURONS, activation=ACTIVATION),
        Dense(1, activation=OUT_ACTIVATION)
    ])
    vanilla_rnn.compile(optimizer=Adam(ADAM_PARAMETER), loss=LOSS_ALGORITHM, metrics=METRICS)
    
    history_vanilla = vanilla_rnn.fit(X_train, y_train, 
                                     validation_data=(X_test, y_test),
                                     epochs=EPOCHS_FOR_MODELS, batch_size=BATCH_SIZE, verbose=0)
    
    models['Vanilla RNN'] = vanilla_rnn
    histories['Vanilla RNN'] = history_vanilla

    end_time = tf.timestamp()
    training_time = end_time - start
    print(f"   â€¢ EÄŸitim sÃ¼resi: {training_time:.2f} saniye")
    

    # LSTM
    print("   ğŸ“Š LSTM...")
    start = tf.timestamp()
    lstm = Sequential([
        Input(shape=(seq_length, n_features)),         
        LSTM(LAYER_1_NEURONS),
        Dense(LAYER_2_NEURONS, activation=ACTIVATION),
        Dense(1, activation=OUT_ACTIVATION)
    ])
    lstm.compile(optimizer=Adam(ADAM_PARAMETER), loss=LOSS_ALGORITHM, metrics=METRICS)
    
    history_lstm = lstm.fit(X_train, y_train,
                           validation_data=(X_test, y_test),
                           epochs=EPOCHS_FOR_MODELS, batch_size=BATCH_SIZE, verbose=0)
    
    models['LSTM'] = lstm
    histories['LSTM'] = history_lstm

    end_time = tf.timestamp()
    training_time = end_time - start
    print(f"   â€¢ EÄŸitim sÃ¼resi: {training_time:.2f} saniye")


    # GRU
    print("   ğŸ“Š GRU...")
    start = tf.timestamp()
    gru = Sequential([
        Input(shape=(seq_length, n_features)),         
        GRU(LAYER_1_NEURONS),
        Dense(LAYER_2_NEURONS, activation=ACTIVATION),
        Dense(1, activation=OUT_ACTIVATION)
    ])
    gru.compile(optimizer=Adam(ADAM_PARAMETER), loss=LOSS_ALGORITHM, metrics=METRICS)
    
    history_gru = gru.fit(X_train, y_train,
                         validation_data=(X_test, y_test),
                         epochs=EPOCHS_FOR_MODELS, batch_size=BATCH_SIZE, verbose=0)
    
    models['GRU'] = gru
    histories['GRU'] = history_gru

    end_time = tf.timestamp()
    training_time = end_time - start
    print(f"   â€¢ EÄŸitim sÃ¼resi: {training_time:.2f} saniye")


    print("âœ… TÃ¼m modeller eÄŸitildi!")
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
    plt.figure(figsize=(15, 10))
    
    # Loss karÅŸÄ±laÅŸtÄ±rmasÄ±
    plt.subplot(2, 3, 1)
    for name, history in histories.items():
        plt.plot(history.history['loss'], label=f'{name} Train', linewidth=2)
        plt.plot(history.history['val_loss'], '--', label=f'{name} Val', linewidth=2)
    plt.title('Training Loss Comparison', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Accuracy karÅŸÄ±laÅŸtÄ±rmasÄ±
    plt.subplot(2, 3, 2)
    for name, history in histories.items():
        plt.plot(history.history['accuracy'], label=f'{name} Train', linewidth=2)
        plt.plot(history.history['val_accuracy'], '--', label=f'{name} Val', linewidth=2)
    plt.title('Accuracy Comparison', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Final performans
    plt.subplot(2, 3, 3)
    final_accuracies = []
    model_names = []
    
    print("   â€¢ Final Validation Accuracy:")
    for name, history in histories.items():
        final_acc = history.history['val_accuracy'][-1]
        print(f"{name:>15} final validation accuracy: {final_acc:.4f}")
        final_accuracies.append(final_acc)
        model_names.append(name)
    
    bars = plt.bar(model_names, final_accuracies, 
                   color=['red', 'blue', 'green'], alpha=0.7)
    plt.title('Final Validation Accuracy', fontweight='bold')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)
    
    for bar, acc in zip(bars, final_accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, acc + 0.01, 
                f'{acc:.3f}', ha='center', fontweight='bold')
    
    plt.grid(True, alpha=0.3)

    print("   â€¢ Model Complexity:")
    # Model karmaÅŸÄ±klÄ±ÄŸÄ±
    plt.subplot(2, 3, 4)
    param_counts = []
    for name, model in models.items():
        param_counts.append(model.count_params())
        print(f"{name:>20} parameter count: {model.count_params():,}")
    
    bars = plt.bar(model_names, param_counts, 
                   color=['red', 'blue', 'green'], alpha=0.7)
    plt.title('Model Parameters', fontweight='bold')
    plt.ylabel('Parameter Count')
    
    for bar, count in zip(bars, param_counts):
        plt.text(bar.get_x() + bar.get_width()/2, count + max(param_counts)*0.01, 
                f'{count:,}', ha='center', fontweight='bold', rotation=45)
    
    plt.grid(True, alpha=0.3)
    
    # EÄŸitim sÃ¼resi (simulated)
    plt.subplot(2, 3, 5)
    training_times = [1.0, 3.2, 2.1]  # Relative times
    bars = plt.bar(model_names, training_times, 
                   color=['red', 'blue', 'green'], alpha=0.7)
    plt.title('Relative Training Time', fontweight='bold')
    plt.ylabel('Relative Time')
    
    for bar, time in zip(bars, training_times):
        plt.text(bar.get_x() + bar.get_width()/2, time + 0.05, 
                f'{time:.1f}x', ha='center', fontweight='bold')
    
    plt.grid(True, alpha=0.3)
    
    # Ã–zellik karÅŸÄ±laÅŸtÄ±rmasÄ±
    plt.subplot(2, 3, 6)
    features = ['Simple', 'Fast', 'Memory', 'Long Dep.', 'Stable']
    vanilla_scores = [1.0, 1.0, 0.3, 0.2, 0.4]
    lstm_scores = [0.6, 0.5, 1.0, 1.0, 0.9]
    gru_scores = [0.7, 0.7, 0.9, 0.9, 0.8]
    
    x = np.arange(len(features))
    width = 0.25
    
    plt.bar(x - width, vanilla_scores, width, label='Vanilla RNN', alpha=0.7, color='red')
    plt.bar(x, lstm_scores, width, label='LSTM', alpha=0.7, color='blue')
    plt.bar(x + width, gru_scores, width, label='GRU', alpha=0.7, color='green')
    
    plt.title('Feature Comparison', fontweight='bold')
    plt.xlabel('Features')
    plt.ylabel('Score (0-1)')
    plt.xticks(x, features, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Performans raporu
    print_section(f"ğŸ“Š PERFORMANS RAPORU:", char="-", single_line=True, width=35)
    
    for name, history in histories.items():
        final_loss = history.history['val_loss'][-1]
        final_acc = history.history['val_accuracy'][-1]
        param_count = models[name].count_params()
        
        print(f"\n{name}:")
        print(f"   Final Validation Loss    : {final_loss:.4f}")
        print(f"   Final Validation Accuracy: {final_acc:.4f}")
        print(f"   Parameters               : {param_count:,}")
    
    # En iyi modeli belirle
    best_model = max(histories.items(), key=lambda x: x[1].history['val_accuracy'][-1])
    print(f"\nğŸ† EN Ä°YÄ° MODEL: {best_model[0]}")
    print(f"   Accuracy: {best_model[1].history['val_accuracy'][-1]:.4f}")

compare_rnn_types()

print_section("VANILLA RNN KULLANIM Ã–NERÄ°LERÄ°")

def print_usage_guidelines():
    """Vanilla RNN kullanÄ±m Ã¶nerilerini yazdÄ±rÄ±r"""
    print("ğŸ’¡ VANILLA RNN NE ZAMAN KULLANILMALI?")
    print("-" * 40)
    print("âœ… UYGUN DURUMLAR:")
    print("   â€¢ KÄ±sa sequence'ler (< 10-20 adÄ±m)")
    print("   â€¢ Basit temporal pattern'ler")
    print("   â€¢ HÄ±zlÄ± prototyping")
    print("   â€¢ SÄ±nÄ±rlÄ± hesaplama kaynaklarÄ±")
    print("   â€¢ EÄŸitici/Ã¶ÄŸrenme amaÃ§lÄ±")

    print("\nâŒ UYGUN OLMAYAN DURUMLAR:")
    print("   â€¢ Uzun sequence'ler (> 20-30 adÄ±m)")
    print("   â€¢ KarmaÅŸÄ±k long-term dependencies")
    print("   â€¢ Ãœretim seviyesi uygulamalar")
    print("   â€¢ YÃ¼ksek accuracy gereken durumlar")

    print("\nğŸ”§ VANILLA RNN Ä°YÄ°LEÅTÄ°RME YÃ–NTEMLERÄ°:")
    print("   1. Gradient Clipping: Exploding gradient Ã¶nler")
    print("   2. Smaller Learning Rate: Daha stabil eÄŸitim")
    print("   3. Regularization: Dropout, L2 regularization")
    print("   4. Proper Weight Initialization: Xavier/He init")
    print("   5. Batch Normalization: Internal covariate shift")

print_usage_guidelines()

def demonstrate_improvements():
    """Vanilla RNN iyileÅŸtirmelerini gÃ¶sterir"""
    
    print_section("VANILLA RNN Ä°YÄ°LEÅTÄ°RME Ã–RNEÄÄ°")
    
    MAX_DATA_POINTS = 1000
    TIME_SERIES_LENGTH = 35  # Time series bÃ¼yÃ¼dÃ¼kÃ§e improved versiyon Ã§ok Ã¶ne geÃ§iyor
    LAYER_1_NEURONS = 20 
    LAYER_2_NEURONS = 10
    LOSS_ALGORITHM = "binary_crossentropy"
    OUTPUT_LAYER_ACTIVATION = "sigmoid"
    MODEL2_DROPOUT = 0.2
    MODEL_2_LEARNING_RATE = 0.005
    EPOCHS = 40

    print(f"   â€¢ Data Points          : {MAX_DATA_POINTS}")
    print(f"   â€¢ Time Series Length   : {TIME_SERIES_LENGTH}")
    print(f"   â€¢ Layer 1 Neurons      : {LAYER_1_NEURONS}")
    print(f"   â€¢ Layer 2 Neurons      : {LAYER_2_NEURONS}")
    print(f"   â€¢ Loss Algorithm       : {LOSS_ALGORITHM}")
    print(f"   â€¢ Output Layer Activation: {OUTPUT_LAYER_ACTIVATION}")
    print(f"   â€¢ Model 2 Dropout      : {MODEL2_DROPOUT}")
    print(f"   â€¢ Model 2 Learning Rate: {MODEL_2_LEARNING_RATE}")
    print(f"   â€¢ Epochs               : {EPOCHS}")

    # Veri hazÄ±rla
    np.random.seed(42)
    X = np.random.randn(MAX_DATA_POINTS, TIME_SERIES_LENGTH, 1)

    # Ã–rnek set Set
    y = np.sum(X[:, :5, 0], axis=1) > 0  # Ä°lk 5 adÄ±mÄ±n toplamÄ± = tÃ¼m data_pointlerin ilk 5 zaman verisinin 0. elemanlarÄ±nÄ±n toplamÄ±
    y = y.astype(int)   # Binary sÄ±nÄ±flandÄ±rma: TRUE ise 1, FALSE ise 0 dÃ¶ndÃ¼rÃ¼r
    # Burada y pozitif ise sonuÃ§ 1 negatif ise 0 olur

    # Daha karmaÅŸÄ±k bir Ã¶rnek: Ä°lk 3 ve son 3 deÄŸerin kombinasyonu
    y1 = np.sum(X[:, :3, 0], axis=1) > 0
    y2 = np.sum(X[:, -3:, 0], axis=1) > 0
    y = (y1 & y2).astype(int)  # Daha zor pattern
    
    print(f"âœ… Veri hazÄ±rlandÄ±: {X.shape}, Hedef: {len(np.unique(y))} sÄ±nÄ±f")

    # Basit Vanilla RNN
    basic_model = Sequential([
        Input(shape=(TIME_SERIES_LENGTH, 1)),
        SimpleRNN(LAYER_1_NEURONS),
        Dense(1, activation=OUTPUT_LAYER_ACTIVATION)
    ])
    basic_model.compile(optimizer='adam', loss=LOSS_ALGORITHM, metrics=['accuracy'])
            
    # Ä°yileÅŸtirilmiÅŸ Vanilla RNN
    improved_model = Sequential([
        Input(shape=(TIME_SERIES_LENGTH, 1)),
        SimpleRNN(LAYER_1_NEURONS, dropout=MODEL2_DROPOUT),
        Dense(LAYER_2_NEURONS, activation='relu'),
        Dropout(MODEL2_DROPOUT),
        Dense(1, activation=OUTPUT_LAYER_ACTIVATION)
    ])
    improved_model.compile(
        optimizer=Adam(learning_rate=MODEL_2_LEARNING_RATE, clipnorm=1.0),  # clipnorm: Gradient clipping
        loss=LOSS_ALGORITHM, 
        metrics=['accuracy']
    )
    
    print(f"Basic Model   : Input ({TIME_SERIES_LENGTH},1) -> SimpleRNN({LAYER_1_NEURONS}) -> Dense(1)")
    print(f"Improved Model: Input ({TIME_SERIES_LENGTH},1) -> SimpleRNN({LAYER_1_NEURONS}, dropout={MODEL2_DROPOUT}) -> Dense({LAYER_2_NEURONS}, relu) -> Dropout({MODEL2_DROPOUT}) -> Dense(1)")
    print("ğŸ“Š Modeller eÄŸitiliyor...")
    
    # EÄŸitim
    history_basic = basic_model.fit(X, y, epochs=EPOCHS, validation_split=0.2, verbose=0)
    history_improved = improved_model.fit(X, y, epochs=EPOCHS, validation_split=0.2, verbose=0)

    # KarÅŸÄ±laÅŸtÄ±rma
    plt.figure(figsize=(13, 6))

    plt.subplots_adjust(left=0.05, right=0.98, top=0.89, bottom=0.16)
    
    current_time = datetime.now()
    params_text = (
        f"{current_time.strftime('%d-%m-%Y  (%H:%M)')} "
        f"  Data Points: {MAX_DATA_POINTS}  |"
        f"  Time Series Length: {TIME_SERIES_LENGTH}  |"
        f"  Layer 1 Neurons: {LAYER_1_NEURONS}  |"
        f"  Layer 2 Neurons: {LAYER_2_NEURONS}  |"
        f"  Loss Algorithm: {LOSS_ALGORITHM}\n"
        f"  Output Layer Activation: {OUTPUT_LAYER_ACTIVATION}  |"
        f"  Model 2 Dropout: {MODEL2_DROPOUT}  |"
        f"  Model 2 Learning Rate: {MODEL_2_LEARNING_RATE}  |"
        f"  Epochs: {EPOCHS}"
    )
    plt.figtext(0.01, 0.02, params_text, ha='left', va='bottom', 
                fontsize=11, color='black', 
                bbox=dict(facecolor="#A9E5FF", alpha=0.9))
        
    plt.subplot(1, 2, 1)
    plt.plot(history_basic.history['loss'], label='Basic - Train', linewidth=2)
    plt.plot(history_basic.history['val_loss'], label='Basic - Val', linewidth=2)
    plt.plot(history_improved.history['loss'], '--', label='Improved - Train', linewidth=2)
    plt.plot(history_improved.history['val_loss'], '--', label='Improved - Val', linewidth=2)
    plt.title('Loss Comparison', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history_basic.history['accuracy'], label='Basic - Train', linewidth=2)
    plt.plot(history_basic.history['val_accuracy'], label='Basic - Val', linewidth=2)
    plt.plot(history_improved.history['accuracy'], '--', label='Improved - Train', linewidth=2)
    plt.plot(history_improved.history['val_accuracy'], '--', label='Improved - Val', linewidth=2)
    plt.title('Accuracy Comparison', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
#     plt.tight_layout()
    plt.show()
    
    # SonuÃ§larÄ± yazdÄ±r
    basic_final = history_basic.history['val_accuracy'][-1]
    improved_final = history_improved.history['val_accuracy'][-1]
    
    print(f"ğŸ“Š SONUÃ‡LAR:")
    print(f"   Basit Model         : {basic_final:.4f}")
    print(f"   Ä°yileÅŸtirilmiÅŸ Model: {improved_final:.4f}")
    print(f"   Ä°yileÅŸtirme         : {(improved_final - basic_final)*100:+.2f}%")

demonstrate_improvements()

def summary_and_results():
    """Ã–zet ve sonuÃ§larÄ± yazdÄ±rÄ±r"""
    print_section("Ã–ZET VE SONUÃ‡LAR")

    print("âœ… Bu Vanilla RNN modÃ¼lÃ¼nde Ã¶ÄŸrendikleriniz:")
    print("  1. ğŸ§®  Vanilla RNN matematiksel formÃ¼lasyonu")
    print("  2. ğŸ”§  Manuel RNN implementasyonu")
    print("  3. âš ï¸  Vanishing gradient problemi")
    print("  4. ğŸ“Š  LSTM/GRU ile performans karÅŸÄ±laÅŸtÄ±rmasÄ±")
    print("  5. ğŸ’¡  Vanilla RNN kullanÄ±m alanlarÄ±")
    print("  6. ğŸ”§  Ä°yileÅŸtirme teknikleri")

    print("\nğŸ’¡ Ana Ã§Ä±karÄ±mlar:")
    print("  â€¢ Vanilla RNN basit ama sÄ±nÄ±rlÄ±")
    print("  â€¢ KÄ±sa sequence'ler iÃ§in yeterli")
    print("  â€¢ LSTM/GRU uzun sequence'ler iÃ§in daha iyi")
    print("  â€¢ DoÄŸru tekniklerle iyileÅŸtirilebilir")

    print("\nğŸ“š Sonraki modÃ¼l: 06_gru_example.py")
    print("GRU'nun LSTM'e gÃ¶re avantajlarÄ±nÄ± gÃ¶receÄŸiz!")

summary_and_results()
