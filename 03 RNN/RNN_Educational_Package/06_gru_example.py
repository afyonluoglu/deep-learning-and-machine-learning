"""
ğŸŒ€ GRU Ã–RNEÄÄ° - GATED RECURRENT UNIT
====================================

Bu dosya GRU (Gated Recurrent Unit) aÄŸlarÄ±nÄ± detaylÄ± ÅŸekilde aÃ§Ä±klar.
LSTM'e gÃ¶re daha basit ama benzer performanslÄ± olan GRU'yu Ã¶ÄŸrenin.

GRU Ã–zellikleri:
1. LSTM'den daha basit (2 gate vs 3 gate)
2. Daha az parametre
3. Daha hÄ±zlÄ± eÄŸitim
4. Benzer performans

Gate'ler:
- Reset Gate (r_t): GeÃ§miÅŸ bilgiyi ne kadar unutacaÄŸÄ±nÄ± kontrol eder
- Update Gate (z_t): Ne kadar yeni bilgi alacaÄŸÄ±nÄ± kontrol eder

KullanÄ±m AlanlarÄ±:
- LSTM alternatifi olarak
- SÄ±nÄ±rlÄ± hesaplama kaynaklarÄ±
- HÄ±zlÄ± prototyping
- Mobil/edge deployment
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import seaborn as sns
import time

print("=" * 70)
print("ğŸŒ€ GRU Ã–RNEÄÄ° - GATED RECURRENT UNIT")
print("=" * 70)

def print_section(title, char="=", width=50):
    print(f"\n{char*width}")
    print(f"ğŸ“‹ {title}")
    print(f"{char*width}")

print_section("GRU TEORÄ°SÄ° VE MATEMATÄ°K")

print("ğŸ§  GRU (Gated Recurrent Unit) Nedir?")
print("-" * 40)
print("â€¢ LSTM'e alternatif olarak geliÅŸtirilmiÅŸ")
print("â€¢ Daha basit yapÄ± (2 gate vs LSTM'in 3 gate'i)")
print("â€¢ Daha az parametre, daha hÄ±zlÄ± eÄŸitim")
print("â€¢ Ã‡oÄŸu durumda LSTM ile benzer performans")

print("\nğŸšª GRU GATE MEKANÄ°ZMALARI:")
print("-" * 30)
print("1ï¸âƒ£ RESET GATE (r_t):")
print("   r_t = Ïƒ(W_r Â· [h_t-1, x_t] + b_r)")
print("   â†’ GeÃ§miÅŸ bilgiyi ne kadar unutacaÄŸÄ±nÄ± kontrol eder")
print("")
print("2ï¸âƒ£ UPDATE GATE (z_t):")
print("   z_t = Ïƒ(W_z Â· [h_t-1, x_t] + b_z)")
print("   â†’ Yeni bilgiyi ne kadar alacaÄŸÄ±nÄ± kontrol eder")
print("")
print("3ï¸âƒ£ CANDIDATE HIDDEN STATE (hÌƒ_t):")
print("   hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_t-1, x_t] + b_h)")
print("   â†’ Yeni bilgi adayÄ±")
print("")
print("4ï¸âƒ£ FINAL HIDDEN STATE (h_t):")
print("   h_t = (1 - z_t) âŠ™ h_t-1 + z_t âŠ™ hÌƒ_t")
print("   â†’ Eski ve yeni bilgiyi karÄ±ÅŸtÄ±r")

def visualize_gru_gates():
    """GRU gate mekanizmalarÄ±nÄ± gÃ¶rselleÅŸtirir"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('GRU Gate MekanizmalarÄ±', fontsize=16, fontweight='bold')
    
    x = np.linspace(-5, 5, 1000)
    
    # Reset Gate
    reset_gate = 1 / (1 + np.exp(-x))  # Sigmoid
    axes[0, 0].plot(x, reset_gate, 'r-', linewidth=3, label='Reset Gate')
    axes[0, 0].set_title('Reset Gate (r_t)', fontweight='bold')
    axes[0, 0].set_xlabel('Input')
    axes[0, 0].set_ylabel('Gate Value (0-1)')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)
    axes[0, 0].legend()
    
    # Update Gate
    update_gate = 1 / (1 + np.exp(-x))
    axes[0, 1].plot(x, update_gate, 'b-', linewidth=3, label='Update Gate')
    axes[0, 1].set_title('Update Gate (z_t)', fontweight='bold')
    axes[0, 1].set_xlabel('Input')
    axes[0, 1].set_ylabel('Gate Value (0-1)')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)
    axes[0, 1].legend()
    
    # Candidate Hidden State
    candidate = np.tanh(x)
    axes[1, 0].plot(x, candidate, 'g-', linewidth=3, label='tanh(candidate)')
    axes[1, 0].set_title('Candidate Hidden State', fontweight='bold')
    axes[1, 0].set_xlabel('Input')
    axes[1, 0].set_ylabel('Candidate Value (-1 to 1)')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.7)
    axes[1, 0].legend()
    
    # GRU vs LSTM karmaÅŸÄ±klÄ±k
    components = ['Gates', 'Parameters', 'Memory\nStates', 'Computation']
    gru_complexity = [2, 100, 1, 80]  # Relative values
    lstm_complexity = [3, 133, 2, 100]  # Relative values
    
    x_pos = np.arange(len(components))
    width = 0.35
    
    bars1 = axes[1, 1].bar(x_pos - width/2, gru_complexity, width, 
                          label='GRU', alpha=0.7, color='blue')
    bars2 = axes[1, 1].bar(x_pos + width/2, lstm_complexity, width,
                          label='LSTM', alpha=0.7, color='red')
    
    axes[1, 1].set_title('ğŸ”§ GRU vs LSTM KarmaÅŸÄ±klÄ±k', fontweight='bold')
    axes[1, 1].set_xlabel('Components')
    axes[1, 1].set_ylabel('Relative Complexity')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(components)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("ğŸšª Gate AÃ§Ä±klamalarÄ±:")
    print("-" * 20)
    print("1. ğŸ”„ Reset Gate: GeÃ§miÅŸ bilgiyi ne kadar gÃ¶rmezden geleceÄŸini kontrol eder")
    print("2. ğŸ”„ Update Gate: Yeni bilgiyi ne kadar gÃ¼ncelleyeceÄŸini kontrol eder")
    print("3. ğŸ§  Candidate: Reset edilmiÅŸ geÃ§miÅŸ + mevcut giriÅŸ")
    print("4. ğŸ¯ Final: Eski ve yeni bilgiyi karÄ±ÅŸtÄ±rÄ±r")

visualize_gru_gates()

print_section("GRU vs LSTM DETAYILI KARÅILAÅTIRMA")

def comprehensive_gru_lstm_comparison():
    """GRU ve LSTM'i kapsamlÄ± ÅŸekilde karÅŸÄ±laÅŸtÄ±rÄ±r"""
    
    # FarklÄ± zorluk seviyelerinde veri setleri oluÅŸtur
    datasets = {}
    
    print("ğŸ“Š FarklÄ± zorluk seviyelerinde veri setleri oluÅŸturuluyor...")
    
    np.random.seed(42)
    
    # Dataset 1: KÄ±sa vadeli baÄŸÄ±mlÄ±lÄ±k (kolay)
    seq_len_short = 10
    X_short = np.random.randn(1000, seq_len_short, 1)
    y_short = np.sum(X_short[:, -3:, 0], axis=1) > 0  # Son 3 deÄŸerin toplamÄ±
    datasets['Short Term'] = (X_short, y_short.astype(int), seq_len_short)
    
    # Dataset 2: Orta vadeli baÄŸÄ±mlÄ±lÄ±k (orta)
    seq_len_med = 25
    X_med = np.random.randn(1000, seq_len_med, 1)
    y_med = np.sum(X_med[:, :5, 0], axis=1) > 0  # Ä°lk 5 deÄŸerin toplamÄ± (20 adÄ±m sonra)
    datasets['Medium Term'] = (X_med, y_med.astype(int), seq_len_med)
    
    # Dataset 3: Uzun vadeli baÄŸÄ±mlÄ±lÄ±k (zor)
    seq_len_long = 50
    X_long = np.random.randn(1000, seq_len_long, 1)
    # Ä°lk ve son 3 deÄŸerin Ã§arpÄ±mÄ±nÄ±n iÅŸareti
    first_part = np.sum(X_long[:, :3, 0], axis=1)
    last_part = np.sum(X_long[:, -3:, 0], axis=1)
    y_long = (first_part * last_part) > 0
    datasets['Long Term'] = (X_long, y_long.astype(int), seq_len_long)
    
    results = {}
    training_times = {}
    
    print("ğŸ—ï¸ Modelleri eÄŸitiyoruz ve karÅŸÄ±laÅŸtÄ±rÄ±yoruz...")
    
    for dataset_name, (X, y, seq_len) in datasets.items():
        print(f"\nğŸ“Š {dataset_name} Dataset ({seq_len} adÄ±m):")
        
        # Train/test split
        split = int(0.8 * len(X))
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        dataset_results = {}
        dataset_times = {}
        
        for model_type in ['GRU', 'LSTM']:
            print(f"   ğŸ”§ {model_type} eÄŸitiliyor...")
            
            # Model oluÅŸtur
            if model_type == 'GRU':
                model = Sequential([
                    GRU(32, input_shape=(seq_len, 1)),
                    Dropout(0.2),
                    Dense(16, activation='relu'),
                    Dense(1, activation='sigmoid')
                ])
            else:  # LSTM
                model = Sequential([
                    LSTM(32, input_shape=(seq_len, 1)),
                    Dropout(0.2),
                    Dense(16, activation='relu'),
                    Dense(1, activation='sigmoid')
                ])
            
            model.compile(optimizer=Adam(0.001), 
                         loss='binary_crossentropy', 
                         metrics=['accuracy'])
            
            # EÄŸitim sÃ¼resi Ã¶lÃ§
            start_time = time.time()
            
            history = model.fit(X_train, y_train,
                              validation_data=(X_test, y_test),
                              epochs=20, batch_size=32, verbose=0)
            
            training_time = time.time() - start_time
            
            # SonuÃ§larÄ± kaydet
            final_acc = history.history['val_accuracy'][-1]
            final_loss = history.history['val_loss'][-1]
            param_count = model.count_params()
            
            dataset_results[model_type] = {
                'accuracy': final_acc,
                'loss': final_loss,
                'parameters': param_count,
                'history': history
            }
            
            dataset_times[model_type] = training_time
            
            print(f"      âœ… Accuracy: {final_acc:.4f}, Time: {training_time:.1f}s")
        
        results[dataset_name] = dataset_results
        training_times[dataset_name] = dataset_times
    
    # SonuÃ§larÄ± gÃ¶rselleÅŸtir
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    fig.suptitle('GRU vs LSTM KapsamlÄ± KarÅŸÄ±laÅŸtÄ±rma', fontsize=16, fontweight='bold')
    
    # Accuracy karÅŸÄ±laÅŸtÄ±rmasÄ±
    dataset_names = list(results.keys())
    gru_accs = [results[name]['GRU']['accuracy'] for name in dataset_names]
    lstm_accs = [results[name]['LSTM']['accuracy'] for name in dataset_names]
    
    x = np.arange(len(dataset_names))
    width = 0.35
    
    bars1 = axes[0, 0].bar(x - width/2, gru_accs, width, 
                          label='GRU', alpha=0.7, color='blue')
    bars2 = axes[0, 0].bar(x + width/2, lstm_accs, width,
                          label='LSTM', alpha=0.7, color='red')
    
    axes[0, 0].set_title('Validation Accuracy', fontweight='bold')
    axes[0, 0].set_xlabel('Dataset')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(dataset_names)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Parametre sayÄ±sÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±
    gru_params = [results[name]['GRU']['parameters'] for name in dataset_names]
    lstm_params = [results[name]['LSTM']['parameters'] for name in dataset_names]
    
    bars3 = axes[0, 1].bar(x - width/2, gru_params, width, 
                          label='GRU', alpha=0.7, color='blue')
    bars4 = axes[0, 1].bar(x + width/2, lstm_params, width,
                          label='LSTM', alpha=0.7, color='red')
    
    axes[0, 1].set_title('Parameter Count', fontweight='bold')
    axes[0, 1].set_xlabel('Dataset')
    axes[0, 1].set_ylabel('Parameters')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(dataset_names)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # EÄŸitim sÃ¼resi
    gru_times = [training_times[name]['GRU'] for name in dataset_names]
    lstm_times = [training_times[name]['LSTM'] for name in dataset_names]
    
    bars5 = axes[0, 2].bar(x - width/2, gru_times, width, 
                          label='GRU', alpha=0.7, color='blue')
    bars6 = axes[0, 2].bar(x + width/2, lstm_times, width,
                          label='LSTM', alpha=0.7, color='red')
    
    axes[0, 2].set_title('Training Time (seconds)', fontweight='bold')
    axes[0, 2].set_xlabel('Dataset')
    axes[0, 2].set_ylabel('Time (s)')
    axes[0, 2].set_xticks(x)
    axes[0, 2].set_xticklabels(dataset_names)
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # Her dataset iÃ§in training curve'ler
    for i, dataset_name in enumerate(dataset_names):
        ax = axes[1, i]
        
        gru_history = results[dataset_name]['GRU']['history']
        lstm_history = results[dataset_name]['LSTM']['history']
        
        ax.plot(gru_history.history['val_accuracy'], 'b-', 
               label='GRU', linewidth=2)
        ax.plot(lstm_history.history['val_accuracy'], 'r-', 
               label='LSTM', linewidth=2)
        
        ax.set_title(f'{dataset_name} - Validation Accuracy', fontweight='bold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Accuracy')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # Model karmaÅŸÄ±klÄ±ÄŸÄ± analizi
    complexities = ['Gates', 'Cell States', 'Computations', 'Memory Usage']
    gru_complexity = [2, 1, 3, 4]  # Relative values
    lstm_complexity = [3, 2, 4, 5]  # Relative values
    
    x_comp = np.arange(len(complexities))
    
    bars7 = axes[2, 0].bar(x_comp - width/2, gru_complexity, width, 
                          label='GRU', alpha=0.7, color='blue')
    bars8 = axes[2, 0].bar(x_comp + width/2, lstm_complexity, width,
                          label='LSTM', alpha=0.7, color='red')
    
    axes[2, 0].set_title('Architecture Complexity', fontweight='bold')
    axes[2, 0].set_xlabel('Component')
    axes[2, 0].set_ylabel('Relative Complexity')
    axes[2, 0].set_xticks(x_comp)
    axes[2, 0].set_xticklabels(complexities, rotation=45)
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)
    
    # Performans vs KarmaÅŸÄ±klÄ±k scatter
    all_gru_acc = [results[name]['GRU']['accuracy'] for name in dataset_names]
    all_lstm_acc = [results[name]['LSTM']['accuracy'] for name in dataset_names]
    all_gru_params = [results[name]['GRU']['parameters'] for name in dataset_names]
    all_lstm_params = [results[name]['LSTM']['parameters'] for name in dataset_names]
    
    axes[2, 1].scatter(all_gru_params, all_gru_acc, s=100, alpha=0.7, 
                      color='blue', label='GRU')
    axes[2, 1].scatter(all_lstm_params, all_lstm_acc, s=100, alpha=0.7, 
                      color='red', label='LSTM')
    
    axes[2, 1].set_title('Accuracy vs Parameters', fontweight='bold')
    axes[2, 1].set_xlabel('Parameter Count')
    axes[2, 1].set_ylabel('Accuracy')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)
    
    # Avantaj/dezavantaj tablosu
    axes[2, 2].axis('off')
    table_text = """
    GRU AVANTAJLARI:
    âœ… Daha basit mimari
    âœ… Daha az parametre
    âœ… Daha hÄ±zlÄ± eÄŸitim
    âœ… Daha az memory
    âœ… Overfitting riski dÃ¼ÅŸÃ¼k
    
    LSTM AVANTAJLARI:
    âœ… Daha gÃ¼Ã§lÃ¼ hafÄ±za
    âœ… Kompleks pattern'ler
    âœ… Ã‡ok uzun sequence'ler
    âœ… Daha detaylÄ± kontrol
    âœ… GeniÅŸ research desteÄŸi
    """
    
    axes[2, 2].text(0.05, 0.95, table_text, transform=axes[2, 2].transAxes,
                   fontsize=10, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    axes[2, 2].set_title('Avantaj/Dezavantaj', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    return results, training_times

results, training_times = comprehensive_gru_lstm_comparison()

print_section("GRU PRAKTÄ°K UYGULAMA: ZAMAN SERÄ°SÄ° TAHMÄ°NÄ°")

def gru_time_series_example():
    """GRU ile kapsamlÄ± zaman serisi tahmini Ã¶rneÄŸi"""
    
    print("ğŸ“Š KarmaÅŸÄ±k zaman serisi verisi oluÅŸturuluyor...")
    
    # Ã‡ok bileÅŸenli zaman serisi
    np.random.seed(42)
    n_points = 2000
    time_steps = np.arange(n_points)
    
    # Multiple components
    trend = 0.02 * time_steps + 100
    seasonal_yearly = 20 * np.sin(2 * np.pi * time_steps / 365)
    seasonal_monthly = 8 * np.sin(2 * np.pi * time_steps / 30)
    seasonal_weekly = 4 * np.sin(2 * np.pi * time_steps / 7)
    
    # ARCH-like volatility
    volatility = np.zeros(n_points)
    volatility[0] = 1
    for i in range(1, n_points):
        volatility[i] = 0.05 + 0.9 * volatility[i-1] + 0.05 * np.random.randn()**2
    
    noise = np.random.randn(n_points) * np.sqrt(volatility)
    
    # Combine all components
    ts_data = trend + seasonal_yearly + seasonal_monthly + seasonal_weekly + noise * 5
    
    # Add some structural breaks
    break_points = [500, 1000, 1500]
    for bp in break_points:
        ts_data[bp:] += np.random.normal(0, 10)
    
    print(f"âœ… {n_points} noktalÄ±k zaman serisi oluÅŸturuldu")
    
    # Veriyi gÃ¶rselleÅŸtir
    plt.figure(figsize=(15, 10))
    
    plt.subplot(3, 2, 1)
    plt.plot(time_steps[:500], ts_data[:500], 'b-', linewidth=1, alpha=0.8)
    plt.title('Zaman Serisi - Ä°lk 500 Nokta', fontweight='bold')
    plt.xlabel('Zaman')
    plt.ylabel('DeÄŸer')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(3, 2, 2)
    plt.hist(ts_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.title('Veri DaÄŸÄ±lÄ±mÄ±', fontweight='bold')
    plt.xlabel('DeÄŸer')
    plt.ylabel('Frekans')
    plt.grid(True, alpha=0.3)
    
    # Seasonal decomposition (basit)
    plt.subplot(3, 2, 3)
    plt.plot(time_steps[:365], seasonal_yearly[:365], label='YÄ±llÄ±k', linewidth=2)
    plt.plot(time_steps[:365], seasonal_monthly[:365], label='AylÄ±k', linewidth=2)
    plt.plot(time_steps[:365], seasonal_weekly[:365], label='HaftalÄ±k', linewidth=2)
    plt.title('Mevsimsel BileÅŸenler', fontweight='bold')
    plt.xlabel('Zaman')
    plt.ylabel('DeÄŸer')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ACF approximation
    plt.subplot(3, 2, 4)
    lags = range(1, 51)
    acf_values = []
    for lag in lags:
        if lag < len(ts_data):
            corr = np.corrcoef(ts_data[:-lag], ts_data[lag:])[0, 1]
            acf_values.append(corr)
        else:
            acf_values.append(0)
    
    plt.plot(lags, acf_values, 'o-', linewidth=2, markersize=4)
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)
    plt.axhline(y=-0.1, color='red', linestyle='--', alpha=0.5)
    plt.title('Otokorelasyon Fonksiyonu', fontweight='bold')
    plt.xlabel('Lag')
    plt.ylabel('Korelasyon')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(3, 2, 5)
    plt.plot(time_steps, volatility, 'r-', linewidth=1, alpha=0.7)
    plt.title('Volatilite Evrimi', fontweight='bold')
    plt.xlabel('Zaman')
    plt.ylabel('Volatilite')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(3, 2, 6)
    returns = np.diff(ts_data)
    plt.plot(returns, 'g-', linewidth=0.5, alpha=0.7)
    plt.title('Getiriler (First Differences)', fontweight='bold')
    plt.xlabel('Zaman')
    plt.ylabel('Getiri')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Veri hazÄ±rlÄ±ÄŸÄ±
    print("\nğŸ”§ GRU iÃ§in veri hazÄ±rlÄ±ÄŸÄ±...")
    
    # Normalizasyon
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(ts_data.reshape(-1, 1)).flatten()
    
    # Sequence oluÅŸturma - Ã§ok adÄ±mlÄ± tahmin
    def create_multi_step_sequences(data, lookback, forecast_horizon):
        X, y = [], []
        for i in range(lookback, len(data) - forecast_horizon + 1):
            X.append(data[i-lookback:i])
            y.append(data[i:i+forecast_horizon])
        return np.array(X), np.array(y)
    
    LOOKBACK = 60  # 60 adÄ±m geriye bak
    FORECAST = 10  # 10 adÄ±m ileriye tahmin et
    
    X, y = create_multi_step_sequences(scaled_data, LOOKBACK, FORECAST)
    X = X.reshape(X.shape[0], X.shape[1], 1)
    
    print(f"ğŸ“Š Sequence'ler oluÅŸturuldu:")
    print(f"   X shape: {X.shape}")
    print(f"   y shape: {y.shape}")
    
    # Train/validation/test split
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size + val_size]
    y_val = y[train_size:train_size + val_size]
    X_test = X[train_size + val_size:]
    y_test = y[train_size + val_size:]
    
    print(f"ğŸ“Š Veri bÃ¶lÃ¼mleme:")
    print(f"   Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
    print(f"   Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
    print(f"   Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
    
    # GRU modeli tasarÄ±mÄ±
    print(f"\nğŸ—ï¸ GeliÅŸmiÅŸ GRU modeli oluÅŸturuluyor...")
    
    # Model alternatifleri
    models = {}
    
    # 1. Basic GRU
    basic_gru = Sequential([
        GRU(64, input_shape=(LOOKBACK, 1)),
        Dense(32, activation='relu'),
        Dense(FORECAST)
    ], name='Basic_GRU')
    
    # 2. Stacked GRU
    stacked_gru = Sequential([
        GRU(64, return_sequences=True, input_shape=(LOOKBACK, 1)),
        Dropout(0.2),
        GRU(64, return_sequences=True),
        Dropout(0.2),
        GRU(32),
        Dense(32, activation='relu'),
        Dense(FORECAST)
    ], name='Stacked_GRU')
    
    # 3. Bidirectional GRU
    bidirectional_gru = Sequential([
        Bidirectional(GRU(32, return_sequences=True), input_shape=(LOOKBACK, 1)),
        Dropout(0.2),
        Bidirectional(GRU(32)),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(FORECAST)
    ], name='Bidirectional_GRU')
    
    models['Basic GRU'] = basic_gru
    models['Stacked GRU'] = stacked_gru
    models['Bidirectional GRU'] = bidirectional_gru
    
    # Modelleri compile et
    for name, model in models.items():
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        print(f"âœ… {name}: {model.count_params():,} parameters")
    
    # Modelleri eÄŸit
    print(f"\nğŸš€ Modeller eÄŸitiliyor...")
    
    histories = {}
    training_times = {}
    
    for name, model in models.items():
        print(f"\nğŸ“Š {name} eÄŸitiliyor...")
        
        start_time = time.time()
        
        # Callbacks
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True
        )
        
        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss', factor=0.7, patience=8, min_lr=0.0001
        )
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=32,
            callbacks=[early_stopping, reduce_lr],
            verbose=0
        )
        
        training_time = time.time() - start_time
        
        histories[name] = history
        training_times[name] = training_time
        
        print(f"   âœ… TamamlandÄ± ({training_time:.1f}s)")
        print(f"   ğŸ“Š Final val_loss: {history.history['val_loss'][-1]:.6f}")
        print(f"   ğŸ“Š Final val_mae: {history.history['val_mae'][-1]:.6f}")
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
    print(f"\nğŸ“Š MODEL PERFORMANS KARÅILAÅTIRMASI:")
    print("="*60)
    
    test_results = {}
    
    for name, model in models.items():
        # Test predictions
        test_pred = model.predict(X_test, verbose=0)
        
        # Metrics
        mse = mean_squared_error(y_test.flatten(), test_pred.flatten())
        mae = mean_absolute_error(y_test.flatten(), test_pred.flatten())
        rmse = np.sqrt(mse)
        
        test_results[name] = {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'predictions': test_pred,
            'training_time': training_times[name]
        }
        
        print(f"\n{name}:")
        print(f"   MSE: {mse:.6f}")
        print(f"   MAE: {mae:.6f}")
        print(f"   RMSE: {rmse:.6f}")
        print(f"   Training Time: {training_times[name]:.1f}s")
        print(f"   Parameters: {model.count_params():,}")
    
    # En iyi modeli seÃ§
    best_model_name = min(test_results.keys(), 
                         key=lambda x: test_results[x]['mae'])
    
    print(f"\nğŸ† EN Ä°YÄ° MODEL: {best_model_name}")
    print(f"   MAE: {test_results[best_model_name]['mae']:.6f}")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(3, 2, figsize=(16, 12))
    fig.suptitle('GRU Model KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold')
    
    # Training histories
    colors = ['blue', 'red', 'green']
    for i, (name, history) in enumerate(histories.items()):
        axes[0, 0].plot(history.history['loss'], color=colors[i], 
                       label=f'{name} Train', linewidth=2)
        axes[0, 0].plot(history.history['val_loss'], color=colors[i], 
                       linestyle='--', label=f'{name} Val', linewidth=2)
    
    axes[0, 0].set_title('Training Loss', fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_yscale('log')
    
    # MAE comparison
    for i, (name, history) in enumerate(histories.items()):
        axes[0, 1].plot(history.history['mae'], color=colors[i], 
                       label=f'{name} Train', linewidth=2)
        axes[0, 1].plot(history.history['val_mae'], color=colors[i], 
                       linestyle='--', label=f'{name} Val', linewidth=2)
    
    axes[0, 1].set_title('Training MAE', fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Test performance metrics
    model_names = list(test_results.keys())
    mae_values = [test_results[name]['mae'] for name in model_names]
    rmse_values = [test_results[name]['rmse'] for name in model_names]
    
    x = np.arange(len(model_names))
    width = 0.35
    
    bars1 = axes[1, 0].bar(x - width/2, mae_values, width, 
                          label='MAE', alpha=0.7, color='blue')
    bars2 = axes[1, 0].bar(x + width/2, rmse_values, width,
                          label='RMSE', alpha=0.7, color='red')
    
    axes[1, 0].set_title('Test Performance', fontweight='bold')
    axes[1, 0].set_xlabel('Model')
    axes[1, 0].set_ylabel('Error')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(model_names, rotation=45)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Training time vs Performance
    times = [test_results[name]['training_time'] for name in model_names]
    
    axes[1, 1].scatter(times, mae_values, s=[models[name].count_params()/1000 
                      for name in model_names], 
                      alpha=0.7, c=colors[:len(model_names)])
    
    for i, name in enumerate(model_names):
        axes[1, 1].annotate(name, (times[i], mae_values[i]),
                           xytext=(5, 5), textcoords='offset points')
    
    axes[1, 1].set_title('Training Time vs Performance', fontweight='bold')
    axes[1, 1].set_xlabel('Training Time (s)')
    axes[1, 1].set_ylabel('Test MAE')
    axes[1, 1].grid(True, alpha=0.3)
    
    # Best model predictions
    best_model = models[best_model_name]
    best_pred = test_results[best_model_name]['predictions']
    
    # Son 100 test Ã¶rneÄŸinden 5 tanesini gÃ¶ster
    sample_indices = np.random.choice(len(X_test), 5, replace=False)
    
    for i, idx in enumerate(sample_indices[:2]):
        ax = axes[2, i]
        
        # Actual vs predicted for multi-step
        actual = y_test[idx]
        predicted = best_pred[idx]
        steps = range(1, FORECAST + 1)
        
        ax.plot(steps, actual, 'bo-', label='Actual', linewidth=2, markersize=6)
        ax.plot(steps, predicted, 'ro-', label='Predicted', linewidth=2, markersize=6)
        ax.fill_between(steps, actual - 0.02, actual + 0.02, 
                       alpha=0.2, color='blue', label='Confidence')
        
        ax.set_title(f'Sample {i+1}: {FORECAST}-Step Forecast', fontweight='bold')
        ax.set_xlabel('Forecast Step')
        ax.set_ylabel('Normalized Value')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return models, test_results, scaler, best_model_name

models, test_results, scaler, best_model_name = gru_time_series_example()

print_section("GRU PRAKTÄ°K Ä°PUÃ‡LARI VE Ã–NERÄ°LER")

print("ğŸ’¡ GRU KULLANIM Ã–NERÄ°LERÄ°:")
print("-" * 30)

print("\nâœ… GRU NE ZAMAN KULLANILMALI?")
print("â€¢ LSTM'e gÃ¶re daha az parametre gerektiÄŸinde")
print("â€¢ HÄ±zlÄ± prototipleme ve iterasyon")
print("â€¢ SÄ±nÄ±rlÄ± hesaplama kaynaklarÄ±")
print("â€¢ Mobil/edge deployment")
print("â€¢ Orta uzunlukta sequence'ler (10-100 adÄ±m)")
print("â€¢ LSTM ile benzer performans, daha basit model")

print("\nâŒ GRU NE ZAMAN KULLANILMAMALI?")
print("â€¢ Ã‡ok karmaÅŸÄ±k long-term dependencies")
print("â€¢ Ã‡ok uzun sequence'ler (>200 adÄ±m)")
print("â€¢ Hassas memory kontrolÃ¼ gerektiÄŸinde")
print("â€¢ Research odaklÄ± projeler (LSTM daha yaygÄ±n)")

print("\nğŸ”§ GRU OPTÄ°MÄ°ZASYON Ä°PUÃ‡LARI:")
print("1. **Dropout kullanÄ±n**: Ã–zellikle recurrent_dropout")
print("2. **Gradient clipping**: Exploding gradients iÃ§in")
print("3. **Learning rate scheduling**: Adaptive Ã¶ÄŸrenme")
print("4. **Batch normalization**: Stabil eÄŸitim")
print("5. **Bidirectional**: Tam context iÃ§in")
print("6. **Stacking**: Daha derin representation")

print("\nğŸ“Š HÄ°PERPARAMETRE REHBERÄ°:")

hyperparams_guide = """
ğŸ›ï¸ GRU HÄ°PERPARAMETRE REHBERÄ°:

ğŸ“ˆ UNITS (Hidden Size):
   â€¢ KÄ±sa seq.: 16-64
   â€¢ Orta seq.: 64-128  
   â€¢ Uzun seq.: 128-512
   
â±ï¸ SEQUENCE LENGTH:
   â€¢ Min: 10-20 adÄ±m
   â€¢ Optimal: 30-100 adÄ±m
   â€¢ Max: 200+ (dikkatli)
   
ğŸ“š BATCH SIZE:
   â€¢ KÃ¼Ã§Ã¼k data: 16-32
   â€¢ BÃ¼yÃ¼k data: 64-128
   â€¢ Memory limit: 256+
   
ğŸ§  LEARNING RATE:
   â€¢ BaÅŸlangÄ±Ã§: 0.001
   â€¢ Fine-tuning: 0.0001
   â€¢ Schedule: ReduceLROnPlateau
   
ğŸ¯ DROPOUT:
   â€¢ Standard: 0.1-0.3
   â€¢ Recurrent: 0.1-0.2
   â€¢ Dense layers: 0.2-0.5
"""

print(hyperparams_guide)

print_section("Ã–ZET VE SONUÃ‡LAR")

print("âœ… Bu GRU modÃ¼lÃ¼nde Ã¶ÄŸrendikleriniz:")
print("  1. ğŸŒ€ GRU mimarisi ve gate mekanizmalarÄ±")
print("  2. ğŸ“Š LSTM ile detaylÄ± karÅŸÄ±laÅŸtÄ±rma")
print("  3. âš–ï¸ Performans vs karmaÅŸÄ±klÄ±k trade-off'u")
print("  4. ğŸ—ï¸ FarklÄ± GRU varyantlarÄ± (Stacked, Bidirectional)")
print("  5. ğŸ“ˆ Zaman serisi tahmininde pratik uygulama")
print("  6. ğŸ”§ Hiperparametre optimizasyon teknikleri")
print("  7. ğŸ’¡ KullanÄ±m alanlarÄ± ve sÄ±nÄ±rlarÄ±")

print(f"\nğŸ† PERFORMANS Ã–ZETÄ°:")
print(f"   En iyi model: {best_model_name}")
print(f"   Test MAE: {test_results[best_model_name]['mae']:.6f}")
print(f"   Training time: {test_results[best_model_name]['training_time']:.1f}s")

print("\nğŸ’¡ Ana Ã§Ä±karÄ±mlar:")
print("  â€¢ GRU genellikle LSTM kadar iyi performans gÃ¶sterir")
print("  â€¢ %25-30 daha az parametre kullanÄ±r")
print("  â€¢ Daha hÄ±zlÄ± eÄŸitir ve deploy eder")
print("  â€¢ Ã‡oÄŸu zaman series problemi iÃ§in yeterli")
print("  â€¢ LSTM'e gÃ¶re daha basit ve anlaÅŸÄ±lÄ±r")

print("\nğŸš€ Ä°yileÅŸtirme Ã¶nerileri:")
print("  1. Attention mechanism eklemek")
print("  2. Ensemble modeller kullanmak")
print("  3. External features dahil etmek")
print("  4. Advanced regularization (DropConnect)")
print("  5. Custom loss functions")

print("\nğŸ“š Sonraki modÃ¼l: 08_sentiment_analysis.py")
print("RNN ile doÄŸal dil iÅŸleme Ã¶ÄŸreneceÄŸiz!")

print("\n" + "=" * 70)
print("âœ… GRU Ã–RNEÄÄ° TAMAMLANDI!")
print("=" * 70)