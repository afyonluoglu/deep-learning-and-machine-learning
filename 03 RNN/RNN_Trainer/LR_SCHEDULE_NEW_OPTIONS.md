# Learning Rate Schedule Yeni SeÃ§enekler

## ğŸ“‹ Eklenen LR Schedule Stratejileri

### 1. **Reduce on Plateau**
KayÄ±p (loss) deÄŸeri dÃ¼zelmediÄŸinde learning rate'i azaltÄ±r.

#### Parametreler:
- **patience**: KaÃ§ epoch beklenecek (default: 10)
- **factor**: LR azaltma faktÃ¶rÃ¼ (default: 0.5)
- **min_lr**: Minimum learning rate (default: 1e-6)

#### Ã‡alÄ±ÅŸma Prensibi:
```python
if loss < best_loss:
    best_loss = loss
    wait = 0
else:
    wait += 1
    if wait >= patience:
        current_lr = max(current_lr * factor, min_lr)
        wait = 0
```

#### Ne Zaman KullanÄ±lÄ±r:
- Loss deÄŸeri platoya ulaÅŸtÄ±ÄŸÄ±nda
- Otomatik LR ayarlamasÄ± istendiÄŸinde
- Model Ã¶ÄŸrenmeyi yavaÅŸladÄ±ÄŸÄ±nda

### 2. **Cyclical LR (CLR)**
Learning rate'i periyodik olarak minimum ve maksimum deÄŸerler arasÄ±nda gidip geldirir.

#### Parametreler:
- **base_lr**: Minimum learning rate (default: initial_lr * 0.1)
- **max_lr**: Maximum learning rate (default: initial_lr)
- **step_size_cycle**: YarÄ±m dÃ¶ngÃ¼ adÄ±m sayÄ±sÄ± (default: 2000)

#### Ã‡alÄ±ÅŸma Prensibi:
```python
cycle = floor(1 + cycle_step / (2 * step_size_cycle))
x = abs(cycle_step / step_size_cycle - 2 * cycle + 1)
lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))
```

#### FormÃ¼l AÃ§Ä±klamasÄ±:
- Her dÃ¶ngÃ¼ `2 * step_size_cycle` adÄ±mdan oluÅŸur
- Ä°lk yarÄ±da: base_lr â†’ max_lr (artÄ±ÅŸ)
- Ä°kinci yarÄ±da: max_lr â†’ base_lr (azalÄ±ÅŸ)
- ÃœÃ§gensel dalga formu

#### Ne Zaman KullanÄ±lÄ±r:
- HÄ±zlÄ± yakÄ±nsama istendiÄŸinde
- Lokal minimumlarden kaÃ§mak iÃ§in
- Saddle point'lerden geÃ§mek iÃ§in

#### AvantajlarÄ±:
- Daha hÄ±zlÄ± eÄŸitim
- Daha iyi genelleme
- Lokal minimumlardan kaÃ§Ä±ÅŸ

### 3. **Warmup + Decay**
BaÅŸlangÄ±Ã§ta yavaÅŸ yavaÅŸ artÄ±rÄ±r (warmup), sonra Ã¼stel olarak azaltÄ±r (decay).

#### Parametreler:
- **warmup_steps**: Warmup adÄ±m sayÄ±sÄ± (default: 1000)
- **decay_steps**: Decay adÄ±m sayÄ±sÄ± (default: 10000)
- **min_lr**: Minimum learning rate (default: 1e-6)

#### Ã‡alÄ±ÅŸma Prensibi:
```python
# Warmup Phase (Linear)
if epoch < warmup_steps:
    lr = initial_lr * (epoch + 1) / warmup_steps

# Decay Phase (Exponential)
else:
    decay_rate = (min_lr / initial_lr) ** (1 / decay_steps)
    steps_after_warmup = epoch - warmup_steps
    lr = initial_lr * (decay_rate ** steps_after_warmup)
    lr = max(lr, min_lr)
```

#### Ä°ki AÅŸama:
1. **Warmup (IsÄ±nma)**: 
   - Linear olarak 0'dan initial_lr'ye
   - Modelin kararlÄ± baÅŸlamasÄ± iÃ§in
   - Warmup_steps kadar sÃ¼rer

2. **Decay (Azalma)**:
   - Ãœstel azalma
   - Min_lr'ye kadar iner
   - Decay_steps kadar sÃ¼rer

#### Ne Zaman KullanÄ±lÄ±r:
- Adam optimizer ile
- Transformer modellerde
- BÃ¼yÃ¼k batch size'larda
- KararlÄ± baÅŸlangÄ±Ã§ istendiÄŸinde

#### AvantajlarÄ±:
- KararlÄ± baÅŸlangÄ±Ã§
- Gradient explosion Ã¶nleme
- Daha iyi yakÄ±nsama

## ğŸ“Š Mevcut LR Schedule'lar (GÃ¼ncellendi)

### 1. Constant
- Sabit learning rate
- En basit yÃ¶ntem

### 2. Step
- Belirli adÄ±mlarda azaltma
- `step_size` her X epoch'ta
- `gamma` ile Ã§arpÄ±lÄ±r

### 3. Exponential
- Ãœstel azalma
- Her epoch'ta `gamma` ile Ã§arpÄ±lÄ±r
- SÃ¼rekli azalma

### 4. Cosine
- KosinÃ¼s fonksiyonu
- YumuÅŸak azalma
- T_max epoch'a kadar

## ğŸ”§ Kod DeÄŸiÅŸiklikleri

### 1. rnn_trainer_app.py
**Line ~217-221**: LR Schedule dropdown gÃ¼ncellendi
```python
lr_schedule_menu = ctk.CTkOptionMenu(
    frame,
    values=["constant", "step", "exponential", "cosine", 
            "reduce_on_plateau", "cyclical", "warmup_decay"],
    variable=self.lr_schedule_var
)
```

### 2. optimizers.py
**LearningRateScheduler SÄ±nÄ±fÄ± GÃ¼ncellendi:**

#### __init__ Metodu:
```python
# Yeni state deÄŸiÅŸkenleri eklendi
self.best_loss = float('inf')      # reduce_on_plateau iÃ§in
self.wait = 0                       # reduce_on_plateau iÃ§in
self.current_lr = initial_lr        # reduce_on_plateau iÃ§in
self.cycle_step = 0                 # cyclical iÃ§in
```

#### get_lr Metodu:
```python
def get_lr(self, epoch: int = None, loss: float = None) -> float:
    # loss parametresi eklendi (reduce_on_plateau iÃ§in)
    
    # 3 yeni schedule tipi eklendi:
    elif self.schedule_type == 'reduce_on_plateau':
        # ... kod ...
    
    elif self.schedule_type == 'cyclical':
        # ... kod ...
    
    elif self.schedule_type == 'warmup_decay':
        # ... kod ...
```

#### step Metodu:
```python
def step(self, loss: float = None):
    # loss parametresi eklendi
    self.current_epoch += 1
    self.cycle_step += 1  # cyclical iÃ§in
```

### 3. rnn_model.py
**Line ~375-380**: Scheduler Ã§aÄŸrÄ±larÄ± gÃ¼ncellendi
```python
# Ã–nceki:
self.optimizer.learning_rate = self.lr_scheduler.get_lr()
self.lr_scheduler.step()

# Yeni:
self.optimizer.learning_rate = self.lr_scheduler.get_lr(loss=avg_loss)
self.lr_scheduler.step(loss=avg_loss)
```

**Line ~48-50**: Docstring gÃ¼ncellendi
```python
lr_schedule: Learning rate schedule ('constant', 'step', 'exponential', 'cosine', 
            'reduce_on_plateau', 'cyclical', 'warmup_decay')
```

## ğŸ“ˆ KullanÄ±m Ã–rnekleri

### Reduce on Plateau
```python
# Model eÄŸitimi sÄ±rasÄ±nda
model = RNNModel(
    learning_rate=0.01,
    lr_schedule='reduce_on_plateau',
    patience=10,         # 10 epoch bekle
    factor=0.5,          # YarÄ±ya indir
    min_lr=1e-6          # En az bu kadar
)
```

**Senaryo:**
- BaÅŸlangÄ±Ã§: lr = 0.01
- 10 epoch boyunca loss dÃ¼zelmezse: lr = 0.005
- 10 epoch daha loss dÃ¼zelmezse: lr = 0.0025
- ... devam eder ...

### Cyclical LR
```python
model = RNNModel(
    learning_rate=0.01,
    lr_schedule='cyclical',
    base_lr=0.001,        # Minimum
    max_lr=0.01,          # Maximum
    step_size_cycle=2000  # 2000 adÄ±mda bir tam dÃ¶ngÃ¼
)
```

**Senaryo:**
- AdÄ±m 0-2000: 0.001 â†’ 0.01 (artÄ±ÅŸ)
- AdÄ±m 2000-4000: 0.01 â†’ 0.001 (azalÄ±ÅŸ)
- AdÄ±m 4000-6000: 0.001 â†’ 0.01 (tekrar)
- ... devam eder ...

### Warmup + Decay
```python
model = RNNModel(
    learning_rate=0.01,
    lr_schedule='warmup_decay',
    warmup_steps=1000,    # Ä°lk 1000 adÄ±m warmup
    decay_steps=10000,    # 10000 adÄ±mda decay
    min_lr=1e-6           # Minimum lr
)
```

**Senaryo:**
- AdÄ±m 0-1000: 0 â†’ 0.01 (linear artÄ±ÅŸ)
- AdÄ±m 1000-11000: 0.01 â†’ 0.000001 (Ã¼stel azalma)
- AdÄ±m 11000+: 0.000001 (sabit minimum)

## ğŸ¯ Hangi Schedule'u Ne Zaman KullanmalÄ±?

### Constant
âœ… **Kullan:**
- KÃ¼Ã§Ã¼k modellerde
- Basit problemlerde
- LR zaten iyi ayarlandÄ±ysa

### Step
âœ… **Kullan:**
- Geleneksel CNN/RNN'lerde
- Epoch sayÄ±sÄ± belli ise
- Manuel kontrol istenirse

### Exponential
âœ… **Kullan:**
- SÃ¼rekli azalan LR istenirse
- Transfer learning'de fine-tuning
- Uzun eÄŸitimlerde

### Cosine
âœ… **Kullan:**
- Modern deep learning'de
- YumuÅŸak azalma istenirse
- SOTA modellerde

### Reduce on Plateau â­ YENÄ°
âœ… **Kullan:**
- Loss platoya ulaÅŸtÄ±ÄŸÄ±nda
- Otomatik ayarlama istenirse
- Uzun eÄŸitim sÃ¼reÃ§lerinde
- Validation loss takip edilirse

âŒ **Kullanma:**
- Ã‡ok gÃ¼rÃ¼ltÃ¼lÃ¼ loss'larda
- KÄ±sa eÄŸitimlerde

### Cyclical LR â­ YENÄ°
âœ… **Kullan:**
- Lokal minimumlardan kaÃ§mak iÃ§in
- HÄ±zlÄ± yakÄ±nsama istenirse
- Saddle point problemlerinde
- ResNet, DenseNet gibi modellerde

âŒ **Kullanma:**
- Ã‡ok hassas eÄŸitimlerde
- Fine-tuning'de

### Warmup + Decay â­ YENÄ°
âœ… **Kullan:**
- Adam optimizer ile
- Transformer modellerde
- BÃ¼yÃ¼k batch size'larda
- BERT, GPT gibi modellerde

âŒ **Kullanma:**
- SGD optimizer ile
- KÃ¼Ã§Ã¼k batch size'larda

## ğŸ“Š LR Schedule KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Schedule | KarmaÅŸÄ±klÄ±k | Otomatik | HÄ±z | KararlÄ±lÄ±k | KullanÄ±m |
|----------|-------------|----------|-----|------------|----------|
| Constant | â­ | âŒ | â­â­ | â­â­â­ | Basit |
| Step | â­â­ | âŒ | â­â­â­ | â­â­â­ | Orta |
| Exponential | â­â­ | âŒ | â­â­â­ | â­â­ | Orta |
| Cosine | â­â­â­ | âŒ | â­â­â­â­ | â­â­â­ | Ä°leri |
| Reduce on Plateau | â­â­â­ | âœ… | â­â­â­ | â­â­â­â­ | Ä°leri |
| Cyclical | â­â­â­â­ | âŒ | â­â­â­â­â­ | â­â­ | Ä°leri |
| Warmup+Decay | â­â­â­â­ | âŒ | â­â­â­â­ | â­â­â­â­â­ | Ä°leri |

## âœ… Test SonuÃ§larÄ±

### BaÅŸarÄ±lÄ± Testler:
1. âœ… GUI'ye 3 yeni schedule eklendi
2. âœ… LearningRateScheduler sÄ±nÄ±fÄ± gÃ¼ncellendi
3. âœ… Reduce on Plateau implementasyonu
4. âœ… Cyclical LR implementasyonu
5. âœ… Warmup + Decay implementasyonu
6. âœ… Model eÄŸitimi sÄ±rasÄ±nda loss geÃ§iÅŸi
7. âœ… Program hatasÄ±z Ã§alÄ±ÅŸÄ±yor

### Test Edilen Ã–zellikler:
- Dropdown menÃ¼de tÃ¼m seÃ§enekler gÃ¶rÃ¼nÃ¼yor
- Model baÅŸarÄ±yla initialize ediliyor
- LR scheduler doÄŸru parametrelerle Ã§alÄ±ÅŸÄ±yor

## ğŸ“ Ã–nemli Notlar

### Reduce on Plateau:
- Loss deÄŸeri her epoch'ta geÃ§ilmeli
- `patience` deÄŸeri veri setine gÃ¶re ayarlanmalÄ±
- Ã‡ok kÃ¼Ã§Ã¼k `patience` â†’ Erken azalma
- Ã‡ok bÃ¼yÃ¼k `patience` â†’ GeÃ§ azalma

### Cyclical LR:
- `step_size_cycle` epoch sayÄ±sÄ±na gÃ¶re ayarlanmalÄ±
- Genellikle: total_epochs / 8 civarÄ±
- base_lr/max_lr oranÄ±: 1:10 veya 1:3 arasÄ±

### Warmup + Decay:
- `warmup_steps` genellikle total_steps'in %5-10'u
- Adam optimizer iÃ§in Ã¶nerilen
- BÃ¼yÃ¼k modellerde etkili

## ğŸ”„ Gelecek Ä°yileÅŸtirmeler (Opsiyonel)

1. **OneCycle Policy**: Cyclical'Ä±n geliÅŸmiÅŸ versiyonu
2. **Polynomial Decay**: Polinom fonksiyonu ile azalma
3. **Linear Schedule with Warmup**: BERT'te kullanÄ±lan
4. **Cosine with Restarts**: Periyodik restart'lar
5. **Custom Schedule**: KullanÄ±cÄ± tanÄ±mlÄ± fonksiyon

## ğŸ“š Referanslar

### Cyclical Learning Rates:
- Paper: "Cyclical Learning Rates for Training Neural Networks" (Leslie Smith, 2017)
- Fikir: LR'yi dÃ¼zenli olarak deÄŸiÅŸtirmek daha iyi sonuÃ§ verir

### Warmup:
- Paper: "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" (Goyal et al., 2017)
- KullanÄ±m: BERT, GPT, Transformer modellerde standart

### Reduce on Plateau:
- PyTorch: ReduceLROnPlateau
- Keras: ReduceLROnPlateau callback
- Otomatik LR ayarlamasÄ±

---

**Tamamlanma Tarihi**: 2025-10-01
**Durum**: âœ… BaÅŸarÄ±yla TamamlandÄ±
**Test Sonucu**: âœ… TÃ¼m Testler GeÃ§ti
**Yeni Ã–zellikler**: 3 yeni LR schedule stratejisi
