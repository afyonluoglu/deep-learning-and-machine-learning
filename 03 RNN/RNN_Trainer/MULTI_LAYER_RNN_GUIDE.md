# Ã‡ok KatmanlÄ± RNN (Multi-Layer/Stacked RNN) KullanÄ±m KÄ±lavuzu

## ğŸ¯ Genel BakÄ±ÅŸ

Bu programda artÄ±k **tek katmanlÄ±** ve **Ã§ok katmanlÄ± (stacked/deep)** RNN modelleri oluÅŸturabilirsiniz!

## ğŸ“Š Ã‡ok KatmanlÄ± RNN Nedir?

Ã‡ok katmanlÄ± RNN (Deep RNN veya Stacked RNN), birden fazla gizli katmanÄ±n Ã¼st Ã¼ste yÄ±ÄŸÄ±ldÄ±ÄŸÄ± bir yapÄ±dÄ±r:

```
Input â†’ Hidden Layer 1 â†’ Hidden Layer 2 â†’ ... â†’ Hidden Layer N â†’ Output
```

Her katman, bir Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ± girdi olarak alÄ±r ve daha soyut Ã¶zellikler Ã¶ÄŸrenir.

## ğŸ“ Neden Ã‡ok KatmanlÄ± RNN KullanmalÄ±?

### âœ… AvantajlarÄ±:

1. **Daha KarmaÅŸÄ±k Desenler**: Her katman farklÄ± soyutlama seviyelerinde Ã¶zellikler Ã¶ÄŸrenir
2. **HiyerarÅŸik Temsiller**: 
   - Alt katmanlar: Basit, lokal desenler
   - Ãœst katmanlar: KarmaÅŸÄ±k, global desenler
3. **Daha Ä°yi Performans**: KarmaÅŸÄ±k zaman serilerinde tek katmana gÃ¶re daha iyi sonuÃ§lar
4. **Zengin Ã–zellik Ã‡Ä±karÄ±mÄ±**: Her katman bilgiyi dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve zenginleÅŸtirir

### âš ï¸ DezavantajlarÄ±:

1. **Daha Fazla Hesaplama**: EÄŸitim sÃ¼resi artar
2. **Daha Fazla Veri Gereksinimi**: Daha fazla parametre = daha fazla eÄŸitim verisi
3. **Overfitting Riski**: KÃ¼Ã§Ã¼k veri setlerinde aÅŸÄ±rÄ± Ã¶ÄŸrenme riski
4. **Gradient Problemi**: Vanishing/exploding gradient sorunu daha ÅŸiddetli olabilir

## ğŸ› ï¸ Programda NasÄ±l KullanÄ±lÄ±r?

### 1. **Hidden Layers (Gizli Katman SayÄ±sÄ±)**

**Model Parameters** bÃ¶lÃ¼mÃ¼nde:
- **Slider**: 1-5 arasÄ± katman sayÄ±sÄ± seÃ§in
- **1**: Tek katmanlÄ± (klasik RNN)
- **2+**: Ã‡ok katmanlÄ± (deep RNN)

### 2. **Hidden Units (NÃ¶ron SayÄ±sÄ±)**

- TÃ¼m katmanlar iÃ§in varsayÄ±lan nÃ¶ron sayÄ±sÄ±
- Ã–rnek: 20 seÃ§erseniz, tÃ¼m katmanlar 20'ÅŸer nÃ¶ron iÃ§erir

### 3. **Layer Sizes (Katman BoyutlarÄ±)**

VirgÃ¼lle ayrÄ±lmÄ±ÅŸ her katmanÄ±n nÃ¶ron sayÄ±sÄ±nÄ± Ã¶zelleÅŸtirebilirsiniz:

**Ã–rnekler:**
```
30,20,10     â†’ 3 katmanlÄ±: 30 â†’ 20 â†’ 10
50,40,30,20  â†’ 4 katmanlÄ±: 50 â†’ 40 â†’ 30 â†’ 20
25,15        â†’ 2 katmanlÄ±: 25 â†’ 15
```

**Not**: BoÅŸ bÄ±rakÄ±rsanÄ±z, tÃ¼m katmanlar "Hidden Units" deÄŸerini kullanÄ±r.

## ğŸ“ˆ Ã–nerilen YapÄ±landÄ±rmalar

### BaÅŸlangÄ±Ã§ Seviyesi (HÄ±zlÄ± Test)
```
Layers: 1
Hidden Units: 20
```

### Orta Seviye (Ä°yi Performans)
```
Layers: 2
Layer Sizes: 30,20
Dropout: 0.2
Optimizer: adam
```

### Ä°leri Seviye (Maksimum Performans)
```
Layers: 3
Layer Sizes: 50,30,20
Dropout: 0.3
Optimizer: adam
Learning Rate: 0.001
LR Schedule: exponential
```

### Ã‡ok KarmaÅŸÄ±k Veriler
```
Layers: 4
Layer Sizes: 100,70,50,30
Dropout: 0.4
Optimizer: adam
Sequence Length: 30
```

## ğŸ”¬ FarklÄ± Katman YapÄ±larÄ±

### 1. Pyramid (Piramit) YapÄ±sÄ±
```
Layer Sizes: 50,40,30,20
```
- Her katman azalarak gider
- **KullanÄ±m**: En yaygÄ±n yapÄ±, Ã§oÄŸu durumda iyi Ã§alÄ±ÅŸÄ±r
- **Avantaj**: Bilgi sÄ±kÄ±ÅŸtÄ±rmasÄ± ve Ã¶zellik seÃ§imi

### 2. Uniform (EÅŸit) YapÄ±sÄ±
```
Layer Sizes: 30,30,30
```
- TÃ¼m katmanlar eÅŸit boyutta
- **KullanÄ±m**: Simetrik problemler
- **Avantaj**: Her katman aynÄ± kapasitede

### 3. Inverted Pyramid (Ters Piramit)
```
Layer Sizes: 20,30,40
```
- Her katman artarak gider
- **KullanÄ±m**: Ã–zellik zenginleÅŸtirme gerektiÄŸinde
- **Avantaj**: Bilgi geniÅŸletme

### 4. Bottleneck (DarboÄŸaz)
```
Layer Sizes: 40,10,40
```
- Ortada kÃ¼Ã§Ã¼k, yanlarda bÃ¼yÃ¼k
- **KullanÄ±m**: Autoencoder benzeri yapÄ±lar
- **Avantaj**: Boyut indirgeme

## ğŸ’¡ Ä°puÃ§larÄ±

### Katman SayÄ±sÄ± SeÃ§imi:
- **1 katman**: Basit, lineer-benzeri desenler
- **2 katman**: Orta karmaÅŸÄ±klÄ±kta zaman serileri (Ã¶nerilen baÅŸlangÄ±Ã§)
- **3 katman**: KarmaÅŸÄ±k, uzun vadeli baÄŸÄ±mlÄ±lÄ±klar
- **4-5 katman**: Ã‡ok karmaÅŸÄ±k, hiyerarÅŸik desenler (dikkatli kullanÄ±n!)

### NÃ¶ron SayÄ±sÄ± SeÃ§imi:
- **Az veri** (< 500 Ã¶rnek): 10-20 nÃ¶ron/katman
- **Orta veri** (500-2000 Ã¶rnek): 20-50 nÃ¶ron/katman
- **Ã‡ok veri** (> 2000 Ã¶rnek): 50-100 nÃ¶ron/katman

### Dropout ile Birlikte KullanÄ±m:
```
2-3 katman â†’ Dropout 0.2-0.3
4+ katman  â†’ Dropout 0.3-0.5
```

### Optimizer Ã–nerisi:
- **Adam**: Ã‡ok katmanlÄ± modeller iÃ§in en iyi seÃ§im
- **RMSprop**: Ä°yi alternatif
- **SGD/Momentum**: Daha yavaÅŸ ama bazen daha iyi sonuÃ§

## ğŸ“Š Model Bilgisi

**Model Info** butonuna tÄ±klayarak:
- Katman sayÄ±sÄ±nÄ±
- Her katmanÄ±n boyutunu
- Toplam parametre sayÄ±sÄ±nÄ±
- Mimari yapÄ±yÄ± gÃ¶rebilirsiniz

## ğŸ¯ Ã–rnek KullanÄ±m Senaryosu

### Senaryo: KarmaÅŸÄ±k sinÃ¼s dalgasÄ± tahmin etme

1. **Data Generation**
   - Wave Type: Mixed Waves
   - Samples: 1000
   - Frequency: 2.0
   - Noise: 0.1

2. **Model Parameters**
   - Layers: 3
   - Layer Sizes: 40,30,20
   - Activation: tanh
   - Dropout: 0.3
   - Optimizer: adam
   - LR Schedule: exponential

3. **Training**
   - Epochs: 50
   - Learning Rate: 0.01

4. **SonuÃ§**: Ã‡ok katmanlÄ± model, tek katmanlÄ± modelden daha dÃ¼ÅŸÃ¼k loss deÄŸerine ulaÅŸÄ±r!

## ğŸš€ Deneyler

AÅŸaÄŸÄ±daki deneyleri yaparak farkÄ± gÃ¶rebilirsiniz:

### Deney 1: Tek vs Ã‡ok Katman
1. Tek katman (20 nÃ¶ron) ile eÄŸitin
2. Ä°ki katman (30â†’20) ile eÄŸitin
3. Loss grafiklerini karÅŸÄ±laÅŸtÄ±rÄ±n

### Deney 2: Katman SayÄ±sÄ±nÄ±n Etkisi
1. 1, 2, 3, 4 katman ile ayrÄ± ayrÄ± eÄŸitin
2. Her birinin loss'unu ve eÄŸitim sÃ¼resini karÅŸÄ±laÅŸtÄ±rÄ±n

### Deney 3: FarklÄ± YapÄ±lar
1. Piramit: 50â†’30â†’10
2. EÅŸit: 30â†’30â†’30
3. Ters Piramit: 10â†’30â†’50
4. Hangisi daha iyi?

## âš™ï¸ Teknik Detaylar

### Forward Pass (Ä°leri Besleme):
```python
# Her katman iÃ§in
for layer in range(num_layers):
    if layer == 0:
        input = x  # Ä°lk katman: girdi verisi
    else:
        input = h[layer-1]  # Sonraki katmanlar: Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±
    
    h[layer] = activation(Wxh[layer] @ input + Whh[layer] @ h_prev[layer])
```

### Backward Pass (Geri YayÄ±lÄ±m):
```python
# Katmanlar ters sÄ±rada (sondan baÅŸa)
for layer in reversed(range(num_layers)):
    # Gradient hesaplama ve yayÄ±lÄ±m
    ...
```

## ğŸ” Sorun Giderme

### Problem: EÄŸitim Ã§ok yavaÅŸ
**Ã‡Ã¶zÃ¼m**: 
- Katman sayÄ±sÄ±nÄ± azaltÄ±n
- Her katmandaki nÃ¶ron sayÄ±sÄ±nÄ± azaltÄ±n
- Sequence length'i azaltÄ±n

### Problem: Overfitting (aÅŸÄ±rÄ± Ã¶ÄŸrenme)
**Ã‡Ã¶zÃ¼m**:
- Dropout'u artÄ±rÄ±n (0.3-0.5)
- Katman sayÄ±sÄ±nÄ± azaltÄ±n
- Daha fazla veri toplayÄ±n

### Problem: Underfitting (yetersiz Ã¶ÄŸrenme)
**Ã‡Ã¶zÃ¼m**:
- Katman sayÄ±sÄ±nÄ± artÄ±rÄ±n
- Her katmandaki nÃ¶ron sayÄ±sÄ±nÄ± artÄ±rÄ±n
- Daha fazla epoch eÄŸitin
- Learning rate'i artÄ±rÄ±n

### Problem: Gradient vanishing
**Ã‡Ã¶zÃ¼m**:
- ReLU aktivasyonu kullanÄ±n
- Learning rate'i artÄ±rÄ±n
- Katman sayÄ±sÄ±nÄ± azaltÄ±n
- Gradient clipping zaten aktif (Â±5)

## ğŸ“š Daha Fazla Bilgi

- **LSTM**: Gradient vanishing problemine Ã§Ã¶zÃ¼m
- **GRU**: LSTM'in daha basit versiyonu
- **Bidirectional RNN**: Ä°leri ve geri yÃ¶nlÃ¼ iÅŸleme
- **Attention Mechanism**: Ã–nemli bÃ¶lgelere odaklanma

---

**Ä°yi EÄŸitimler! ğŸš€**
