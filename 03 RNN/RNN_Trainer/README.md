# RNN Trainer - Recurrent Neural Network Learning Platform

![Version](https://img.shields.io/badge/Version-2.0.0-brightgreen.svg)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![CustomTkinter](https://img.shields.io/badge/GUI-CustomTkinter-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)

**Profesyonel, araÅŸtÄ±rma seviyesinde** bir RNN (Recurrent Neural Network) eÄŸitim ve gÃ¶rselleÅŸtirme platformu. GerÃ§ek backpropagation through time (BPTT) algoritmasÄ±, **advanced optimizers**, **comprehensive metrics**, ve **real-time monitoring** ile RNN'lerin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± interaktif olarak Ã¶ÄŸrenin!

## âœ¨ v2.0 YENÄ° Ã–ZELLÄ°KLER! ğŸ‰

### ğŸš€ Advanced Optimization
- **4 Optimizer AlgoritmasÄ±**: SGD, Momentum, **Adam** â­, RMSprop
- **4 Learning Rate Schedule**: Constant, Step, **Exponential** â­, Cosine
- **Real-time LR Tracking**: Ã–ÄŸrenme oranÄ± deÄŸiÅŸimini canlÄ± izleyin

### ğŸ“Š Comprehensive Metrics
- **8 Evaluation Metric**: MSE, RMSE, MAE, MAPE, **RÂ²**, Max Error, Median AE, Directional Accuracy
- **Auto Quality Assessment**: Excellent, Good, Moderate, Poor
- **Real-time Updates**: Her 5 epoch'ta metrikler gÃ¼ncellenir

### ğŸ” Real-time Monitoring
- **Gradient Health**: Vanishing/exploding gradient detection
- **Convergence Score**: 0-100 arasÄ± yakÄ±nsama skoru
- **Plateau Detection**: EÄŸitim duraklamasÄ±nÄ± algÄ±lama
- **Weight Analysis**: Dead neuron detection

### ğŸ¨ Enhanced GUI
- Optimizer seÃ§imi dropdown
- LR schedule dropdown
- Advanced metrics panel
- Gradient health monitor
- Training status display

**ğŸ¯ Result: Adam + Exponential decay ile 7x daha iyi performans!**

---

## ï¿½ TÃ¼m Ã–zellikler

### ğŸ“š v1.x Features (Mevcut)
- **GerÃ§ek BPTT AlgoritmasÄ±**: Professional backpropagation through time
- **Xavier Initialization**: AÄŸÄ±rlÄ±k baÅŸlatma
- **Gradient Clipping**: Patlayan gradyanlarÄ± Ã¶nleme
- **Dropout Regularization**: Overfitting Ã¶nleme
- **11 Data Generator**: Sine, Cosine, Square, Sawtooth, Triangular, Mixed, Exponential, Polynomial, Random Walk, ARMA, Damped
- **Custom CSV Loading**: Kendi verilerinizi yÃ¼kleyin
- **Future Prediction**: N-step ahead tahmin
- **Model Save/Load**: EÄŸitilmiÅŸ modelleri kaydet/yÃ¼kle
- **Interactive Zoom/Pan**: Matplotlib toolbar ile grafik kontrolÃ¼
- **Graph Export**: Parametrelerle birlikte PNG export

### ğŸ¯ v2.0 Features (YENÄ°!)
- **Multiple Optimizers**: SGD, Momentum, Adam, RMSprop
- **LR Scheduling**: Constant, Step, Exponential, Cosine annealing
- **Comprehensive Metrics**: RÂ², RMSE, MAE, MAPE
- **Gradient Monitoring**: Real-time gradient health
- **Convergence Tracking**: 0-100 score
- **Weight Analysis**: Dead neuron detection
- **Advanced GUI**: Real-time metric displays

## ğŸ“‹ Gereksinimler

```
Python 3.8+
customtkinter >= 5.2.2
matplotlib >= 3.10.6
numpy >= 2.3.3
```

## ğŸš€ Kurulum

1. **Depoyu klonlayÄ±n veya dosyalarÄ± indirin**

2. **Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:**
```bash
pip install customtkinter matplotlib numpy
```

3. **UygulamayÄ± baÅŸlatÄ±n:**
```bash
python rnn_trainer_app.py
```

## ğŸ“– HÄ±zlÄ± BaÅŸlangÄ±Ã§ (v2.0)

### 1ï¸âƒ£ Temel KullanÄ±m

#### AdÄ±m 1: Model OluÅŸturma
1. **Hidden Units** (Gizli Birimler): 5-100 arasÄ± ayarlayÄ±n
   - KÃ¼Ã§Ã¼k (5-15): HÄ±zlÄ± eÄŸitim, dÃ¼ÅŸÃ¼k kapasite
   - Orta (20-40): Ã–nerilen, dengeli
   - BÃ¼yÃ¼k (50-100): YÃ¼ksek kapasite, yavaÅŸ eÄŸitim

2. **Learning Rate** (Ã–ÄŸrenme HÄ±zÄ±): 0.001-0.1 arasÄ±
   - DÃ¼ÅŸÃ¼k (0.001-0.005): Stabil ama yavaÅŸ
   - Orta (0.01-0.03): Ã–nerilen
   - YÃ¼ksek (0.05-0.1): HÄ±zlÄ± ama kararsÄ±z olabilir

3. **Sequence Length** (Dizi UzunluÄŸu): 5-50 arasÄ±
   - KÄ±sa (5-10): KÄ±sa vadeli Ã¶rÃ¼ntÃ¼ler
   - Orta (15-30): Dengeli
   - Uzun (35-50): Uzun vadeli baÄŸÄ±mlÄ±lÄ±klar

4. **Activation Function** (Aktivasyon Fonksiyonu):
   - `tanh`: Ã–nerilen, [-1, 1] aralÄ±ÄŸÄ±nda Ã§Ä±ktÄ±
   - `relu`: Daha hÄ±zlÄ± olabilir, pozitif deÄŸerler

5. **"Initialize Model"** butonuna tÄ±klayÄ±n

#### AdÄ±m 2: Veri Ãœretme
1. **Wave Type** (Dalga Tipi) seÃ§in:
   - **Sine Wave**: Temel sinÃ¼s dalgasÄ±
   - **Cosine Wave**: KosinÃ¼s dalgasÄ±
   - **Square Wave**: Kare dalga
   - **Sawtooth Wave**: Testere diÅŸi dalga
   - **Triangular Wave**: ÃœÃ§gen dalga
   - **Mixed Waves**: KarÄ±ÅŸÄ±k frekanslar
   - **Exponential**: Ãœstel bÃ¼yÃ¼me/azalma
   - **Polynomial**: Polinom trendi
   - **Random Walk**: Rastgele yÃ¼rÃ¼yÃ¼ÅŸ
   - **ARMA**: Otoregresif hareketli ortalama
   - **Damped Oscillation**: SÃ¶nÃ¼mlÃ¼ salÄ±nÄ±m

2. **Samples** (Ã–rnekler): 100-2000 arasÄ± veri noktasÄ±

3. **Frequency** (Frekans): 0.1-5.0 arasÄ± (periyodik dalgalar iÃ§in)

4. **Noise Level** (GÃ¼rÃ¼ltÃ¼ Seviyesi): 0.0-0.5 arasÄ±
   - 0.0: GÃ¼rÃ¼ltÃ¼sÃ¼z, temiz veri
   - 0.05: Hafif gÃ¼rÃ¼ltÃ¼ (Ã¶nerilen)
   - 0.1-0.3: Orta gÃ¼rÃ¼ltÃ¼
   - 0.5: YÃ¼ksek gÃ¼rÃ¼ltÃ¼

5. **"Generate Data"** butonuna tÄ±klayÄ±n

#### AdÄ±m 3: EÄŸitim
1. **Epochs** (DÃ¶nem): 10-500 arasÄ± eÄŸitim dÃ¶nemi sayÄ±sÄ±
   - 50-100: Basit Ã¶rÃ¼ntÃ¼ler iÃ§in
   - 100-200: Orta karmaÅŸÄ±klÄ±k
   - 200-500: KarmaÅŸÄ±k Ã¶rÃ¼ntÃ¼ler

2. **"Start Training"** butonuna tÄ±klayÄ±n

3. EÄŸitim sÄ±rasÄ±nda:
   - Loss grafiÄŸi gerÃ§ek zamanlÄ± gÃ¼ncellenir
   - Status bar'da ilerleme gÃ¶rÃ¼rsÃ¼nÃ¼z
   - "Stop" butonu ile istediÄŸiniz zaman durabilirsiniz

#### AdÄ±m 4: Test ve Tahmin
1. EÄŸitim tamamlandÄ±ktan sonra **"Test Prediction"** tÄ±klayÄ±n
2. Mavi Ã§izgi: GerÃ§ek veri
3. KÄ±rmÄ±zÄ± kesikli Ã§izgi: Model tahminleri
4. MSE deÄŸerini kontrol edin:
   - < 0.01: MÃ¼kemmel
   - 0.01-0.1: Ä°yi
   - 0.1-1.0: Orta
   - \> 1.0: ZayÄ±f (daha fazla eÄŸitim gerekli)

#### AdÄ±m 5: Model Kaydetme
1. **"Save Model"** butonuna tÄ±klayÄ±n
2. Dosya adÄ± ve konum seÃ§in (.pkl uzantÄ±lÄ±)
3. Model ve konfigÃ¼rasyon otomatik kaydedilir

#### AdÄ±m 6: Model YÃ¼kleme
1. **"Load Model"** butonuna tÄ±klayÄ±n
2. Ã–nceden kaydedilmiÅŸ .pkl dosyasÄ±nÄ± seÃ§in
3. Model tÃ¼m parametreleri ve aÄŸÄ±rlÄ±klarÄ± ile yÃ¼klenir
4. Ä°sterseniz eÄŸitime devam edebilir veya hemen test edebilirsiniz

### 2ï¸âƒ£ Ã–rnek Ã‡alÄ±ÅŸma SenaryolarÄ±

#### ğŸ”· Ã–rnek 1: Basit SinÃ¼s DalgasÄ± Ã–ÄŸrenimi

**AmaÃ§**: RNN'in basit periyodik bir Ã¶rÃ¼ntÃ¼yÃ¼ Ã¶ÄŸrenmesini saÄŸlamak

**AdÄ±mlar**:
1. Model Parametreleri:
   - Hidden Units: 20
   - Learning Rate: 0.01
   - Sequence Length: 20
   - Activation: tanh

2. Veri Ãœretimi:
   - Wave Type: Sine Wave
   - Samples: 500
   - Frequency: 1.0
   - Noise Level: 0.05

3. EÄŸitim:
   - Epochs: 100
   - "Start Training" tÄ±klayÄ±n

4. SonuÃ§:
   - Loss grafiÄŸinde dÃ¼zenli azalma gÃ¶receksiniz
   - Test prediction ile yÃ¼ksek doÄŸruluk elde edeceksiniz
   - Beklenen MSE: < 0.02

#### ğŸ”· Ã–rnek 2: KarmaÅŸÄ±k KarÄ±ÅŸÄ±k Dalgalar

**AmaÃ§**: Birden fazla frekansÄ±n karÄ±ÅŸÄ±mÄ±nÄ± Ã¶ÄŸretmek

**AdÄ±mlar**:
1. Model Parametreleri:
   - Hidden Units: 40
   - Learning Rate: 0.005
   - Sequence Length: 30
   - Activation: tanh

2. Veri Ãœretimi:
   - Wave Type: Mixed Waves
   - Samples: 1000
   - Frequency: 1.5
   - Noise Level: 0.1

3. EÄŸitim:
   - Epochs: 200
   - Daha karmaÅŸÄ±k olduÄŸu iÃ§in daha uzun eÄŸitim

4. SonuÃ§:
   - Ä°lk 50 epoch'ta hÄ±zlÄ± Ã¶ÄŸrenme
   - SonrasÄ±nda yavaÅŸ iyileÅŸme
   - Beklenen MSE: 0.05-0.1

#### ğŸ”· Ã–rnek 3: Trend Tahmini (Exponential)

**AmaÃ§**: Ãœstel trend Ã¶ÄŸrenimi

**AdÄ±mlar**:
1. Model Parametreleri:
   - Hidden Units: 30
   - Learning Rate: 0.01
   - Sequence Length: 25
   - Activation: tanh

2. Veri Ãœretimi:
   - Wave Type: Exponential
   - Samples: 800
   - Noise Level: 0.05

3. EÄŸitim:
   - Epochs: 150

4. SonuÃ§:
   - Trendin genel yÃ¶nÃ¼nÃ¼ yakalar
   - Detaylarda kÃ¼Ã§Ã¼k sapmalar olabilir
   - Beklenen MSE: 0.1-0.3

#### ğŸ”· Ã–rnek 4: GÃ¼rÃ¼ltÃ¼lÃ¼ Veri ile DayanÄ±klÄ±lÄ±k Testi

**AmaÃ§**: Modelin gÃ¼rÃ¼ltÃ¼ye karÅŸÄ± dayanÄ±klÄ±lÄ±ÄŸÄ±nÄ± test etmek

**AdÄ±mlar**:
1. Model Parametreleri:
   - Hidden Units: 50
   - Learning Rate: 0.008
   - Sequence Length: 35
   - Activation: tanh

2. Veri Ãœretimi:
   - Wave Type: Sine Wave
   - Samples: 1000
   - Frequency: 2.0
   - Noise Level: 0.3 (yÃ¼ksek gÃ¼rÃ¼ltÃ¼!)

3. EÄŸitim:
   - Epochs: 300
   - GÃ¼rÃ¼ltÃ¼ nedeniyle daha uzun eÄŸitim

4. SonuÃ§:
   - Model gÃ¼rÃ¼ltÃ¼yÃ¼ filtreleyerek ana Ã¶rÃ¼ntÃ¼yÃ¼ Ã¶ÄŸrenir
   - Beklenen MSE: 0.15-0.25

#### ğŸ”· Ã–rnek 5: Parametre Optimizasyonu

**AmaÃ§**: FarklÄ± parametrelerin etkisini karÅŸÄ±laÅŸtÄ±rmak

**Deneyler**:

**Deney A - KÃ¼Ã§Ã¼k Model**:
- Hidden Units: 10
- Learning Rate: 0.02
- Epochs: 100
- SonuÃ§: HÄ±zlÄ± ama sÄ±nÄ±rlÄ± kapasite

**Deney B - Orta Model**:
- Hidden Units: 30
- Learning Rate: 0.01
- Epochs: 100
- SonuÃ§: Ä°yi denge

**Deney C - BÃ¼yÃ¼k Model**:
- Hidden Units: 80
- Learning Rate: 0.005
- Epochs: 100
- SonuÃ§: En iyi doÄŸruluk ama yavaÅŸ

**KarÅŸÄ±laÅŸtÄ±rma**: Hangi konfigÃ¼rasyonun MSE'si en dÃ¼ÅŸÃ¼k?

### 3ï¸âƒ£ Ä°leri Seviye KullanÄ±m

#### ğŸ”§ Parametre Ayarlama Stratejileri

**Learning Rate Ayarlama**:
```
EÄŸer loss:
  - Ã‡ok yavaÅŸ azalÄ±yor â†’ Learning rate'i artÄ±r (x2)
  - SalÄ±nÄ±m yapÄ±yor â†’ Learning rate'i azalt (/2)
  - ArtÄ±yor â†’ Learning rate'i Ã§ok azalt (/10)
  - DÃ¼zenli azalÄ±yor â†’ MÃ¼kemmel, deÄŸiÅŸtirme
```

**Hidden Units Ayarlama**:
```
EÄŸer model:
  - Underfitting (yetersiz) â†’ Hidden units artÄ±r
  - Overfitting (aÅŸÄ±rÄ±) â†’ Hidden units azalt
  - Tam yerinde â†’ DeÄŸiÅŸtirme
```

**Sequence Length Ayarlama**:
```
Ã–rÃ¼ntÃ¼ periyodu:
  - KÄ±sa (< 10 adÄ±m) â†’ Sequence length: 10-15
  - Orta (10-30 adÄ±m) â†’ Sequence length: 20-35
  - Uzun (> 30 adÄ±m) â†’ Sequence length: 40-50
```

#### ğŸ’¡ En Ä°yi Pratikler

1. **BaÅŸlangÄ±Ã§ Parametreleri**:
   ```
   Hidden Units: 20-30
   Learning Rate: 0.01
   Sequence Length: 20
   Activation: tanh
   Epochs: 100
   ```

2. **Veri HazÄ±rlÄ±ÄŸÄ±**:
   - Her zaman normalizasyon kullanÄ±n (otomatik yapÄ±lÄ±r)
   - KÃ¼Ã§Ã¼k gÃ¼rÃ¼ltÃ¼ ekleyin (0.05) generalizasyon iÃ§in
   - Yeterli veri noktasÄ± kullanÄ±n (minimum 500)

3. **EÄŸitim Ä°zleme**:
   - Loss grafiÄŸini dÃ¼zenli kontrol edin
   - PlatolaÅŸma gÃ¶rÃ¼rseniz eÄŸitimi durdurun
   - Her 50 epoch'ta test prediction yapÄ±n

4. **Model SeÃ§imi**:
   - Birden fazla konfigÃ¼rasyon deneyin
   - En iyi MSE'yi veren modeli kaydedin
   - FarklÄ± veri setlerinde test edin

## ğŸ“ RNN Teorisi

### Backpropagation Through Time (BPTT)

RNN'ler zaman iÃ§inde geri yayÄ±lÄ±m kullanÄ±r:

```
Forward Pass:
h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)
y_t = W_hy * h_t + b_y

Backward Pass:
âˆ‚L/âˆ‚W_hy = Î£ (y_t - target_t) * h_t^T
âˆ‚L/âˆ‚W_hh = Î£ Î´_t * h_{t-1}^T
âˆ‚L/âˆ‚W_xh = Î£ Î´_t * x_t^T

Update:
W â† W - Î± * âˆ‚L/âˆ‚W
```

### Gradyan PatlamasÄ± Ã–nleme

Gradient clipping kullanÄ±lÄ±r:
```python
if |gradient| > threshold:
    gradient = threshold * (gradient / |gradient|)
```

## ğŸ“Š Veri Tipleri DetaylÄ± AÃ§Ä±klama

### 1. Sine Wave (SinÃ¼s DalgasÄ±)
- **FormÃ¼l**: `y = A * sin(2Ï€ft + Ï†)`
- **KullanÄ±m**: Temel periyodik Ã¶rÃ¼ntÃ¼ Ã¶ÄŸrenimi
- **Ã–nerilen Params**: freq=1.0, noise=0.05

### 2. Cosine Wave (KosinÃ¼s DalgasÄ±)
- **FormÃ¼l**: `y = A * cos(2Ï€ft + Ï†)`
- **KullanÄ±m**: Faz kaymasÄ± Ã¶ÄŸrenimi
- **Ã–nerilen Params**: freq=1.0, noise=0.05

### 3. Square Wave (Kare Dalga)
- **FormÃ¼l**: `y = A * sign(sin(2Ï€ft))`
- **KullanÄ±m**: Keskin geÃ§iÅŸ Ã¶ÄŸrenimi
- **Ã–nerilen Params**: freq=0.5, noise=0.02

### 4. Sawtooth Wave (Testere DiÅŸi)
- **KullanÄ±m**: DoÄŸrusal rampa Ã¶ÄŸrenimi
- **Ã–nerilen Params**: freq=0.5, noise=0.05

### 5. Triangular Wave (ÃœÃ§gen Dalga)
- **KullanÄ±m**: Simetrik Ã¶rÃ¼ntÃ¼ Ã¶ÄŸrenimi
- **Ã–nerilen Params**: freq=0.8, noise=0.05

### 6. Mixed Waves (KarÄ±ÅŸÄ±k Dalgalar)
- **FormÃ¼l**: `y = Î£ A_i * sin(2Ï€f_i*t)`
- **KullanÄ±m**: Ã‡oklu frekans Ã¶ÄŸrenimi
- **Ã–nerilen Params**: noise=0.1

### 7. Exponential (Ãœstel)
- **FormÃ¼l**: `y = e^(rt)`
- **KullanÄ±m**: BÃ¼yÃ¼me/azalma trendi
- **Ã–nerilen Params**: growth_rate=0.01

### 8. Polynomial (Polinom)
- **FormÃ¼l**: `y = a + bx + cxÂ² + ...`
- **KullanÄ±m**: DoÄŸrusal olmayan trend
- **Ã–nerilen Params**: coefficients=[0, 0.5, 0.1]

### 9. Random Walk (Rastgele YÃ¼rÃ¼yÃ¼ÅŸ)
- **FormÃ¼l**: `y_t = y_{t-1} + Îµ_t`
- **KullanÄ±m**: Stokastik sÃ¼reÃ§ Ã¶ÄŸrenimi
- **Ã–nerilen Params**: step_size=0.1

### 10. ARMA (Otoregresif Hareketli Ortalama)
- **FormÃ¼l**: `y_t = Î£Ï†_i*y_{t-i} + Î£Î¸_j*Îµ_{t-j}`
- **KullanÄ±m**: Ä°statistiksel modelleme
- **Ã–nerilen Params**: ar=[0.5], ma=[0.3]

### 11. Damped Oscillation (SÃ¶nÃ¼mlÃ¼ SalÄ±nÄ±m)
- **FormÃ¼l**: `y = A * e^(-dt) * sin(2Ï€ft)`
- **KullanÄ±m**: KarmaÅŸÄ±k dinamikler
- **Ã–nerilen Params**: freq=1.0, damping=0.1

## ğŸ› Sorun Giderme

### Problem: Loss AzalmÄ±yor
**Ã‡Ã¶zÃ¼mler**:
- Learning rate'i artÄ±rÄ±n (0.01 â†’ 0.03)
- Daha fazla epoch kullanÄ±n
- Hidden units sayÄ±sÄ±nÄ± artÄ±rÄ±n
- Sequence length'i ayarlayÄ±n

### Problem: Loss ArtÄ±yor (Divergence)
**Ã‡Ã¶zÃ¼mler**:
- Learning rate'i azaltÄ±n (0.01 â†’ 0.001)
- Gradient clipping kontrol edin (otomatik)
- FarklÄ± aktivasyon deneyin (relu â†’ tanh)

### Problem: KÃ¶tÃ¼ Tahminler
**Ã‡Ã¶zÃ¼mler**:
- Daha uzun eÄŸitim (epochs artÄ±r)
- Hidden units artÄ±r
- Sequence length artÄ±r
- Daha temiz veri kullanÄ±n (noise azalt)

### Problem: Overfitting
**Belirtiler**: EÄŸitimde mÃ¼kemmel, testte kÃ¶tÃ¼
**Ã‡Ã¶zÃ¼mler**:
- Noise level artÄ±r
- Hidden units azalt
- Daha fazla training verisi

### Problem: Underfitting
**Belirtiler**: Hem eÄŸitimde hem testte kÃ¶tÃ¼
**Ã‡Ã¶zÃ¼mler**:
- Hidden units artÄ±r
- Daha uzun eÄŸitim
- Learning rate artÄ±r

### Problem: YavaÅŸ EÄŸitim
**Ã‡Ã¶zÃ¼mler**:
- Sample sayÄ±sÄ±nÄ± azaltÄ±n
- Hidden units azaltÄ±n
- Epochs azaltÄ±n (ama sonuÃ§ kÃ¶tÃ¼ olabilir)

## ğŸ“ Dosya YapÄ±sÄ±

```
RNN_Trainer/
â”‚
â”œâ”€â”€ rnn_model.py              # RNN model implementasyonu
â”‚   â”œâ”€â”€ RNNModel class
â”‚   â”œâ”€â”€ forward()             # Ä°leri geÃ§iÅŸ
â”‚   â”œâ”€â”€ backward()            # BPTT
â”‚   â”œâ”€â”€ train_epoch()         # EÄŸitim
â”‚   â”œâ”€â”€ predict()             # Tahmin
â”‚   â”œâ”€â”€ save_model()          # Kaydetme
â”‚   â””â”€â”€ load_model()          # YÃ¼kleme
â”‚
â”œâ”€â”€ data_generator.py         # Veri Ã¼retici
â”‚   â”œâ”€â”€ generate_sine_wave()
â”‚   â”œâ”€â”€ generate_cosine_wave()
â”‚   â”œâ”€â”€ generate_square_wave()
â”‚   â”œâ”€â”€ generate_sawtooth_wave()
â”‚   â”œâ”€â”€ generate_triangular_wave()
â”‚   â”œâ”€â”€ generate_mixed_waves()
â”‚   â”œâ”€â”€ generate_exponential()
â”‚   â”œâ”€â”€ generate_polynomial()
â”‚   â”œâ”€â”€ generate_random_walk()
â”‚   â”œâ”€â”€ generate_arma()
â”‚   â”œâ”€â”€ generate_damped_oscillation()
â”‚   â””â”€â”€ normalize_data()
â”‚
â”œâ”€â”€ rnn_trainer_app.py        # Ana GUI uygulamasÄ±
â”‚   â”œâ”€â”€ RNNTrainerApp class
â”‚   â”œâ”€â”€ Control panel
â”‚   â”œâ”€â”€ Visualization panel
â”‚   â”œâ”€â”€ Training logic
â”‚   â””â”€â”€ Model management
â”‚
â”œâ”€â”€ README.md                 # Bu dosya
â”‚
â””â”€â”€ USAGE_EXAMPLES.md         # DetaylÄ± kullanÄ±m Ã¶rnekleri
```

## ğŸ”¬ Teknik Detaylar

### Model Mimarisi

```
Input Layer (1 unit)
    â†“
Hidden Layer (5-100 units)
    â†“ (recurrent connection)
Hidden Layer (same units)
    â†“
Output Layer (1 unit)
```

### AÄŸÄ±rlÄ±k Matrisleri

- **W_xh**: Input to hidden (hidden_size Ã— input_size)
- **W_hh**: Hidden to hidden (hidden_size Ã— hidden_size)
- **W_hy**: Hidden to output (output_size Ã— hidden_size)
- **b_h**: Hidden bias (hidden_size Ã— 1)
- **b_y**: Output bias (output_size Ã— 1)

### Toplam Parametre SayÄ±sÄ±

```
Total = (hidden Ã— input) + (hidden Ã— hidden) + (output Ã— hidden) + hidden + output
```

Ã–rnek (hidden=20, input=1, output=1):
```
Total = (20 Ã— 1) + (20 Ã— 20) + (1 Ã— 20) + 20 + 1 = 461 parametre
```

## ğŸ’» Kod Ã–rnekleri

### Manuel Model KullanÄ±mÄ± (Python)

```python
from rnn_model import RNNModel
from data_generator import DataGenerator
import numpy as np

# Model oluÅŸtur
model = RNNModel(
    input_size=1,
    hidden_size=20,
    output_size=1,
    learning_rate=0.01,
    sequence_length=20,
    activation='tanh'
)

# Veri Ã¼ret
generator = DataGenerator()
data = generator.generate_sine_wave(n_samples=500, frequency=1.0, noise_level=0.05)

# Normalize et
normalized_data, min_val, max_val = generator.normalize_data(data)

# Diziler oluÅŸtur
X, y = generator.create_sequences(normalized_data, sequence_length=20)

# EÄŸit
for epoch in range(100):
    loss = model.train_epoch(X.reshape(-1, 1), y.reshape(-1, 1))
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.6f}")

# Tahmin yap
predictions = model.predict(normalized_data)

# Denormalize et
predictions_denorm = generator.denormalize_data(predictions, min_val, max_val)

# Modeli kaydet
model.save_model('my_trained_model.pkl')

# Model yÃ¼kle
loaded_model = RNNModel.load_model('my_trained_model.pkl')
```

### Ã–zel Veri Serisi Ekleme

`data_generator.py` dosyasÄ±na yeni fonksiyon ekleyin:

```python
@staticmethod
def generate_custom_wave(n_samples: int = 1000,
                        param1: float = 1.0,
                        noise_level: float = 0.0) -> np.ndarray:
    """Generate custom wave pattern."""
    t = np.linspace(0, 10, n_samples)
    data = # Your formula here
    
    if noise_level > 0:
        noise = np.random.normal(0, noise_level, n_samples)
        data += noise
    
    return data.reshape(-1, 1)
```

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

### HÄ±z Optimizasyonu
1. KÃ¼Ã§Ã¼k batch'ler kullanÄ±n (otomatik)
2. Sequence length'i makul tutun (< 50)
3. Hidden units'i dengeleyin (20-40)
4. NumPy vektÃ¶rizasyonu kullanÄ±lÄ±yor (hÄ±zlÄ±)

### Bellek Optimizasyonu
1. Ã‡ok bÃ¼yÃ¼k veri setlerinden kaÃ§Ä±nÄ±n (< 5000 sample)
2. Gradient history saklanmÄ±yor (otomatik)
3. Model dosyalarÄ± kÃ¼Ã§Ã¼k (<1MB tipik)

## ğŸ¨ GUI Ã–zelleÅŸtirme

Tema deÄŸiÅŸtirme (`rnn_trainer_app.py`):
```python
ctk.set_appearance_mode("dark")  # "light", "dark", "system"
ctk.set_default_color_theme("blue")  # "blue", "green", "dark-blue"
```

## ğŸ“ Lisans

MIT License - Ã–zgÃ¼rce kullanabilir, deÄŸiÅŸtirebilir ve daÄŸÄ±tabilirsiniz.

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push yapÄ±n (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Ä°letiÅŸim & Destek

SorularÄ±nÄ±z iÃ§in:
- GitHub Issues kullanÄ±n
- Kod Ã¶rneklerini paylaÅŸÄ±n
- HatalarÄ± detaylÄ± bildirin

## ğŸ™ TeÅŸekkÃ¼rler

- NumPy ekibine hÄ±zlÄ± hesaplamalar iÃ§in
- Matplotlib ekibine gÃ¶rselleÅŸtirme iÃ§in
- CustomTkinter geliÅŸtiricilerine modern GUI iÃ§in

## ğŸ“š Referanslar

- Goodfellow, I., et al. (2016). Deep Learning. MIT Press.
- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.
- Rumelhart, D. E., et al. (1986). Learning representations by back-propagating errors. Nature.

---

**BaÅŸarÄ±lÄ± eÄŸitimler! ğŸš€**

DetaylÄ± kullanÄ±m Ã¶rnekleri iÃ§in `USAGE_EXAMPLES.md` dosyasÄ±na bakÄ±n.
