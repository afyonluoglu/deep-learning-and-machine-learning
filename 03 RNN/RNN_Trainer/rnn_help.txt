RNN TRAINER - HELP & DOCUMENTATION (v1.1)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“– OVERVIEW
This application allows you to train and visualize Recurrent Neural Networks (RNN) 
with real backpropagation through time (BPTT) algorithm.

ğŸ†• NEW in v1.1:
   â€¢ Dropout Regularization for preventing overfitting
   â€¢ Custom CSV data loading
   â€¢ Future value prediction
   â€¢ Auto-update panel when loading models

ğŸš€ QUICK START GUIDE

1. INITIALIZE MODEL:
   - Set Hidden Units (5-100): Number of neurons in hidden layer
   - Set Learning Rate (0.001-0.1): Speed of learning
   - Set Sequence Length (5-50): Length of input sequences
   - Choose Activation Function: tanh or relu
   - Set Dropout Rate (0.0-0.9): Regularization strength [NEW!]
   - Click "Initialize Model"

2. GENERATE DATA:
   - Select Wave Type (11 different types available)
   - Set Number of Samples (100-2000)
   - Adjust Frequency (0.1-5.0)
   - Set Noise Level (0.0-0.5)
   - Click "Generate Data"
   
   OR LOAD CUSTOM DATA: [NEW!]
   - Click "Load CSV Data"
   - Select your CSV file (at least 10 values)
   - Data will be visualized automatically

3. TRAIN MODEL:
   - Set Number of Epochs (10-500)
   - Click "Start Training"
   - Watch real-time loss decrease
   - Click "Stop" to interrupt training

4. TEST PREDICTIONS:
   - Click "Test Prediction" after training
   - Compare actual vs predicted values
   - Check MSE (Mean Squared Error)
   
   OR PREDICT FUTURE: [NEW!]
   - Click "Predict Future Values"
   - Enter number of steps (e.g., 5, 10, 50)
   - See future predictions on graph

5. SAVE/LOAD MODEL:
   - Save trained model for later use
   - Load previous models to continue training
   - Configuration is saved automatically
   - Panel updates automatically when loading [FIXED!]

ğŸ”§ PARAMETER GUIDE

HIDDEN UNITS:
  â€¢ Small (5-15): Fast training, less capacity
  â€¢ Medium (20-40): Good balance
  â€¢ Large (50-100): Better accuracy, slower training

LEARNING RATE:
  â€¢ Low (0.001-0.005): Stable but slow
  â€¢ Medium (0.01-0.03): Recommended
  â€¢ High (0.05-0.1): Fast but may diverge

DROPOUT RATE: [NEW!]
  â€¢ 0.0: No regularization (default)
  â€¢ 0.1-0.3: Light regularization (recommended for start)
  â€¢ 0.4-0.6: Medium regularization (good for overfitting)
  â€¢ 0.7-0.9: Strong regularization (use with caution)

SEQUENCE LENGTH:
  â€¢ Short (5-10): Learns short-term patterns
  â€¢ Medium (15-30): Balanced
  â€¢ Long (35-50): Learns long-term dependencies

ACTIVATION FUNCTIONS:
  â€¢ tanh: Smooth, outputs in [-1, 1], recommended
  â€¢ relu: Non-saturating, can be faster

ğŸ“Š DATA TYPES

1. SINE WAVE: Smooth periodic oscillation
   Best for: Learning basic periodicity

2. COSINE WAVE: Phase-shifted sine wave
   Best for: Testing phase learning

3. SQUARE WAVE: Sharp transitions
   Best for: Testing edge detection

4. SAWTOOTH WAVE: Linear ramps
   Best for: Learning linear trends

5. TRIANGULAR WAVE: Symmetric triangular pattern
   Best for: Testing symmetry learning

6. MIXED WAVES: Combination of frequencies
   Best for: Complex pattern recognition

7. EXPONENTIAL: Growth/decay patterns
   Best for: Trend prediction

8. POLYNOMIAL: Non-linear trends
   Best for: Complex trend learning

9. RANDOM WALK: Stochastic process
   Best for: Testing generalization

10. ARMA: AutoRegressive Moving Average
    Best for: Statistical modeling

11. DAMPED OSCILLATION: Decaying oscillation
    Best for: Complex dynamics

ğŸ’¡ TRAINING TIPS

1. Start with simple data (sine wave)
2. Use medium parameters initially
3. Train for at least 50 epochs
4. Watch loss decrease steadily
5. If loss increases, reduce learning rate
6. If training is too slow, increase learning rate
7. More hidden units = more capacity but slower
8. Add noise to improve generalization

ğŸ¯ EXAMPLE WORKFLOWS

EXAMPLE 1 - Basic Sine Wave Prediction:
  1. Hidden Units: 20
  2. Learning Rate: 0.01
  3. Sequence Length: 20
  4. Generate Sine Wave (500 samples, freq=1.0, noise=0.05)
  5. Train for 100 epochs
  6. Test prediction

EXAMPLE 2 - Complex Pattern:
  1. Hidden Units: 40
  2. Learning Rate: 0.005
  3. Sequence Length: 30
  4. Generate Mixed Waves (1000 samples, noise=0.1)
  5. Train for 200 epochs
  6. Test prediction

EXAMPLE 3 - Trend Learning:
  1. Hidden Units: 30
  2. Learning Rate: 0.01
  3. Sequence Length: 25
  4. Generate Exponential (800 samples, noise=0.05)
  5. Train for 150 epochs
  6. Test prediction

ğŸ“ˆ INTERPRETING RESULTS

LOSS PLOT:
  â€¢ Should decrease over time
  â€¢ Log scale shows relative improvements
  â€¢ Plateaus indicate convergence
  â€¢ Spikes may indicate learning issues

PREDICTION PLOT:
  â€¢ Blue line = Actual data
  â€¢ Red dashed = Model predictions
  â€¢ Closer alignment = better performance
  â€¢ Check both trends and details

MSE (Mean Squared Error):
  â€¢ Lower is better
  â€¢ < 0.01: Excellent
  â€¢ 0.01-0.1: Good
  â€¢ 0.1-1.0: Fair
  â€¢ > 1.0: Poor (needs more training or parameter tuning)

âš ï¸ TROUBLESHOOTING

PROBLEM: Loss not decreasing
SOLUTION: Increase learning rate or train longer

PROBLEM: Loss exploding (increasing)
SOLUTION: Decrease learning rate

PROBLEM: Poor predictions
SOLUTION: Increase hidden units or train longer

PROBLEM: Training too slow
SOLUTION: Reduce samples, hidden units, or epochs

PROBLEM: Model overfitting (perfect on train, poor on test)
SOLUTION: Add more noise, reduce hidden units

ğŸ”¬ ADVANCED FEATURES

1. PARAMETER EXPERIMENTATION:
   - Change parameters during training
   - Compare different configurations
   - Find optimal settings

2. MODEL PERSISTENCE:
   - Save best performing models
   - Load and continue training
   - Share models with others

3. CUSTOM DATA: [NEW!]
   - Load your own CSV files
   - Predict future values
   - Use real-world data for training

ğŸ”® FUTURE PREDICTION GUIDE [NEW!]

HOW IT WORKS:
  1. Load custom CSV data (or use generated data)
  2. Model uses last 'sequence_length' values as seed
  3. Predicts next N values step by step
  4. Each prediction becomes input for next step

EXAMPLE WORKFLOW:
  Step 1: Prepare CSV with 30 temperature values
  Step 2: Train model with similar pattern (sine wave)
  Step 3: Load CSV with "Load CSV Data"
  Step 4: Click "Predict Future Values"
  Step 5: Enter 7 (predict next week)
  Step 6: View predictions on graph

CSV FORMAT:
  Temperature
  15.2
  16.8
  18.5
  ...
  
  Rules:
  â€¢ First row = header (optional, will be skipped)
  â€¢ One number per row
  â€¢ Minimum 10 values
  â€¢ Comma or dot decimal separator

TIPS:
  â€¢ More historical data = better predictions
  â€¢ Train with similar pattern types
  â€¢ Short-term predictions (5-10 steps) are more accurate
  â€¢ Long-term predictions (100+ steps) accumulate error

ğŸ“š TECHNICAL DETAILS

The RNN uses:
  â€¢ Backpropagation Through Time (BPTT)
  â€¢ Gradient clipping (prevents exploding gradients)
  â€¢ Xavier weight initialization
  â€¢ Mean squared error loss
  â€¢ Gradient descent optimization
  â€¢ Inverted Dropout for regularization [NEW!]

Formula:
  h_t = activation(W_xh * x_t + W_hh * h_{t-1} + b_h)
  
  [With Dropout during training]:
  h_t = h_t * dropout_mask / (1 - dropout_rate)
  
  y_t = W_hy * h_t + b_y

Dropout Mechanism:
  â€¢ Training: Random units dropped with probability p
  â€¢ Inference: All units active, no scaling needed
  â€¢ Prevents co-adaptation of neurons
  â€¢ Improves generalization

ğŸ’¾ FILE FORMATS

Models are saved as:
  â€¢ .pkl: Model weights and parameters (includes dropout_rate)
  â€¢ _config.json: Data normalization info

CSV files:
  â€¢ .csv or .txt: Time series data
  â€¢ One value per line
  â€¢ First line can be header

Both files needed for complete model restoration.

â“ FAQ

Q: How long should I train?
A: Until loss stabilizes (usually 50-200 epochs)

Q: What's the best learning rate?
A: Start with 0.01, adjust based on loss behavior

Q: Can I train on custom data?
A: Yes, modify data_generator.py to add custom functions

Q: Why use normalization?
A: Improves training stability and speed

Q: What's BPTT?
A: Backpropagation Through Time - extends backprop to sequences

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For more information, see README.md
