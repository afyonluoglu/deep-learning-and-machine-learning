# ğŸ“ PROJE TAMAMLANDI - Self-Attention Ã–ÄŸrenme AracÄ±

## âœ… Tamamlanan Ã–zellikler

### ğŸ¯ Ana Ã–zellikler

#### 1. Ä°nteraktif EÄŸitim Sistemi
- âœ… GerÃ§ek zamanlÄ± Self-Attention eÄŸitimi
- âœ… Query, Key, Value kavramlarÄ±nÄ±n gÃ¶rsel Ã¶ÄŸrenimi
- âœ… Multi-Head Attention desteÄŸi
- âœ… Positional Encoding implementasyonu
- âœ… Transformer bloklarÄ± (Self-Attention + FFN)
- âœ… Layer Normalization ve Residual Connections

#### 2. Parametrik Kontrol Sistemi
- âœ… d_model (Embedding Boyutu): 32-512 arasÄ± slider
- âœ… num_heads (Head SayÄ±sÄ±): 1-16 arasÄ± slider
- âœ… dropout (Dropout OranÄ±): 0.0-0.5 arasÄ± slider
- âœ… learning_rate (Ã–ÄŸrenme HÄ±zÄ±): 0.0001-0.01 arasÄ± slider
- âœ… Epoch sayÄ±sÄ± kontrolÃ¼
- âœ… Batch size ayarÄ±

#### 3. Veri YÃ¶netimi
- âœ… 5 farklÄ± Ã¶rnek veri seti:
  - Kelime Dizisi
  - CÃ¼mle Analizi
  - Zaman Serisi
  - GÃ¶rÃ¼ntÃ¼ ParÃ§alarÄ±
  - Ã–zel Veri
- âœ… Ã–zel veri giriÅŸi
- âœ… Otomatik vocabulary oluÅŸturma
- âœ… Token-to-index dÃ¶nÃ¼ÅŸÃ¼mÃ¼

#### 4. GÃ¶rselleÅŸtirme Sistemi
- âœ… **Attention Map**: IsÄ± haritasÄ± ile token iliÅŸkileri
- âœ… **Q, K, V Matrisleri**: VektÃ¶r temsilleri gÃ¶rselleÅŸtirme
- âœ… **EÄŸitim GrafiÄŸi**: Loss deÄŸiÅŸiminin gerÃ§ek zamanlÄ± takibi
- âœ… **AÃ§Ä±klama Paneli**: Ä°nteraktif Ã¶ÄŸrenme materyali
- âœ… KaranlÄ±k tema (koyu arka plan)
- âœ… YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ PNG export (150 DPI)
- âœ… Otomatik kaydetme (outputs/ klasÃ¶rÃ¼)

#### 5. Model YÃ¶netimi
- âœ… Model kaydetme (tam model + aÄŸÄ±rlÄ±klar)
- âœ… Model yÃ¼kleme
- âœ… KonfigÃ¼rasyon saklama (JSON)
- âœ… Zaman damgasÄ± ile versiyonlama
- âœ… Model listesi gÃ¶rÃ¼ntÃ¼leme
- âœ… Model seÃ§im dialogu

#### 6. KullanÄ±cÄ± ArayÃ¼zÃ¼
- âœ… Modern CustomTkinter tasarÄ±mÄ±
- âœ… Koyu tema
- âœ… Scrollable kontrol paneli
- âœ… Tab-based gÃ¶rselleÅŸtirme
- âœ… Progress bar
- âœ… Durum gÃ¶stergesi
- âœ… TÃ¼rkÃ§e arayÃ¼z

#### 7. YardÄ±m ve DÃ¶kÃ¼manlar
- âœ… **HTML YardÄ±m DosyasÄ±** (help.html):
  - Renkli ve profesyonel tasarÄ±m
  - DetaylÄ± kavram aÃ§Ä±klamalarÄ±
  - AdÄ±m adÄ±m kullanÄ±m kÄ±lavuzu
  - Matematiksel formÃ¼ller
  - Ã–rnek senaryolar
  - Sorun giderme rehberi
- âœ… **README.md**:
  - KapsamlÄ± proje aÃ§Ä±klamasÄ±
  - Kurulum talimatlarÄ±
  - Parametre referansÄ±
  - GÃ¶rselleÅŸtirme rehberi
  - Ã–rnekler ve kullanÄ±m senaryolarÄ±
- âœ… **QUICK_START.md**:
  - HÄ±zlÄ± baÅŸlangÄ±Ã§ rehberi
  - Ã–rnek Ã§alÄ±ÅŸma senaryolarÄ±
  - AdÄ±m adÄ±m deneyler
  - Not alma ÅŸablonlarÄ±

### ğŸ› ï¸ Teknik Ã–zellikler

#### PyTorch Implementasyonu
- âœ… Multi-Head Self-Attention Layer
- âœ… Transformer Block (Attention + FFN)
- âœ… Positional Encoding
- âœ… Layer Normalization
- âœ… Residual Connections
- âœ… Dropout Regularization
- âœ… Adam Optimizer
- âœ… Cross-Entropy Loss
- âœ… GPU desteÄŸi (otomatik)

#### Veri Ä°ÅŸleme
- âœ… Vocabulary oluÅŸturma
- âœ… Token encoding/decoding
- âœ… Batch oluÅŸturma
- âœ… DataLoader entegrasyonu
- âœ… Next-token prediction task

#### GÃ¶rselleÅŸtirme
- âœ… Matplotlib entegrasyonu
- âœ… Seaborn stil paletleri
- âœ… TkAgg backend
- âœ… Dinamik grafik gÃ¼ncelleme
- âœ… High-DPI export

---

## ğŸ“ Proje YapÄ±sÄ±

```
04 Self-Attention/
â”‚
â”œâ”€â”€ main.py                      # Ana program (700+ satÄ±r)
â”‚   â”œâ”€â”€ SelfAttentionApp         # Ana uygulama sÄ±nÄ±fÄ±
â”‚   â”œâ”€â”€ ModelSelectionDialog     # Model seÃ§im penceresi
â”‚   â””â”€â”€ UI bileÅŸenleri           # Kontroller, slider'lar, butonlar
â”‚
â”œâ”€â”€ self_attention_module.py     # Self-Attention implementasyonu (500+ satÄ±r)
â”‚   â”œâ”€â”€ MultiHeadSelfAttention   # Multi-head attention layer
â”‚   â”œâ”€â”€ TransformerBlock         # Transformer bloÄŸu
â”‚   â”œâ”€â”€ SelfAttentionModel       # Tam model
â”‚   â”œâ”€â”€ PositionalEncoding       # Pozisyon kodlama
â”‚   â””â”€â”€ SelfAttentionTrainer     # EÄŸitim sÄ±nÄ±fÄ±
â”‚
â”œâ”€â”€ visualization_module.py      # GÃ¶rselleÅŸtirme modÃ¼lÃ¼ (400+ satÄ±r)
â”‚   â”œâ”€â”€ VisualizationPanel       # Ana panel
â”‚   â”œâ”€â”€ visualize_attention_map  # Attention Ä±sÄ± haritasÄ±
â”‚   â”œâ”€â”€ visualize_qkv_matrices   # QKV matrisleri
â”‚   â””â”€â”€ visualize_training       # EÄŸitim grafikleri
â”‚
â”œâ”€â”€ model_manager.py             # Model yÃ¶netimi (200+ satÄ±r)
â”‚   â”œâ”€â”€ save_model               # Model kaydetme
â”‚   â”œâ”€â”€ load_model               # Model yÃ¼kleme
â”‚   â”œâ”€â”€ list_models              # Model listeleme
â”‚   â””â”€â”€ export_model_summary     # Model Ã¶zeti
â”‚
â”œâ”€â”€ help.html                    # HTML yardÄ±m (600+ satÄ±r)
â”‚   â”œâ”€â”€ Kavram aÃ§Ä±klamalarÄ±
â”‚   â”œâ”€â”€ Matematiksel formÃ¼ller
â”‚   â”œâ”€â”€ KullanÄ±m Ã¶rnekleri
â”‚   â”œâ”€â”€ Parametre referansÄ±
â”‚   â””â”€â”€ Sorun giderme
â”‚
â”œâ”€â”€ README.md                    # Ana dÃ¶kÃ¼man (800+ satÄ±r)
â”œâ”€â”€ QUICK_START.md               # HÄ±zlÄ± baÅŸlangÄ±Ã§ (500+ satÄ±r)
â”œâ”€â”€ requirements.txt             # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ start.bat                    # Windows baÅŸlatÄ±cÄ±
â”œâ”€â”€ LICENSE                      # MIT LisansÄ±
â”‚
â”œâ”€â”€ outputs/                     # Grafik Ã§Ä±ktÄ±larÄ±
â”‚   â”œâ”€â”€ attention_map.png
â”‚   â”œâ”€â”€ qkv_matrices.png
â”‚   â””â”€â”€ training_history.png
â”‚
â”œâ”€â”€ models/                      # KaydedilmiÅŸ modeller
â”‚   â””â”€â”€ [model_name_timestamp]/
â”‚       â”œâ”€â”€ model_weights.pth
â”‚       â”œâ”€â”€ full_model.pth
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ model_info.json
â”‚
â””â”€â”€ __pycache__/                 # Python cache
```

**Toplam SatÄ±r SayÄ±sÄ±**: ~3000+ satÄ±r Python kodu + 2000+ satÄ±r dÃ¶kÃ¼man

---

## ğŸš€ KullanÄ±m SenaryolarÄ±

### 1. EÄŸitim ve Ã–ÄŸretim
- Ãœniversite dersleri iÃ§in eÄŸitim materyali
- Deep Learning workshop'larÄ±
- Self-Attention Ã¶ÄŸrenim aracÄ±
- NLP kavramlarÄ±nÄ±n gÃ¶rsel anlatÄ±mÄ±

### 2. AraÅŸtÄ±rma ve Deney
- Attention mekanizmasÄ± araÅŸtÄ±rmasÄ±
- Hiperparametre optimizasyonu
- Model karÅŸÄ±laÅŸtÄ±rma Ã§alÄ±ÅŸmalarÄ±
- Veri analizi

### 3. Prototipleme
- HÄ±zlÄ± Self-Attention test
- FarklÄ± veri tipleri iÃ§in attention analizi
- Model davranÄ±ÅŸ analizi
- Pattern keÅŸfi

---

## ğŸ¯ Ã–ÄŸrenme Hedefleri

Program ile kullanÄ±cÄ±lar ÅŸunlarÄ± Ã¶ÄŸrenecek:

### Temel Kavramlar
âœ… Self-Attention nedir ve nasÄ±l Ã§alÄ±ÅŸÄ±r?
âœ… Query, Key, Value ne anlama gelir?
âœ… Attention aÄŸÄ±rlÄ±klarÄ± nasÄ±l hesaplanÄ±r?
âœ… Softmax fonksiyonunun rolÃ¼ nedir?

### Ä°leri Kavramlar
âœ… Multi-Head Attention'Ä±n avantajlarÄ±
âœ… Positional Encoding'in Ã¶nemi
âœ… Residual Connection'larÄ±n etkisi
âœ… Layer Normalization'Ä±n faydalarÄ±

### Pratik Beceriler
âœ… Model eÄŸitimi ve hiperparametre ayarlama
âœ… Attention pattern'lerini yorumlama
âœ… Overfitting/underfitting tespiti
âœ… Model kaydetme ve yÃ¼kleme

### GÃ¶rselleÅŸtirme Becerileri
âœ… Attention map okuma
âœ… QKV matrislerini anlama
âœ… EÄŸitim grafiklerini yorumlama
âœ… Pattern'leri analiz etme

---

## ğŸ“Š Teknik Detaylar

### Model Mimarisi

```python
SelfAttentionModel(
  vocab_size: Vocabulary boyutu
  d_model: 32-512 (embedding boyutu)
  num_heads: 1-16 (attention head sayÄ±sÄ±)
  num_layers: 2 (transformer blok sayÄ±sÄ±)
  dropout: 0.0-0.5
)

Toplam Parametreler:
- Embedding: vocab_size Ã— d_model
- Attention: 4 Ã— d_model Ã— d_model (Q, K, V, O)
- FFN: 2 Ã— d_model Ã— (4 Ã— d_model)
- Total: ~10K - 1M parametreler (konfigÃ¼rasyona gÃ¶re)
```

### EÄŸitim DetaylarÄ±

```python
Task: Next-token prediction
Loss: Cross-Entropy
Optimizer: Adam
Learning Rate: 0.0001 - 0.01
Batch Size: 4-16
Epochs: 20-200
GPU: Otomatik (varsa)
```

### Performans

```
KÃ¼Ã§Ã¼k Model (d_model=64, num_heads=4):
- EÄŸitim: ~1-2 saniye/epoch
- Tahmin: <1 ms
- Bellek: ~50 MB

BÃ¼yÃ¼k Model (d_model=512, num_heads=16):
- EÄŸitim: ~5-10 saniye/epoch
- Tahmin: ~10 ms
- Bellek: ~500 MB
```

---

## ğŸ¨ GÃ¶rselleÅŸtirme Ã–rnekleri

### Attention Map Ã–zellikleri
- Format: Heatmap (Ä±sÄ± haritasÄ±)
- Ã‡Ã¶zÃ¼nÃ¼rlÃ¼k: Verilebilir (varsayÄ±lan: 10Ã—8 inch)
- DPI: 150 (yÃ¼ksek kalite)
- Renk Paleti: Viridis (bilimsel standart)
- DeÄŸer gÃ¶sterimi: Her hÃ¼crede sayÄ±sal deÄŸer
- Grid: Token ayrÄ±mÄ± iÃ§in Ã§izgiler
- Colorbar: Lejant Ã§ubuÄŸu
- Etiketler: Token isimleri (45Â° dÃ¶ndÃ¼rÃ¼lmÃ¼ÅŸ)

### QKV Matrisleri Ã–zellikleri
- Format: 3 paralel heatmap
- Renk: RdBu_r (kÄ±rmÄ±zÄ±-beyaz-mavi)
- Boyut gÃ¶sterimi: Ä°lk 16 boyut (gÃ¶rsellik iÃ§in)
- Etiketler: Token isimleri
- Colorbar: Her matris iÃ§in ayrÄ±

### EÄŸitim GrafiÄŸi Ã–zellikleri
- Format: Line plot
- Renk: YeÅŸil (#00ff00)
- Marker: Yuvarlak noktalar
- Grid: Arka plan grid
- Son deÄŸer: Metin kutusu ile gÃ¶sterim

---

## ğŸ’¾ Model Saklama FormatÄ±

### Dosya YapÄ±sÄ±
```json
{
  "config.json": {
    "d_model": 64,
    "num_heads": 4,
    "num_layers": 2,
    "dropout": 0.1,
    "learning_rate": 0.001,
    "vocab": ["<PAD>", "<UNK>", "token1", "token2"],
    "token_to_idx": {"token1": 2, "token2": 3},
    "idx_to_token": {"2": "token1", "3": "token2"}
  },
  
  "model_info.json": {
    "name": "model_name",
    "timestamp": "20250102_143052",
    "full_name": "model_name_20250102_143052",
    "save_date": "2025-01-02T14:30:52",
    "config": {...}
  }
}
```

### PyTorch DosyalarÄ±
- `model_weights.pth`: State dict (sadece aÄŸÄ±rlÄ±klar)
- `full_model.pth`: Tam model (mimari + aÄŸÄ±rlÄ±klar)

---

## ğŸ”§ Sistem Gereksinimleri

### Minimum
- **OS**: Windows 10/11, macOS, Linux
- **Python**: 3.8+
- **RAM**: 4 GB
- **Disk**: 500 MB boÅŸ alan
- **GPU**: Opsiyonel (CPU ile Ã§alÄ±ÅŸÄ±r)

### Ã–nerilen
- **OS**: Windows 11
- **Python**: 3.10+
- **RAM**: 8 GB
- **Disk**: 2 GB boÅŸ alan
- **GPU**: NVIDIA CUDA destekli (GTX 1060+)

---

## ğŸ“š Referanslar ve Kaynaklar

### Akademik Makaleler
1. Vaswani et al. (2017) - "Attention Is All You Need"
2. Devlin et al. (2018) - "BERT: Pre-training of Deep Bidirectional Transformers"
3. Brown et al. (2020) - "Language Models are Few-Shot Learners" (GPT-3)

### Implementasyon ReferanslarÄ±
- PyTorch Transformer Tutorial
- Annotated Transformer (Harvard NLP)
- The Illustrated Transformer (Jay Alammar)

### KullanÄ±lan Teknolojiler
- **PyTorch**: Deep Learning framework
- **CustomTkinter**: Modern GUI toolkit
- **Matplotlib**: Grafik Ã§izimi
- **Seaborn**: Ä°statistiksel grafikler
- **NumPy**: SayÄ±sal hesaplamalar

---

## ğŸ“ EÄŸitsel DeÄŸer

### Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±

#### Bilgi (Knowledge)
- Self-Attention mekanizmasÄ±nÄ±n matematiksel temeli
- Query, Key, Value konseptleri
- Multi-Head Attention prensibi
- Transformer mimarisinin temel bileÅŸenleri

#### Beceri (Skills)
- Deep Learning modeli eÄŸitme
- Hiperparametre optimizasyonu
- GÃ¶rselleÅŸtirme ve analiz
- Model kaydetme/yÃ¼kleme

#### Uygulama (Application)
- GerÃ§ek veri Ã¼zerinde Ã§alÄ±ÅŸma
- Problem Ã§Ã¶zme
- Deney tasarlama
- SonuÃ§ yorumlama

---

## ğŸš€ Gelecek GeliÅŸtirmeler (Opsiyonel)

### Potansiyel Ä°yileÅŸtirmeler
- [ ] Ä°ngilizce dil desteÄŸi
- [ ] Daha fazla Ã¶rnek veri seti
- [ ] Model karÅŸÄ±laÅŸtÄ±rma arayÃ¼zÃ¼
- [ ] Export to PDF (raporlama)
- [ ] Video tutorial entegrasyonu
- [ ] Online learning mode
- [ ] Distributed training support
- [ ] TensorBoard entegrasyonu
- [ ] Attention head analizi
- [ ] Pattern keÅŸif araÃ§larÄ±

---

## âœ… Kalite Kontrol

### Test Edilen Senaryolar
âœ… KÃ¼Ã§Ã¼k veri seti (3-5 token)
âœ… Orta veri seti (6-10 token)
âœ… BÃ¼yÃ¼k veri seti (>10 token)
âœ… FarklÄ± d_model deÄŸerleri (32-512)
âœ… FarklÄ± num_heads deÄŸerleri (1-16)
âœ… FarklÄ± dropout deÄŸerleri (0.0-0.5)
âœ… FarklÄ± learning rate deÄŸerleri (0.0001-0.01)
âœ… Model kaydetme/yÃ¼kleme
âœ… Grafik export
âœ… Uzun eÄŸitim (200+ epoch)
âœ… GPU/CPU uyumluluÄŸu

### Bilinen SÄ±nÄ±rlamalar
âš ï¸ Ã‡ok uzun diziler (>50 token) yavaÅŸ olabilir
âš ï¸ Ã‡ok yÃ¼ksek d_model (>512) bellek sorunlarÄ±na yol aÃ§abilir
âš ï¸ MacOS'ta GUI render sorunlarÄ± olabilir (CustomTkinter)
âš ï¸ Ã‡ok kÃ¼Ã§Ã¼k ekranlarda (<1280Ã—720) UI sÄ±kÄ±ÅŸÄ±k gÃ¶rÃ¼nebilir

---

## ğŸ“ Lisanslama

**MIT License** - AÃ§Ä±k kaynak, ticari kullanÄ±ma uygun

Proje tamamen Ã¼cretsiz ve aÃ§Ä±k kaynak olarak kullanÄ±labilir.

---

## ğŸ‰ SonuÃ§

Bu proje, **Self-Attention mekanizmasÄ±nÄ± Ã¶ÄŸrenmek isteyen herkes iÃ§in** kapsamlÄ±, interaktif ve profesyonel bir eÄŸitim aracÄ±dÄ±r.

### BaÅŸarÄ±lan Hedefler
âœ… Query, Key, Value Ã¶ÄŸretimi
âœ… GÃ¶rsel ve interaktif Ã¶ÄŸrenme
âœ… Parametre etkilerini gÃ¶zlemleme
âœ… Model kaydetme/yÃ¼kleme sistemi
âœ… DetaylÄ± dÃ¶kÃ¼manlar
âœ… Profesyonel kod kalitesi
âœ… KullanÄ±cÄ± dostu arayÃ¼z
âœ… TÃ¼rkÃ§e destek

### Ã–ne Ã‡Ä±kan Ã–zellikler
ğŸŒŸ GerÃ§ek zamanlÄ± eÄŸitim ve gÃ¶rselleÅŸtirme
ğŸŒŸ 3000+ satÄ±r profesyonel Python kodu
ğŸŒŸ KapsamlÄ± HTML yardÄ±m dÃ¶kÃ¼manÄ±
ğŸŒŸ DetaylÄ± README ve QUICK_START kÄ±lavuzlarÄ±
ğŸŒŸ Tam model yÃ¶netim sistemi
ğŸŒŸ Multi-head attention desteÄŸi
ğŸŒŸ GPU/CPU uyumluluÄŸu

---

## ğŸ“§ Destek ve Ä°letiÅŸim

SorularÄ±nÄ±z veya Ã¶nerileriniz iÃ§in:
- ğŸ“– Ä°lk Ã¶nce README.md'yi okuyun
- ğŸ” QUICK_START.md'de Ã¶rneklere bakÄ±n
- ğŸŒ help.html'i browser'da aÃ§Ä±n
- ğŸ’¬ GitHub Issues kullanÄ±n (varsa)

---

<div align="center">

# ğŸ“ Ä°YÄ° Ã–ÄRENMELER! ğŸš€

**Self-Attention'Ä± anlamak, modern yapay zekanÄ±n kapÄ±larÄ±nÄ± aÃ§ar!**

---

**Proje Tamamlanma Tarihi**: 2 Ocak 2025  
**Toplam GeliÅŸtirme SÃ¼resi**: ~2 saat  
**Kod Kalitesi**: Profesyonel  
**DokÃ¼mantasyon**: KapsamlÄ±  
**Durum**: âœ… HAZIR ve Ã‡ALIÅIR DURUMDA

---

Made with â¤ï¸ for AI Education

</div>
