# ğŸ¯ Self-Attention Ã–ÄŸrenme AracÄ± - HÄ±zlÄ± BaÅŸlangÄ±Ã§ Rehberi

## ğŸš€ Program BaÅŸlatma

```bash
cd "c:\Users\ASUS\Desktop\Python with AI\04 Self-Attention"
python main.py
```

---

## ğŸ“š Ã–rnek Ã‡alÄ±ÅŸma SenaryolarÄ±

### Senaryo 1: Ä°lk Denemeniz (5 dakika)

**Hedef**: Self-Attention'Ä±n temel Ã§alÄ±ÅŸma prensibini anlamak

1. **ProgramÄ± BaÅŸlatÄ±n**
   - `python main.py` komutuyla baÅŸlatÄ±n

2. **VarsayÄ±lan AyarlarÄ± KullanÄ±n**
   - Veri: "Ben, BugÃ¼n, Okula, Gittim" (otomatik yÃ¼klÃ¼)
   - d_model: 64
   - num_heads: 4
   - dropout: 0.1
   - learning_rate: 0.001

3. **EÄŸitimi BaÅŸlatÄ±n**
   - Epoch: 50
   - Batch Size: 8
   - "ğŸš€ EÄŸitimi BaÅŸlat" butonuna tÄ±klayÄ±n

4. **SonuÃ§larÄ± Ä°nceleyin**
   - ğŸ” **Attention Map** tabÄ±na gidin
   - "Gittim" satÄ±rÄ±na bakÄ±n
   - "Ben" ve "Okula" ile gÃ¼Ã§lÃ¼ baÄŸlantÄ± gÃ¶rmelisiniz
   - Bu, "gittim" fiilinin Ã¶zne ve yeri aradÄ±ÄŸÄ±nÄ± gÃ¶sterir!

**Beklenen SÃ¼re**: ~2 dakika (eÄŸitim + inceleme)

---

### Senaryo 2: Head SayÄ±sÄ±nÄ±n Etkisi (15 dakika)

**Hedef**: Multi-Head Attention'Ä±n avantajlarÄ±nÄ± gÃ¶rmek

#### Deney 1: Az Head
```
Veri: Kedi, Mat, Ãœzerinde, Oturdu
d_model: 64
num_heads: 2
epochs: 50
Model AdÄ±: "az_head_model"
```
- EÄŸitimi tamamlayÄ±n
- Attention map'i inceleyin
- Modeli kaydedin

#### Deney 2: Ã‡ok Head
```
Veri: Kedi, Mat, Ãœzerinde, Oturdu (AYNI VERÄ°!)
d_model: 64
num_heads: 8
epochs: 50
Model AdÄ±: "cok_head_model"
```
- EÄŸitimi tamamlayÄ±n
- Attention map'i inceleyin
- Modeli kaydedin

#### KarÅŸÄ±laÅŸtÄ±rma
1. "az_head_model" yÃ¼kleyin â†’ Attention map ekran gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±n
2. "cok_head_model" yÃ¼kleyin â†’ Attention map ekran gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±n
3. Ä°ki grafiÄŸi yan yana koyun

**GÃ¶zlem**: Daha fazla head, daha detaylÄ± ve Ã§eÅŸitli iliÅŸkiler Ã¶ÄŸrenir!

---

### Senaryo 3: Embedding Boyutunun Etkisi (20 dakika)

**Hedef**: d_model parametresinin modelin kapasitesine etkisini gÃ¶rmek

#### Test 1: KÃ¼Ã§Ã¼k Model
```
Veri: Pazartesi, SalÄ±, Ã‡arÅŸamba, PerÅŸembe, Cuma, Cumartesi
d_model: 32
num_heads: 4
epochs: 100
```

#### Test 2: Orta Model
```
Veri: Pazartesi, SalÄ±, Ã‡arÅŸamba, PerÅŸembe, Cuma, Cumartesi (AYNI)
d_model: 128
num_heads: 4
epochs: 100
```

#### Test 3: BÃ¼yÃ¼k Model
```
Veri: Pazartesi, SalÄ±, Ã‡arÅŸamba, PerÅŸembe, Cuma, Cumartesi (AYNI)
d_model: 256
num_heads: 4
epochs: 100
```

**KarÅŸÄ±laÅŸtÄ±rÄ±n**:
- ğŸ“ˆ EÄŸitim GrafiÄŸi tabÄ±nda loss deÄŸerlerini karÅŸÄ±laÅŸtÄ±rÄ±n
- ğŸ“Š Q, K, V Matrisleri tabÄ±nda vektÃ¶rlerin zenginliÄŸini gÃ¶zlemleyin

**GÃ¶zlem**: Daha bÃ¼yÃ¼k d_model â†’ Daha dÃ¼ÅŸÃ¼k loss ama daha yavaÅŸ eÄŸitim!

---

### Senaryo 4: Dropout ve Overfitting (25 dakika)

**Hedef**: Dropout'un overfitting'i nasÄ±l Ã¶nlediÄŸini gÃ¶rmek

#### Baseline (Dropout Yok)
```
Veri: Kedi, KÃ¶pek, KuÅŸ, BalÄ±k
d_model: 64
num_heads: 4
dropout: 0.0
epochs: 200 (UZUN!)
Model: "no_dropout"
```

#### Dropout Ä°le
```
Veri: Kedi, KÃ¶pek, KuÅŸ, BalÄ±k (AYNI)
d_model: 64
num_heads: 4
dropout: 0.3
epochs: 200 (AYNI!)
Model: "with_dropout"
```

**KarÅŸÄ±laÅŸtÄ±rÄ±n**:
- ğŸ“ˆ EÄŸitim GrafiÄŸi'nde loss eÄŸrilerini inceleyin
- Dropout=0.0 â†’ Loss daha hÄ±zlÄ± dÃ¼ÅŸer ama overfitting riski
- Dropout=0.3 â†’ Loss daha yavaÅŸ ama daha stabil

---

### Senaryo 5: Learning Rate Optimizasyonu (30 dakika)

**Hedef**: Optimal Ã¶ÄŸrenme hÄ±zÄ±nÄ± bulmak

#### Test 1: Ã‡ok DÃ¼ÅŸÃ¼k
```
learning_rate: 0.0001
epochs: 50
GÃ¶zlem: Loss Ã§ok yavaÅŸ dÃ¼ÅŸer
```

#### Test 2: Orta (Optimal)
```
learning_rate: 0.001
epochs: 50
GÃ¶zlem: Loss dengeli ÅŸekilde dÃ¼ÅŸer
```

#### Test 3: YÃ¼ksek
```
learning_rate: 0.01
epochs: 50
GÃ¶zlem: Loss dalgalÄ±, bazen patlar (NaN)
```

#### Test 4: Ã‡ok YÃ¼ksek
```
learning_rate: 0.1
epochs: 50
GÃ¶zlem: Loss hemen NaN olur!
```

**Ders**: learning_rate = 0.001 genellikle gÃ¼venli bir baÅŸlangÄ±Ã§ noktasÄ±dÄ±r.

---

### Senaryo 6: GerÃ§ek DÃ¼nya UygulamasÄ± - CÃ¼mle Analizi (40 dakika)

**Hedef**: KarmaÅŸÄ±k bir cÃ¼mlenin attention pattern'lerini analiz etmek

#### Veri
```
BugÃ¼n
Hava
Ã‡ok
GÃ¼zel
OlduÄŸu
Ä°Ã§in
Parka
Gittik
```

#### Optimal Parametreler (bulacaÄŸÄ±z!)
```
BaÅŸlangÄ±Ã§:
d_model: 128
num_heads: 8
dropout: 0.2
learning_rate: 0.001
epochs: 100
```

#### Analiz AdÄ±mlarÄ±

1. **Ä°lk EÄŸitim**: YukarÄ±daki parametrelerle eÄŸitin
2. **Attention Analizi**:
   - "Gittik" satÄ±rÄ±na bakÄ±n
   - Hangi kelimelerle gÃ¼Ã§lÃ¼ baÄŸlantÄ± var?
   - Beklenen: "Biz" (Ã¶rtÃ¼k Ã¶zne), "Parka"
   
3. **Ä°yileÅŸtirme Denemeleri**:
   - num_heads'i 12'ye Ã§Ä±karÄ±n â†’ Fark var mÄ±?
   - d_model'i 256'ya Ã§Ä±karÄ±n â†’ Loss daha mÄ± dÃ¼ÅŸÃ¼k?
   - dropout'u 0.1'e dÃ¼ÅŸÃ¼rÃ¼n â†’ Overfitting oldu mu?

4. **SonuÃ§**: En iyi parametreleri bulup kaydedin

---

## ğŸ¨ GRAFÄ°KLERÄ° ANLAMAK: BAÅLANGIÃ‡ SEVIYESI REHBER

### ğŸ“Š Grafik 1: Attention Map

#### Bu Grafik Neyi GÃ¶sterir?
Attention Map, **her kelimenin diÄŸer kelimelere ne kadar "dikkat ettiÄŸini"** gÃ¶steren bir Ä±sÄ± haritasÄ±dÄ±r. Bu, Self-Attention'Ä±n kalbindeki mekanizmadÄ±r!

#### GrafiÄŸi Okuma Rehberi

**Eksenler:**
- **Y Ekseni (Solda, Dikey)**: Query kelimeleri - "Hangi kelime soruyor?"
- **X Ekseni (Altta, Yatay)**: Key kelimeleri - "Hangi kelimeye bakÄ±yor?"
- **Her hÃ¼cre**: Bir kelimenin baÅŸka bir kelimeye verdiÄŸi Ã¶nemi gÃ¶sterir

**Renkler:**
- ğŸŸ¨ **SarÄ±/AÃ§Ä±k Renkler**: YÃœKSEK dikkat (0.7 - 1.0)
  - Bu kelimeler birbirine Ã§ok Ã¶nemli!
  - GÃ¼Ã§lÃ¼ iliÅŸki var
  - Ã–rnek: "Kedi" â†’ "Oturdu" (Ã¶zne-fiil iliÅŸkisi)

- ğŸŸ§ **Turuncu Renkler**: ORTA dikkat (0.3 - 0.7)
  - Var olan bir iliÅŸki
  - Ã–nemli ama birincil deÄŸil
  - Ã–rnek: "Kedi" â†’ "Mat" (Ã¶zne-yer iliÅŸkisi)

- ğŸŸ¦ **Mavi/Koyu Renkler**: DÃœÅÃœK dikkat (0.0 - 0.3)
  - ZayÄ±f veya yok iliÅŸki
  - Kelimeler birbirini gÃ¶rmezden geliyor
  - Ã–rnek: "Pazartesi" â†’ "Cuma" (uzak gÃ¼nler)

#### AdÄ±m AdÄ±m Analiz Ã–rneÄŸi

**Ã–rnek Veri**: "Ben, BugÃ¼n, Okula, Gittim"

**1. Ä°lk BakÄ±ÅŸ:**
```
GrafiÄŸi aÃ§Ä±n â†’ 4x4'lÃ¼k bir tablo gÃ¶receksiniz
Her satÄ±r bir kelime, her sÃ¼tun bir kelime
16 hÃ¼cre toplam (4 kelime x 4 kelime)
```

**2. Bir SatÄ±rÄ± Ä°nceleyin (Ã–rnek: "Gittim"):**
```
"Gittim" satÄ±rÄ±nÄ± bulun (en altta)
Bu satÄ±r "Gittim" kelimesinin bakÄ±ÅŸ aÃ§Ä±sÄ±

SÃ¼tunlara bakÄ±n:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gittim â†’ Ben      : 0.52 (PARLAK!) â”‚ â† Ã–zne arÄ±yor!
â”‚ Gittim â†’ BugÃ¼n    : 0.21 (Orta)    â”‚ â† Zaman Ã¶nemli
â”‚ Gittim â†’ Okula    : 0.25 (Orta)    â”‚ â† Yer Ã¶nemli
â”‚ Gittim â†’ Gittim   : 0.02 (Koyu)    â”‚ â† Kendine bakmÄ±yor
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YORUM: "Gittim" fiili, Ã¶zneyi (Ben) arÄ±yor! 
Bu DOÄRU bir dilbilgisel iliÅŸki!
```

**3. Ã‡apraz Ä°liÅŸkileri Ä°nceleyin:**
```
"Ben" satÄ±rÄ±, "Gittim" sÃ¼tunu: 0.48 (Parlak)
"Gittim" satÄ±rÄ±, "Ben" sÃ¼tunu: 0.52 (Parlak)

YORUM: Ä°ki yÃ¶nlÃ¼ gÃ¼Ã§lÃ¼ iliÅŸki! 
Ã–zne ve fiil birbirini tanÄ±yor.
```

**4. KÃ¶ÅŸegen (Diagonal) Ä°nceleyin:**
```
KÃ¶ÅŸegen = Her kelimenin kendisine dikkat etmesi

Ben â†’ Ben      : Genellikle DÃœÅÃœK (0.1-0.2)
BugÃ¼n â†’ BugÃ¼n  : Genellikle DÃœÅÃœK (0.1-0.2)
Okula â†’ Okula  : Genellikle DÃœÅÃœK (0.1-0.2)
Gittim â†’ Gittim: Genellikle DÃœÅÃœK (0.1-0.2)

YORUM: Ä°yi! Kelimeler kendi iÃ§lerine deÄŸil, 
diÄŸer kelimelere bakÄ±yor. Self-attention Ã§alÄ±ÅŸÄ±yor!

âš ï¸ UYARI: KÃ¶ÅŸegen Ã‡OK PARLAK ise sorun var!
Bu, modelin diÄŸer kelimeleri gÃ¶rmezden geldiÄŸi anlamÄ±na gelir.
```

#### GerÃ§ek DÃ¼nya Ã–rnekleri

**Ã–rnek 1: Fiil-Ã–zne Ä°liÅŸkisi**
```
Veri: "Kedi, Mat, Ãœzerinde, Oturdu"

Attention Map'e bakÄ±n:
"Oturdu" (satÄ±r) â†’ "Kedi" (sÃ¼tun): 0.65 (Ã‡OK PARLAK!)

Neden? Ã‡Ã¼nkÃ¼ "oturdu" fiili bir Ã¶zne arÄ±yor!
"Oturdu" â†’ "Mat": 0.25 (orta, yer belirtir)
"Oturdu" â†’ "Ãœzerinde": 0.08 (dÃ¼ÅŸÃ¼k, ek bilgi)

âœ… Model dilbilgisel iliÅŸkiyi DOÄRU Ã¶ÄŸrenmiÅŸ!
```

**Ã–rnek 2: Zaman Serisi Ä°liÅŸkisi**
```
Veri: "Pazartesi, SalÄ±, Ã‡arÅŸamba, PerÅŸembe"

Attention Map'te bakÄ±n:
"SalÄ±" (satÄ±r) â†’ "Pazartesi" (sÃ¼tun): 0.45
"SalÄ±" (satÄ±r) â†’ "SalÄ±" (sÃ¼tun): 0.30
"SalÄ±" (satÄ±r) â†’ "Ã‡arÅŸamba" (sÃ¼tun): 0.20
"SalÄ±" (satÄ±r) â†’ "PerÅŸembe" (sÃ¼tun): 0.05

YORUM: SalÄ±, en Ã§ok Ã¶nceki gÃ¼ne (Pazartesi) bakÄ±yor!
ArdÄ±ÅŸÄ±klÄ±k iliÅŸkisi Ã¶ÄŸrenilmiÅŸ. Uzak gÃ¼nler (PerÅŸembe) az ilgili.

âœ… Model zamansal sÄ±ralamayÄ± DOÄRU Ã¶ÄŸrenmiÅŸ!
```

**Ã–rnek 3: AnlamsÄ±z Veri**
```
Veri: "Elma, Araba, Pazartesi, KÄ±rmÄ±zÄ±" (ilgisiz kelimeler)

Attention Map'te bakÄ±n:
TÃ¼m hÃ¼creler benzer deÄŸerler (~0.25)
Hepsi eÅŸit derecede "belirsiz"
KÃ¶ÅŸegen daha parlak (kendi kendine bakÄ±yor)

âŒ Model anlamlÄ± iliÅŸki bulamÄ±yor!
Bu NORMAL, Ã§Ã¼nkÃ¼ kelimeler gerÃ§ekten ilgisiz.
```

#### SÄ±k KarÅŸÄ±laÅŸÄ±lan Durumlar ve AnlamlarÄ±

**Durum 1: TÃ¼m SatÄ±r EÅŸit DaÄŸÄ±lÄ±mlÄ±**
```
Ben â†’ Ben    : 0.25
Ben â†’ BugÃ¼n  : 0.25
Ben â†’ Okula  : 0.25
Ben â†’ Gittim : 0.25

Anlam: Bu kelime herkese eÅŸit dikkat ediyor
Yorum: Model net bir iliÅŸki Ã¶ÄŸrenememiÅŸ
Ã‡Ã¶zÃ¼m: Daha fazla epoch, daha bÃ¼yÃ¼k d_model deneyin
```

**Durum 2: Bir SÃ¼tun Ã‡ok Parlak**
```
TÃ¼m kelimeler â†’ "Gittim": PARLAK

Anlam: "Gittim" kelimesi merkezi bir rol oynuyor
Yorum: Fiil/ana kelime olduÄŸu iÃ§in DOÄRU!
Ã‡Ã¶zÃ¼m: Sorun yok, bu iyi bir ÅŸey
```

**Durum 3: Sadece KÃ¶ÅŸegen Parlak**
```
Ben â†’ Ben       : 0.90 âŒ
BugÃ¼n â†’ BugÃ¼n   : 0.85 âŒ
Okula â†’ Okula   : 0.80 âŒ

Anlam: Kelimeler sadece kendilerine bakÄ±yor!
Yorum: Self-attention Ã§alÄ±ÅŸmÄ±yor, model Ã¶ÄŸrenememiÅŸ
Ã‡Ã¶zÃ¼m: Learning rate artÄ±rÄ±n, epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n
```

---

### ğŸ“Š Grafik 2: Q, K, V Matrisleri (Ä°leri Seviye)

#### Bu Grafik Neyi GÃ¶sterir?
Self-Attention'Ä±n "kaputun altÄ±"! Her kelimenin Query, Key, Value vektÃ¶rlerini gÃ¶sterir.

#### Ã–nce Teori: Q, K, V Nedir?

**GerÃ§ek DÃ¼nya Analojisi:**
```
ğŸ” Query (Q): "Ne arÄ±yorum?"
   Ã–rnek: KÃ¼tÃ¼phanede "tarih kitabÄ± arÄ±yorum"
   
ğŸ”‘ Key (K): "Ben kimim? Ne sunuyorum?"
   Ã–rnek: Kitap rafÄ±nda "Ben bir tarih kitabÄ±yÄ±m"
   
ğŸ’ Value (V): "Bulunursam ne veriyorum?"
   Ã–rnek: "OsmanlÄ± Ä°mparatorluÄŸu hakkÄ±nda bilgi"

Attention = Q ve K'nin uyumu Ã— V'nin iÃ§eriÄŸi
```

**Matematiksel (BasitleÅŸtirilmiÅŸ):**
```
1. Her kelime â†’ Q, K, V vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
2. Q ve K karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r â†’ Benzerlik skoru (attention aÄŸÄ±rlÄ±ÄŸÄ±)
3. Bu skor ile V Ã§arpÄ±lÄ±r â†’ BaÄŸlamsal temsil

FormÃ¼l: Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd) Â· V
```

#### GrafiÄŸi Okuma Rehberi

**GÃ¶rsel YapÄ±:**
```
ÃœÃ§ adet heatmap gÃ¶receksiniz:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Q Matrix  â”‚  â”‚   K Matrix  â”‚  â”‚   V Matrix  â”‚
â”‚  (Query)    â”‚  â”‚   (Key)     â”‚  â”‚  (Value)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Her matris:
- SatÄ±rlar: Kelimeler (Ã¶rn: Ben, BugÃ¼n, Okula, Gittim)
- SÃ¼tunlar: Embedding boyutlarÄ± (d_model kadar, Ã¶rn: 64 sÃ¼tun)
- Renkler: VektÃ¶r deÄŸerleri (-1 ile +1 arasÄ±)
```

**Renk Yorumlama:**
- ğŸ”´ **KÄ±rmÄ±zÄ±/SarÄ± (+0.5 ile +1.0)**: POZÄ°TÄ°F aktivasyon, gÃ¼Ã§lÃ¼ Ã¶zellik
- âšª **Beyaz/AÃ§Ä±k Gri (-0.2 ile +0.2)**: NÃ–TR, Ã¶nemsiz boyut
- ğŸ”µ **Mavi/Lacivert (-1.0 ile -0.5)**: NEGATÄ°F aktivasyon, ters Ã¶zellik

#### AdÄ±m AdÄ±m Analiz

**1. Q Matrisi (Query) Analizi:**
```
Soru: "Her kelime ne arÄ±yor?"

Ã–rnek: "Gittim" kelimesinin satÄ±rÄ±
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pozisyon 0-15:  KÄ±rmÄ±zÄ± bantlar (0.8)     â”‚ â† Ã–zne Ã¶zellikleri arÄ±yor
â”‚ Pozisyon 16-31: Mavi bantlar (-0.6)       â”‚ â† Zaman Ã¶zelliklerini es geÃ§iyor
â”‚ Pozisyon 32-47: KarÄ±ÅŸÄ±k pattern           â”‚ â† Nesne Ã¶zellikleri arÄ±yor
â”‚ Pozisyon 48-63: NÃ¶tr (beyaz)              â”‚ â† Bu boyutlar Ã¶nemsiz
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YORUM: "Gittim" fiili, Ã¶zne ve nesne pattern'leri arÄ±yor.
Zaman bilgisini es geÃ§iyor (mavi = negatif).
```

**2. K Matrisi (Key) Analizi:**
```
Soru: "Her kelime kendini nasÄ±l tanÄ±tÄ±yor?"

Ã–rnek: "Ben" kelimesinin satÄ±rÄ±
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pozisyon 0-15:  Ã‡OK kÄ±rmÄ±zÄ± (0.9)         â”‚ â† "Ben Ã¶zneyim!"
â”‚ Pozisyon 16-31: NÃ¶tr                      â”‚ â† Zaman deÄŸilim
â”‚ Pozisyon 32-47: Hafif kÄ±rmÄ±zÄ± (0.3)       â”‚ â† Belki nesne de olabilirim
â”‚ Pozisyon 48-63: Mavi (-0.4)               â”‚ â† Fiil kesinlikle deÄŸilim!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YORUM: "Ben" Ã¶zne olduÄŸunu gÃ¼Ã§lÃ¼ bir ÅŸekilde belirtiyor.
Q ve K'nin 0-15 pozisyonlarÄ± uyuÅŸuyor â†’ YÃ¼ksek attention!
```

**3. V Matrisi (Value) Analizi:**
```
Soru: "Her kelime hangi bilgiyi taÅŸÄ±yor?"

Ã–rnek: "Okula" kelimesinin satÄ±rÄ±
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TÃ¼m boyutlar zengin pattern'ler          â”‚
â”‚ Ã‡ok renkli, karÄ±ÅŸÄ±k (iyi bir ÅŸey!)       â”‚
â”‚ Pozitif ve negatif deÄŸerler karÄ±ÅŸÄ±k      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

YORUM: V matrisi en zengin olmalÄ±!
Ã‡Ã¼nkÃ¼ tÃ¼m baÄŸlamsal bilgi burada saklanÄ±r.
```

#### Pattern'leri KarÅŸÄ±laÅŸtÄ±rma

**Benzer Kelimeler â†’ Benzer Pattern'ler**
```
Veri: "Elma, Armut, Muz" (meyveler) + "Araba" (farklÄ±)

K Matrisi'nde:
"Elma" satÄ±rÄ±:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (pattern A)
"Armut" satÄ±rÄ±: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (pattern A - benzer!)
"Muz" satÄ±rÄ±:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (pattern A - benzer!)
"Araba" satÄ±rÄ±: â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (pattern B - farklÄ±!)

âœ… Model benzer kelimeleri benzer encode ediyor!
```

**FarklÄ± Roller â†’ FarklÄ± Pattern'ler**
```
Veri: "Kedi" (Ã¶zne) vs "Oturdu" (fiil)

Q Matrisi'nde:
"Kedi":   â–ˆâ–‘â–‘â–‘â–ˆâ–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘ (pattern X - Ã¶zne arÄ±yor)
"Oturdu": â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ (pattern Y - Ã¶zne arÄ±yor)

"Kedi" ve "Oturdu" farklÄ± roller ama Q pattern'leri benzer!
Neden? Ä°kisi de Ã¶zne arÄ±yor â†’ Q pattern'leri benzer olmalÄ±!
```

#### Ne Zaman EndiÅŸelenmelisiniz?

**âŒ Sorun 1: TÃ¼m SatÄ±rlar AynÄ±**
```
Q/K/V'de tÃ¼m kelimeler aynÄ± pattern â†’ Model Ã¶ÄŸrenmemiÅŸ!
Ã‡Ã¶zÃ¼m: Daha fazla epoch, farklÄ± learning rate
```

**âŒ Sorun 2: Sadece NÃ¶tr Renkler (Beyaz)**
```
HiÃ§ kÄ±rmÄ±zÄ±/mavi yok, hep beyaz â†’ Model aktivasyon Ã¼retemiyor!
Ã‡Ã¶zÃ¼m: Learning rate artÄ±rÄ±n, model Ã§ok kÃ¼Ã§Ã¼k olabilir
```

**âŒ Sorun 3: AÅŸÄ±rÄ± KÄ±rmÄ±zÄ± veya AÅŸÄ±rÄ± Mavi**
```
Sadece Ã§ok parlak kÄ±rmÄ±zÄ± veya Ã§ok koyu mavi â†’ Exploding gradients!
Ã‡Ã¶zÃ¼m: Learning rate dÃ¼ÅŸÃ¼rÃ¼n, gradient clipping ekleyin
```

**âœ… Ä°yi GÃ¶rÃ¼nÃ¼m:**
```
- KarÄ±ÅŸÄ±k renkler (kÄ±rmÄ±zÄ±, mavi, beyaz)
- Her satÄ±r farklÄ± pattern
- V matrisi en zengin (en karÄ±ÅŸÄ±k)
- Benzer kelimeler benzer pattern'ler
```

---

### ğŸ“Š Grafik 3: EÄŸitim GeÃ§miÅŸi (Training Loss)

#### Bu Grafik Neyi GÃ¶sterir?
Modelin **Ã¶ÄŸrenme sÃ¼recini** gÃ¶sterir. Her epoch'ta modelin ne kadar "hata yaptÄ±ÄŸÄ±nÄ±" (loss) gÃ¶rÃ¼rsÃ¼nÃ¼z.

#### GrafiÄŸi Okuma Rehberi

**Eksenler:**
- **X Ekseni (Yatay)**: Epoch sayÄ±sÄ± (zaman Ã§izelgesi)
  - BaÅŸlangÄ±Ã§: 0
  - Son: BelirlediÄŸiniz epoch (Ã¶rn: 50, 100, 200)
  
- **Y Ekseni (Dikey)**: Loss deÄŸeri (hata miktarÄ±)
  - YÃ¼ksek deÄŸer = Ã‡ok hata (kÃ¶tÃ¼)
  - DÃ¼ÅŸÃ¼k deÄŸer = Az hata (iyi)
  - AralÄ±k: Genellikle 0.0 - 3.0

**Ã‡izgi:**
- Kahverengi Ã§izgi: Modelin Ã¶ÄŸrenme yolculuÄŸu
- BaÅŸlangÄ±Ã§ (sol): Genellikle yÃ¼ksek (model cahil)
- Son (saÄŸ): Genellikle dÃ¼ÅŸÃ¼k (model Ã¶ÄŸrendi)

#### AdÄ±m AdÄ±m Analiz

**1. BaÅŸlangÄ±Ã§ DeÄŸeri (Ä°lk Epoch):**
```
Epoch 0'da loss: 2.5 - 3.5 arasÄ±

Ne anlama gelir?
- Model henÃ¼z hiÃ§bir ÅŸey Ã¶ÄŸrenmemiÅŸ
- Rastgele tahmin yapÄ±yor
- NORMAL bir durum, endiÅŸelenmeyin!

Ã–rnek:
Epoch 1: Loss = 2.87 â† Ä°yi bir baÅŸlangÄ±Ã§
```

**2. Ã–ÄŸrenme EÄŸrisi (Ä°lk 10-20 Epoch):**
```
Ä°deal GÃ¶rÃ¼nÃ¼m:
Epoch 1:  2.87
Epoch 2:  2.45 â†“ (azalÄ±yor - iyi!)
Epoch 3:  2.12 â†“
Epoch 4:  1.85 â†“
Epoch 5:  1.62 â†“
...
Epoch 10: 0.95 â†“

âœ… Loss hÄ±zla dÃ¼ÅŸÃ¼yor = Model aktif Ã¶ÄŸreniyor!

Sorunlu GÃ¶rÃ¼nÃ¼m:
Epoch 1:  2.87
Epoch 2:  2.85 (Ã§ok az dÃ¼ÅŸtÃ¼)
Epoch 3:  2.84 (hala Ã§ok az)
Epoch 4:  2.83
Epoch 5:  2.82 (Ã§ok yavaÅŸ!)

âŒ Loss Ã§ok yavaÅŸ dÃ¼ÅŸÃ¼yor = Learning rate Ã§ok dÃ¼ÅŸÃ¼k!
Ã‡Ã¶zÃ¼m: learning_rate'i 0.001'den 0.005'e Ã§Ä±karÄ±n
```

**3. Stabilizasyon (Orta DÃ¶nem):**
```
Epoch 20-50 arasÄ±:

Ä°deal GÃ¶rÃ¼nÃ¼m:
Epoch 20: 0.75
Epoch 25: 0.68
Epoch 30: 0.64
Epoch 35: 0.61
Epoch 40: 0.59
Epoch 45: 0.58
Epoch 50: 0.57 (Ã§ok az dÃ¼ÅŸÃ¼yor artÄ±k)

âœ… Loss yavaÅŸ yavaÅŸ dÃ¼zleÅŸiyor = Model optimuma yaklaÅŸÄ±yor!

Sorunlu GÃ¶rÃ¼nÃ¼m 1: Dalgalanma
Epoch 20: 0.75
Epoch 25: 0.65
Epoch 30: 0.80 â†‘ (yÃ¼kseldi!)
Epoch 35: 0.60
Epoch 40: 0.85 â†‘ (tekrar yÃ¼kseldi!)

âŒ Loss dalgalanÄ±yor = Learning rate Ã§ok yÃ¼ksek VEYA batch size Ã§ok kÃ¼Ã§Ã¼k!
Ã‡Ã¶zÃ¼m: learning_rate'i 0.001'den 0.0005'e dÃ¼ÅŸÃ¼rÃ¼n

Sorunlu GÃ¶rÃ¼nÃ¼m 2: Erken Durdurma
Epoch 20: 0.75
Epoch 25: 0.75 (deÄŸiÅŸmedi)
Epoch 30: 0.75 (hala aynÄ±)
Epoch 35: 0.75

âŒ Loss dÃ¼ÅŸmÃ¼yor = Model yerel minimumda sÄ±kÄ±ÅŸtÄ±!
Ã‡Ã¶zÃ¼m: Modeli yeniden baÅŸlatÄ±n, farklÄ± learning rate deneyin
```

**4. Final DeÄŸer (Son Epoch):**
```
Epoch 50'de loss ne olmalÄ±?

MÃ¼kemmel: < 0.3
  â”œâ”€ Model Ã§ok iyi Ã¶ÄŸrendi!
  â””â”€ Attention map'ler Ã§ok net olacak

Ä°yi: 0.3 - 0.6
  â”œâ”€ Model yeterli Ã¶ÄŸrendi
  â””â”€ Pratik kullanÄ±m iÃ§in uygun

Orta: 0.6 - 1.0
  â”œâ”€ Model kÄ±smen Ã¶ÄŸrendi
  â””â”€ Daha fazla epoch deneyin

KÃ¶tÃ¼: > 1.0
  â”œâ”€ Model yeterince Ã¶ÄŸrenemedi
  â””â”€ Parametreleri deÄŸiÅŸtirin

Ã–rnek:
Epoch 50: Loss = 0.42 â† Ä°yi bir sonuÃ§!
```

#### GerÃ§ek DÃ¼nya Ã–rnekleri

**Ã–rnek 1: MÃ¼kemmel EÄŸitim**
```
Veri: "Ben, BugÃ¼n, Okula, Gittim" (4 token, basit)
Parametreler: d_model=64, num_heads=4, lr=0.001

Grafik:
     3.0 |â—
         |  â—
     2.0 |    â—â—
         |       â—â—â—
Loss 1.0 |           â—â—â—â—
         |                â—â—â—â—â—
     0.0 |________________________â—â—â—â—â—â—â—â—
         0   10   20   30   40   50
              Epoch

âœ… DÃ¼zgÃ¼n azalÄ±ÅŸ, son deÄŸer 0.25, MÃœKEMMELl!
```

**Ã–rnek 2: Learning Rate Ã‡ok YÃ¼ksek**
```
Parametreler: lr=0.01 (10x fazla!)

Grafik:
     3.0 |â—
         | â— â—
     2.0 |  â—  â—
         |â—  â— â—  â—
Loss 1.0 | â—    â—  â—
         |â—  â—   â—
     0.0 |________________________
         0   10   20   30   40   50
              Epoch

âŒ DalgalÄ±, dÃ¼zensiz, dÃ¼ÅŸmÃ¼yor!
Ã‡Ã¶zÃ¼m: lr'yi 0.001'e dÃ¼ÅŸÃ¼rÃ¼n
```

**Ã–rnek 3: Learning Rate Ã‡ok DÃ¼ÅŸÃ¼k**
```
Parametreler: lr=0.00001 (100 kat az!)

Grafik:
     3.0 |â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         |
     2.0 |
         |
Loss 1.0 |
         |
     0.0 |________________________
         0   10   20   30   40   50
              Epoch

âŒ HiÃ§ dÃ¼ÅŸmÃ¼yor, dÃ¼z Ã§izgi!
Ã‡Ã¶zÃ¼m: lr'yi 0.001'e Ã§Ä±karÄ±n
```

**Ã–rnek 4: Overfitting**
```
Veri: 3 token (Ã§ok az!)
Parametreler: d_model=256 (Ã§ok bÃ¼yÃ¼k!), epochs=200

Grafik:
     3.0 |â—
         |  â—â—
     2.0 |     â—â—
         |        â—â—â—â—â—
Loss 1.0 |              â—â—â—â—â—â—
         |                     â—â—â—â—â—â—â”€â”€â”€ (dÃ¼zleÅŸti)
     0.0 |_____________________________  â•± (sonra yÃ¼kselmeye baÅŸladÄ±!)
         0   50   100  150  200
              Epoch

âŒ Ã–nce dÃ¼ÅŸtÃ¼, sonra tekrar yÃ¼kseldi!
Ã‡Ã¶zÃ¼m: Dropout artÄ±rÄ±n (0.3'e Ã§Ä±karÄ±n), erken durdurun
```

#### Grafikteki Anomaliler

**Anomali 1: Ani SÄ±Ã§rama**
```
Epoch 35: 0.65
Epoch 36: 0.64
Epoch 37: 0.63
Epoch 38: 0.62
Epoch 39: 15.87 â† ANÄ° SIÃ‡RAMA!
Epoch 40: NaN

Anlam: Exploding gradient! Model patladÄ±!
Ã‡Ã¶zÃ¼m: Learning rate'i yarÄ±ya indirin, gradient clipping ekleyin
```

**Anomali 2: Plato (DÃ¼zlÃ¼k)**
```
Epoch 20-50: Loss hep 1.25 civarÄ±nda

Anlam: Model yerel minimumda sÄ±kÄ±ÅŸtÄ±
Ã‡Ã¶zÃ¼m: Learning rate'i artÄ±rÄ±n veya modeli sÄ±fÄ±rdan baÅŸlatÄ±n
```

**Anomali 3: Negatif Loss**
```
Epoch 50: Loss = -0.45

Anlam: Kodda hata var! Loss negatif olamaz!
Ã‡Ã¶zÃ¼m: Bunu gÃ¶rÃ¼rseniz kodu inceleyin, bu bir bug
```

#### Parametrelerin Etkisi (KarÅŸÄ±laÅŸtÄ±rma)

**d_model Etkisi:**
```
d_model=32:  Final loss = 0.75 (yÃ¼ksek)
d_model=64:  Final loss = 0.42 (orta) â† STANDART
d_model=128: Final loss = 0.28 (dÃ¼ÅŸÃ¼k)
d_model=256: Final loss = 0.15 (Ã§ok dÃ¼ÅŸÃ¼k ama yavaÅŸ)

Kural: Daha bÃ¼yÃ¼k model â†’ Daha dÃ¼ÅŸÃ¼k loss (ama daha yavaÅŸ)
```

**num_heads Etkisi:**
```
num_heads=2:  Final loss = 0.55 (orta)
num_heads=4:  Final loss = 0.42 (iyi) â† STANDART
num_heads=8:  Final loss = 0.38 (daha iyi)
num_heads=16: Final loss = 0.36 (en iyi ama yavaÅŸ)

Kural: Daha fazla head â†’ Daha dÃ¼ÅŸÃ¼k loss (ama azalan getiri)
```

**dropout Etkisi:**
```
dropout=0.0: Loss hÄ±zla dÃ¼ÅŸer ama sonra yÃ¼kselir (overfitting)
dropout=0.1: Dengeli dÃ¼ÅŸÃ¼ÅŸ â† STANDART
dropout=0.3: YavaÅŸ ama stabil dÃ¼ÅŸÃ¼ÅŸ
dropout=0.5: Ã‡ok yavaÅŸ, belki hiÃ§ dÃ¼ÅŸmez

Kural: Dropout daha stabil ama daha yavaÅŸ Ã¶ÄŸrenme saÄŸlar
```

---

## ğŸ”¬ TÃ¼m Grafikleri Birlikte DeÄŸerlendirme

### Senaryo 1: MÃ¼kemmel SonuÃ§

**EÄŸitim GrafiÄŸi:**
- Loss 2.8'den 0.25'e dÃ¼ÅŸtÃ¼ âœ…
- DÃ¼zgÃ¼n, dalgasÄ±z eÄŸri âœ…
- Son 10 epoch dÃ¼zleÅŸmiÅŸ âœ…

**Attention Map:**
- AnlamlÄ± pattern'ler (fiil-Ã¶zne iliÅŸkisi) âœ…
- KÃ¶ÅŸegen koyu (kendi kendine bakmÄ±yor) âœ…
- Net, parlak baÄŸlantÄ±lar âœ…

**Q, K, V Matrisleri:**
- Her kelime farklÄ± pattern âœ…
- Benzer kelimeler benzer vector'ler âœ…
- V matrisi zengin ve karÄ±ÅŸÄ±k âœ…

**SONUÃ‡**: Model mÃ¼kemmel Ã¶ÄŸrenmiÅŸ! ğŸ‰

---

### Senaryo 2: KÃ¶tÃ¼ SonuÃ§

**EÄŸitim GrafiÄŸi:**
- Loss 2.8'de kaldÄ±, dÃ¼ÅŸmedi âŒ
- DÃ¼z bir Ã§izgi âŒ

**Attention Map:**
- TÃ¼m hÃ¼creler ~0.25 (eÅŸit daÄŸÄ±lÄ±m) âŒ
- KÃ¶ÅŸegen parlak âŒ
- Net pattern yok âŒ

**Q, K, V Matrisleri:**
- TÃ¼m satÄ±rlar aynÄ± pattern âŒ
- NÃ¶tr renkler (beyaz), aktivasyon yok âŒ

**SONUÃ‡**: Model Ã¶ÄŸrenemedi!
**Ã‡Ã¶zÃ¼m**: Learning rate artÄ±rÄ±n, daha fazla epoch deneyin

---

### Senaryo 3: Overfitting

**EÄŸitim GrafiÄŸi:**
- Loss baÅŸta dÃ¼ÅŸtÃ¼, sonra tekrar yÃ¼kseldi âš ï¸

**Attention Map:**
- AÅŸÄ±rÄ± keskin pattern'ler âš ï¸
- Sadece 1-2 baÄŸlantÄ± Ã§ok gÃ¼Ã§lÃ¼ âš ï¸

**Q, K, V Matrisleri:**
- AÅŸÄ±rÄ± parlak renkler (Ã§ok kÄ±rmÄ±zÄ±/mavi) âš ï¸

**SONUÃ‡**: Model ezberlemiÅŸ, genelleÅŸtiremiyor!
**Ã‡Ã¶zÃ¼m**: Dropout artÄ±rÄ±n (0.3), daha az epoch kullanÄ±n

---

## ğŸ“Œ HÄ±zlÄ± Referans Tablosu

### Grafik Kontrol Listesi

| Grafik | Ne Kontrol Edin | Ä°yi GÃ¶rÃ¼nÃ¼m | KÃ¶tÃ¼ GÃ¶rÃ¼nÃ¼m |
|--------|----------------|-------------|--------------|
| **Attention Map** | Ä°liÅŸki pattern'leri | Net, parlak baÄŸlantÄ±lar | EÅŸit daÄŸÄ±lÄ±m, kÃ¶ÅŸegen parlak |
| **Q, K, V** | VektÃ¶r Ã§eÅŸitliliÄŸi | KarÄ±ÅŸÄ±k renkler, farklÄ± satÄ±rlar | TÃ¼m satÄ±rlar aynÄ±, nÃ¶tr |
| **Training Loss** | DÃ¼ÅŸÃ¼ÅŸ trendi | DÃ¼zgÃ¼n azalÄ±ÅŸ, dÃ¼ÅŸÃ¼k final | DalgalÄ±, yÃ¼ksek final |

### Loss DeÄŸeri Yorumlama

| Final Loss | Yorum | YapÄ±lacak |
|------------|-------|-----------|
| < 0.3 | MÃ¼kemmel! | HiÃ§bir ÅŸey, devam edin |
| 0.3 - 0.6 | Ä°yi | Belki d_model artÄ±rÄ±n |
| 0.6 - 1.0 | Orta | Daha fazla epoch, lr ayarlayÄ±n |
| 1.0 - 2.0 | ZayÄ±f | Parametreleri deÄŸiÅŸtirin |
| > 2.0 | Ã‡ok kÃ¶tÃ¼ | Her ÅŸeyi deÄŸiÅŸtirin |
| NaN | HATA! | lr'yi Ã§ok dÃ¼ÅŸÃ¼rÃ¼n |

### Attention AÄŸÄ±rlÄ±k Yorumlama

| DeÄŸer | Renk | Anlam |
|-------|------|-------|
| 0.0 - 0.1 | Koyu mavi | Ä°liÅŸki yok, gÃ¶rmezden geliyor |
| 0.1 - 0.3 | AÃ§Ä±k mavi | ZayÄ±f iliÅŸki, az Ã¶nemli |
| 0.3 - 0.5 | Turuncu | Orta iliÅŸki, kÄ±smen Ã¶nemli |
| 0.5 - 0.7 | SarÄ± | GÃ¼Ã§lÃ¼ iliÅŸki, Ã§ok Ã¶nemli |
| 0.7 - 1.0 | Parlak sarÄ± | Ã‡ok gÃ¼Ã§lÃ¼ iliÅŸki, kritik |

---

## ğŸ¯ Pratik Egzersizler

### Egzersiz 1: Attention Map Okuma
1. ProgramÄ± aÃ§Ä±n, "Ben, BugÃ¼n, Okula, Gittim" veriyle eÄŸitin
2. Attention Map'i aÃ§Ä±n
3. "Gittim" satÄ±rÄ±nÄ± bulun
4. En parlak hÃ¼cre hangisi? (Cevap: "Ben" sÃ¼tunu olmalÄ±)
5. Bu neden mantÄ±klÄ±? (Cevap: Fiil Ã¶zne arÄ±yor!)

### Egzersiz 2: Loss GrafiÄŸi Yorumlama
1. Ä°lk loss deÄŸerini not alÄ±n (Ã¶rn: 2.87)
2. Final loss deÄŸerini not alÄ±n (Ã¶rn: 0.42)
3. FarkÄ± hesaplayÄ±n: 2.87 - 0.42 = 2.45 azalma
4. YÃ¼zde hesaplama: (2.45 / 2.87) Ã— 100 = %85.4 iyileÅŸme!
5. Grafik dÃ¼zgÃ¼n mÃ¼ yoksa dalgalÄ± mÄ±?

### Egzersiz 3: Q, K, V Pattern KarÅŸÄ±laÅŸtÄ±rma
1. Q matrisinde "Gittim" satÄ±rÄ±nÄ± bulun
2. K matrisinde "Ben" satÄ±rÄ±nÄ± bulun
3. Ä°lk 10 sÃ¼tunu karÅŸÄ±laÅŸtÄ±rÄ±n
4. Pattern'ler benzer mi? (Benzer olmalÄ±, ikisi de Ã¶zne-fiil iliÅŸkisi!)

---

## ğŸ’¡ Son Tavsiyeler

### Grafik Analizi Ä°Ã§in AltÄ±n Kurallar

1. **Ã–nce EÄŸitim GrafiÄŸini Ä°nceleyin**
   - Loss dÃ¼ÅŸmediyse, diÄŸer grafikler anlamsÄ±z!
   - Loss dÃ¼ÅŸtÃ¼yse, attention'a bakmaya deÄŸer

2. **Attention Map'te Hikaye ArayÄ±n**
   - "Bu kelime neden bu kelimeye bakÄ±yor?"
   - Dilbilgisel veya anlamsal mantÄ±k var mÄ±?

3. **Q, K, V'yi Ä°leri Seviye Ä°Ã§in SaklayÄ±n**
   - BaÅŸlangÄ±Ã§ta sadece Attention Map yeterli
   - DetaylÄ± analiz iÃ§in sonra inceleyin

4. **Parametreleri Tek Tek DeÄŸiÅŸtirin**
   - Her seferinde sadece bir parametreyi deÄŸiÅŸtirin
   - Etkiyi net gÃ¶rmek iÃ§in karÅŸÄ±laÅŸtÄ±rÄ±n

5. **Grafikleri Kaydedin ve KarÅŸÄ±laÅŸtÄ±rÄ±n**
   - PNG dosyalarÄ±nÄ± yan yana koyun
   - Zamanla pattern'leri anlamaya baÅŸlayacaksÄ±nÄ±z!

**Bu rehber ile artÄ±k Self-Attention grafiklerin profesyonel deÄŸerlendiricisisiniz! ğŸ“**

---

## ğŸ“Š Veri HazÄ±rlama Ä°puÃ§larÄ±

### Ä°yi Veri Ã–rnekleri

#### Dil Ä°ÅŸleme
```
âœ“ Ä°yi: AnlamlÄ± cÃ¼mleler
"Kedi mat Ã¼zerinde oturdu"
"Ben bugÃ¼n okula gittim"

âœ— KÃ¶tÃ¼: Rastgele kelimeler
"Masa araba gÃ¶kyÃ¼zÃ¼ bilgisayar"
```

#### Zaman Serisi
```
âœ“ Ä°yi: SÄ±ralÄ± veriler
"Ocak, Åubat, Mart, Nisan"
"Sabah, Ã–ÄŸle, AkÅŸam, Gece"

âœ— KÃ¶tÃ¼: KarÄ±ÅŸÄ±k sÄ±ra
"Mart, Ocak, Nisan, Åubat"
```

#### Kategorik Veriler
```
âœ“ Ä°yi: Ä°liÅŸkili kategoriler
"Elma, Armut, Muz, ÃœzÃ¼m" (meyveler)
"KÄ±rmÄ±zÄ±, Mavi, YeÅŸil, SarÄ±" (renkler)

âœ— KÃ¶tÃ¼: Ä°lgisiz kategoriler
"Elma, Araba, Pazartesi, Mavi"
```

### Veri Boyutu Ã–nerileri

```
Minimum: 3-4 token
Optimal: 5-10 token
Maksimum: 20-30 token (performans iÃ§in)

Not: Ã‡ok az token â†’ Basit iliÅŸkiler
     Ã‡ok fazla token â†’ YavaÅŸ eÄŸitim
```

---

## ğŸ¯ Ã–ÄŸrenme Hedefleri ve Kontrol Listesi

### Temel Seviye (1-2 hafta)
- [ ] Self-Attention'Ä±n ne olduÄŸunu anlÄ±yorum
- [ ] Q, K, V kavramlarÄ±nÄ± aÃ§Ä±klayabiliyorum
- [ ] Attention map'i okuyabiliyorum
- [ ] Basit parametreleri ayarlayabiliyorum
- [ ] Model kaydedip yÃ¼kleyebiliyorum

### Orta Seviye (3-4 hafta)
- [ ] Multi-head attention'Ä±n avantajlarÄ±nÄ± anlÄ±yorum
- [ ] Parametrelerin etkilerini tahmin edebiliyorum
- [ ] Attention pattern'lerini yorumlayabiliyorum
- [ ] Kendi verilerimle deney yapabiliyorum
- [ ] EÄŸitim grafiklerini analiz edebiliyorum

### Ä°leri Seviye (5+ hafta)
- [ ] Optimal hiperparametreleri bulabiliyorum
- [ ] Overfitting/underfitting'i tespit edebiliyorum
- [ ] KarmaÅŸÄ±k iliÅŸkileri modelleyebiliyorum
- [ ] FarklÄ± attention pattern'lerini karÅŸÄ±laÅŸtÄ±rabiliyorum
- [ ] GerÃ§ek problemlere uygulayabiliyorum

---

## ğŸ’¡ SÄ±k Sorulan Sorular

### Q: EÄŸitim ne kadar sÃ¼rmeli?
**A**: Veri boyutuna baÄŸlÄ±:
- 3-5 token: 20-50 epoch (~1 dakika)
- 6-10 token: 50-100 epoch (~2-3 dakika)
- 10+ token: 100-200 epoch (~5-10 dakika)

### Q: En iyi parametreler neler?
**A**: Veri boyutuna gÃ¶re deÄŸiÅŸir:
```
KÃ¼Ã§Ã¼k veri (<10 token):
d_model=64, num_heads=4, dropout=0.1, lr=0.001

Orta veri (10-20 token):
d_model=128, num_heads=8, dropout=0.2, lr=0.001

BÃ¼yÃ¼k veri (>20 token):
d_model=256, num_heads=16, dropout=0.3, lr=0.0005
```

### Q: GPU gerekli mi?
**A**: HayÄ±r! CPU ile de rahatÃ§a Ã§alÄ±ÅŸÄ±r. GPU varsa otomatik kullanÄ±lÄ±r.

### Q: Modeller ne kadar yer kaplar?
**A**: Model baÅŸÄ±na ~1-10 MB (parametrelere baÄŸlÄ±)

### Q: KaÃ§ model kaydedebilirim?
**A**: SÄ±nÄ±rsÄ±z! Ama dÃ¼zenli temizlik yapÄ±n.

---

## ğŸš€ Ä°leri Seviye Deneyler

### Deney 1: Positional Encoding Etkisi
```
AynÄ± kelimeleri farklÄ± sÄ±ralarda deneyin:
Veri 1: "Kedi kÃ¶pek kuÅŸ"
Veri 2: "KuÅŸ kÃ¶pek kedi"

GÃ¶zlem: FarklÄ± attention pattern'leri gÃ¶receksiniz!
Bu, positional encoding'in Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.
```

### Deney 2: Uzun Mesafe BaÄŸÄ±mlÄ±lÄ±klar
```
Veri: "Ben, BugÃ¼n, Ã‡ok, Erken, KalktÄ±m, Ve, Ä°ÅŸe, Gittim"

GÃ¶zlem: "Ben" ve "Gittim" arasÄ±ndaki iliÅŸki
7 kelime mesafeden tespit edilebilir mi?
```

### Deney 3: EÅŸ AnlamlÄ± Kelimeler
```
Veri: "GÃ¼zel, Ã‡irkin, HoÅŸ, Ä°ÄŸrenÃ§"

GÃ¶zlem: "GÃ¼zel" ve "HoÅŸ" benzer attention pattern'leri
oluÅŸturur mu? (EÅŸ anlamlÄ± olduklarÄ± iÃ§in olmalÄ±!)
```

---

## ğŸ“ˆ Ä°lerleme Takibi

### GÃ¼nlÃ¼k Kontrol Listesi
```
GÃ¼n 1: [ ] ProgramÄ± Ã§alÄ±ÅŸtÄ±r, temel kavramlarÄ± Ã¶ÄŸren
GÃ¼n 2: [ ] Ä°lk Ã¶rneÄŸi dene, attention map'i incele
GÃ¼n 3: [ ] num_heads'i deÄŸiÅŸtir, farkÄ± gÃ¶zle
GÃ¼n 4: [ ] d_model'i deÄŸiÅŸtir, etkiyi analiz et
GÃ¼n 5: [ ] Kendi verini oluÅŸtur, dene
GÃ¼n 6: [ ] Optimal parametreleri bul
GÃ¼n 7: [ ] Ã–ÄŸrendiklerini not al, Ã¶zetle
```

### HaftalÄ±k Hedefler
```
Hafta 1: Temel kavramlar ve ilk deneyler
Hafta 2: Parametre etkilerini anlama
Hafta 3: KarmaÅŸÄ±k deneyler yapma
Hafta 4: GerÃ§ek problemlere uygulama
```

---

## ğŸ“ SonuÃ§

Bu program ile:
- âœ… Self-Attention'Ä± **gÃ¶rsel olarak** anlayacaksÄ±nÄ±z
- âœ… Q, K, V kavramlarÄ±nÄ± **Ã¶rneklerle** Ã¶ÄŸreneceksiniz
- âœ… Parametrelerin etkilerini **aktif olarak** gÃ¶zleyeceksiniz
- âœ… Modern AI sistemlerinin temelini **deneyerek** Ã¶ÄŸreneceksiniz

**Ä°yi Ã¶ÄŸrenmeler! ğŸš€**

---

## ğŸ“ Not Alma Åablonu

Her deney iÃ§in bu ÅŸablonu kullanÄ±n:

```markdown
## Deney: [Deney AdÄ±]
Tarih: [Tarih]

### Parametreler
- Veri: [Token listesi]
- d_model: [DeÄŸer]
- num_heads: [DeÄŸer]
- dropout: [DeÄŸer]
- learning_rate: [DeÄŸer]
- epochs: [DeÄŸer]

### SonuÃ§lar
- Final Loss: [DeÄŸer]
- EÄŸitim SÃ¼resi: [Dakika]
- GÃ¶zlemler: [Notlar]

### Attention Pattern'leri
- En gÃ¼Ã§lÃ¼ baÄŸlantÄ±: [Token1] â†’ [Token2] (aÄŸÄ±rlÄ±k: X.XX)
- En zayÄ±f baÄŸlantÄ±: [Token3] â†’ [Token4] (aÄŸÄ±rlÄ±k: X.XX)

### Ã–ÄŸrenilen Dersler
1. [Ders 1]
2. [Ders 2]
3. [Ders 3]

### Sonraki AdÄ±mlar
- [ ] [YapÄ±lacak 1]
- [ ] [YapÄ±lacak 2]
```

**Bu ÅŸablonu her deney iÃ§in kullanarak ilerlemenizi takip edin!**
