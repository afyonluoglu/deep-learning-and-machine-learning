# ğŸ“ Self-Attention Ã–ÄŸrenme AracÄ±

Yapay sinir aÄŸlarÄ±nda **Self-Attention** mekanizmasÄ±nÄ± interaktif olarak Ã¶ÄŸrenmek iÃ§in profesyonel bir eÄŸitim aracÄ±.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![CustomTkinter](https://img.shields.io/badge/CustomTkinter-5.0+-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [Kurulum](#-kurulum)
- [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)
- [KullanÄ±m KÄ±lavuzu](#-kullanÄ±m-kÄ±lavuzu)
- [Self-Attention Nedir?](#-self-attention-nedir)
- [Parametreler](#ï¸-parametreler)
- [GÃ¶rselleÅŸtirmeler](#-gÃ¶rselleÅŸtirmeler)
- [Ã–rnekler](#-Ã¶rnekler)
- [Model YÃ¶netimi](#-model-yÃ¶netimi)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Sorun Giderme](#-sorun-giderme)
- [KatkÄ±da Bulunma](#-katkÄ±da-bulunma)
- [Lisans](#-lisans)

---

## âœ¨ Ã–zellikler

### ğŸ¯ EÄŸitim Ã–zellikleri
- âœ… **Ä°nteraktif Ã–ÄŸrenme**: Query, Key, Value kavramlarÄ±nÄ± Ã¶rneklerle Ã¶ÄŸrenin
- âœ… **GerÃ§ek ZamanlÄ± EÄŸitim**: Modeli canlÄ± olarak eÄŸitin ve sonuÃ§larÄ± anÄ±nda gÃ¶rÃ¼n
- âœ… **Parametre Deneyleri**: Parametreleri deÄŸiÅŸtirerek etkilerini gÃ¶zlemleyin
- âœ… **Multi-Head Attention**: Birden fazla attention head'in gÃ¼cÃ¼nÃ¼ keÅŸfedin

### ğŸ“Š GÃ¶rselleÅŸtirme
- âœ… **Attention Map**: Token'lar arasÄ± iliÅŸkileri Ä±sÄ± haritasÄ± olarak gÃ¶rÃ¼n
- âœ… **Q, K, V Matrisleri**: Query, Key, Value matrislerini gÃ¶rselleÅŸtirin
- âœ… **EÄŸitim Grafikleri**: Loss deÄŸiÅŸimini gerÃ§ek zamanlÄ± takip edin
- âœ… **Profesyonel Grafikler**: YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼, karanlÄ±k tema grafikleri

### ğŸ’¾ Model YÃ¶netimi
- âœ… **Model Kaydetme**: EÄŸitilmiÅŸ modelleri tÃ¼m parametreleriyle kaydedin
- âœ… **Model YÃ¼kleme**: KaydedilmiÅŸ modelleri tekrar kullanÄ±n
- âœ… **KonfigÃ¼rasyon Saklama**: TÃ¼m ayarlar otomatik kaydedilir
- âœ… **Zaman DamgasÄ±**: Her model iÃ§in benzersiz versiyon takibi

### ğŸ¨ KullanÄ±cÄ± ArayÃ¼zÃ¼
- âœ… **Modern TasarÄ±m**: CustomTkinter ile modern, koyu tema arayÃ¼z
- âœ… **KullanÄ±cÄ± Dostu**: Sezgisel kontroller ve aÃ§Ä±klayÄ±cÄ± etiketler
- âœ… **HTML YardÄ±m**: DetaylÄ±, Ã¶rnekli HTML yardÄ±m dÃ¶kÃ¼manÄ±
- âœ… **TÃ¼rkÃ§e Destek**: Tam TÃ¼rkÃ§e arayÃ¼z ve dÃ¶kÃ¼manlar

---

## ğŸš€ Kurulum

### Gereksinimler

```bash
Python 3.8+
```

### Gerekli KÃ¼tÃ¼phaneler

```bash
pip install torch torchvision
pip install customtkinter
pip install matplotlib
pip install seaborn
pip install numpy
```

### HÄ±zlÄ± Kurulum

```bash
# Repoyu klonlayÄ±n
git clone <repo-url>
cd "04 Self-Attention"

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin
pip install -r requirements.txt
```

### requirements.txt

Program ile birlikte aÅŸaÄŸÄ±daki `requirements.txt` dosyasÄ± oluÅŸturulmuÅŸtur:

```txt
torch>=2.0.0
customtkinter>=5.0.0
matplotlib>=3.5.0
seaborn>=0.12.0
numpy>=1.21.0
```

---

## ğŸ¯ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. ProgramÄ± Ã‡alÄ±ÅŸtÄ±rma

```bash
python main.py
```

### 2. Ä°lk Deneyiniz

1. **Ã–rnek Veri**: "Kelime Dizisi" seÃ§in (varsayÄ±lan)
2. **Parametreler**: VarsayÄ±lan deÄŸerleri kullanÄ±n
3. **EÄŸitim**: "ğŸš€ EÄŸitimi BaÅŸlat" butonuna tÄ±klayÄ±n
4. **SonuÃ§lar**: GÃ¶rselleÅŸtirmeleri inceleyin

### 3. Ä°lk Deneyiniz

```
Veri: Ben, BugÃ¼n, Okula, Gittim
Parametreler: d_model=64, num_heads=4, dropout=0.1
Epoch: 50
```

**GÃ¶zlem**: "Gittim" kelimesinin "Ben" ve "Okula" ile gÃ¼Ã§lÃ¼ attention baÄŸlantÄ±larÄ± kurduÄŸunu gÃ¶receksiniz!

---

## ğŸ“– KullanÄ±m KÄ±lavuzu

### AdÄ±m 1: Veri HazÄ±rlama

```
ğŸ“Š Ã–rnek Veri Seti seÃ§eneÄŸinden birini seÃ§in:
- Kelime Dizisi
- CÃ¼mle Analizi  
- Zaman Serisi
- GÃ¶rÃ¼ntÃ¼ ParÃ§alarÄ±
- Ã–zel Veri (kendi verinizi girin)
```

**Not**: Her satÄ±r bir token'Ä± temsil eder.

### AdÄ±m 2: Parametre Ayarlama

```
âš™ï¸ Self-Attention Parametreleri:
- Embedding Boyutu (d_model): 32-512
- Attention Head SayÄ±sÄ±: 1-16
- Dropout OranÄ±: 0.0-0.5
- Ã–ÄŸrenme HÄ±zÄ±: 0.0001-0.01
```

### AdÄ±m 3: EÄŸitim

```
ğŸ¯ EÄŸitim Kontrolleri:
- Epoch SayÄ±sÄ±: 20-200 (Ã¶nerilen: 50)
- Batch Size: 4-16 (Ã¶nerilen: 8)
- "ğŸš€ EÄŸitimi BaÅŸlat" butonuna tÄ±klayÄ±n
```

### AdÄ±m 4: SonuÃ§larÄ± Ä°nceleme

```
Tablar:
1. ğŸ” Attention Map - Token'lar arasÄ± iliÅŸkiler
2. ğŸ“Š Q, K, V Matrisleri - VektÃ¶r temsilleri
3. ğŸ“ˆ EÄŸitim GrafiÄŸi - Loss deÄŸiÅŸimi
4. ğŸ’¡ AÃ§Ä±klama - DetaylÄ± bilgiler
```

---

## ğŸ§  Self-Attention Nedir?

### Temel Kavram

Self-Attention, bir dizideki her elemanÄ±n diÄŸer tÃ¼m elemanlarla **iliÅŸkisini Ã¶ÄŸrenen** gÃ¼Ã§lÃ¼ bir mekanizmadÄ±r.

### Ana BileÅŸenler

#### 1. Query (Q) - "Neyi arÄ±yorum?"
```
Her token, Query vektÃ¶rÃ¼ ile diÄŸer token'lardan 
ne tÃ¼r bilgi istediÄŸini belirtir.
```

#### 2. Key (K) - "Ben neyim?"
```
Her token, Key vektÃ¶rÃ¼ ile kendini tanÄ±tÄ±r ve 
diÄŸer token'larÄ±n sorgularÄ±na cevap verir.
```

#### 3. Value (V) - "Ne bilgi taÅŸÄ±yorum?"
```
Attention hesaplandÄ±ktan sonra aktarÄ±lacak 
gerÃ§ek bilgiyi iÃ§erir.
```

### Matematiksel FormÃ¼l

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

**AdÄ±mlar**:
1. Q, K, V hesapla: `Q = X Ã— W_q`, `K = X Ã— W_k`, `V = X Ã— W_v`
2. SkorlarÄ± hesapla: `Scores = QK^T / âˆšd_k`
3. Softmax uygula: `Weights = softmax(Scores)`
4. Value'larÄ± aÄŸÄ±rlÄ±klandÄ±r: `Output = Weights Ã— V`

### Multi-Head Attention

```
Birden fazla attention "head" kullanarak 
farklÄ± iliÅŸki tÃ¼rlerini paralel olarak Ã¶ÄŸrenme
```

**Avantajlar**:
- Her head farklÄ± bir pattern Ã¶ÄŸrenir
- Daha zengin temsiller
- Daha iyi performans

---

## âš™ï¸ Parametreler

### d_model (Embedding Boyutu)

| DeÄŸer | AÃ§Ä±klama | KullanÄ±m |
|-------|----------|----------|
| 32-64 | KÃ¼Ã§Ã¼k, hÄ±zlÄ± | Deneyler iÃ§in |
| 128-256 | Orta, dengeli | Ã‡oÄŸu uygulama |
| 512+ | BÃ¼yÃ¼k, zengin | KarmaÅŸÄ±k problemler |

**Etki**: â†‘ = Daha zengin temsil, â†“ = Daha hÄ±zlÄ±

### num_heads (Head SayÄ±sÄ±)

| DeÄŸer | AÃ§Ä±klama | d_model Uyumu |
|-------|----------|---------------|
| 1-2 | Az head | Basit iliÅŸkiler |
| 4-8 | Orta | Dengeli |
| 12-16 | Ã‡ok head | KarmaÅŸÄ±k iliÅŸkiler |

**Ã–nemli**: `d_model % num_heads == 0` olmalÄ±!

### dropout

| DeÄŸer | AÃ§Ä±klama | KullanÄ±m |
|-------|----------|----------|
| 0.0 | Dropout yok | KÃ¼Ã§Ã¼k veri |
| 0.1-0.2 | Hafif | Standart |
| 0.3-0.5 | GÃ¼Ã§lÃ¼ | Overfitting varsa |

**Etki**: Overfitting'i Ã¶nler

### learning_rate

| DeÄŸer | AÃ§Ä±klama | Durum |
|-------|----------|-------|
| 0.0001 | Ã‡ok yavaÅŸ | Stabil eÄŸitim |
| 0.001 | Orta | Standart |
| 0.01 | HÄ±zlÄ± | Dikkatli kullanÄ±n |

**Etki**: Ã–ÄŸrenme hÄ±zÄ± ve kararlÄ±lÄ±k dengesi

---

## ğŸ“Š GÃ¶rselleÅŸtirmeler

### 1. ğŸ” Attention Map

```
IsÄ± haritasÄ± formatÄ±nda attention aÄŸÄ±rlÄ±klarÄ±

SatÄ±rlar: Query token'larÄ±
SÃ¼tunlar: Key token'larÄ±
Renkler: Ä°liÅŸki gÃ¼cÃ¼ (0-1)

Parlak = GÃ¼Ã§lÃ¼ iliÅŸki
Koyu = ZayÄ±f iliÅŸki
```

**Ã–rnek Yorum**:
```
"Oturdu" satÄ±rÄ±nda "Kedi" ve "Mat" sÃ¼tunlarÄ± parlaksa,
"oturdu" kelimesi bu kelimelere gÃ¼Ã§lÃ¼ attention veriyor.
```

### 2. ğŸ“Š Q, K, V Matrisleri

```
Her token iÃ§in Q, K, V vektÃ¶rlerinin gÃ¶rselleÅŸtirilmesi

Her satÄ±r: Bir token
Renkler: VektÃ¶r deÄŸerleri
- KÄ±rmÄ±zÄ±: Pozitif
- Mavi: Negatif  
- Beyaz: SÄ±fÄ±ra yakÄ±n
```

### 3. ğŸ“ˆ EÄŸitim GrafiÄŸi

```
Loss deÄŸiÅŸiminin epoch'lara gÃ¶re grafiÄŸi

X Ekseni: Epoch sayÄ±sÄ±
Y Ekseni: Loss deÄŸeri

Ä°deal: Azalan trend
Dikkat: Plato = durma, artÄ±ÅŸ = overfitting
```

**TÃ¼m grafikler `outputs/` klasÃ¶rÃ¼ne kaydedilir!**

---

## ğŸ“š Ã–rnekler

### Ã–rnek 1: Basit CÃ¼mle

```python
Veri:
Kedi
Mat
Ãœzerinde
Oturdu

Parametreler:
- d_model: 64
- num_heads: 4
- epochs: 50

Beklenen: "Oturdu" -> "Kedi", "Mat" gÃ¼Ã§lÃ¼ baÄŸlantÄ±
```

### Ã–rnek 2: Zaman Serisi

```python
Veri:
Pazartesi
SalÄ±
Ã‡arÅŸamba
PerÅŸembe
Cuma

Parametreler:
- d_model: 128
- num_heads: 8
- epochs: 100

Beklenen: ArdÄ±ÅŸÄ±k gÃ¼nler arasÄ± gÃ¼Ã§lÃ¼ baÄŸlantÄ±
```

### Ã–rnek 3: Parametre KarÅŸÄ±laÅŸtÄ±rma

```python
Deney 1:
- num_heads: 2
- Model kaydet: "az_head"

Deney 2:
- num_heads: 8
- Model kaydet: "cok_head"

KarÅŸÄ±laÅŸtÄ±r: Attention map farklarÄ±
```

### Ã–rnek 4: Dropout Etkisi

```python
Deney 1:
- dropout: 0.0
- epochs: 100
- GÃ¶zlem: Overfitting olabilir

Deney 2:
- dropout: 0.3
- epochs: 100
- GÃ¶zlem: Daha genel model
```

---

## ğŸ’¾ Model YÃ¶netimi

### Model Kaydetme

```python
1. EÄŸitimi tamamlayÄ±n
2. Model adÄ± girin: "deneme_model"
3. "ğŸ’¾ Modeli Kaydet" tÄ±klayÄ±n
4. Otomatik kaydedilir: "deneme_model_20250102_143052"
```

**Kaydedilen Bilgiler**:
- Model aÄŸÄ±rlÄ±klarÄ±
- TÃ¼m parametreler
- Vocabulary
- EÄŸitim geÃ§miÅŸi
- Zaman damgasÄ±

### Model YÃ¼kleme

```python
1. "ğŸ“‚ Model YÃ¼kle" tÄ±klayÄ±n
2. Listeden model seÃ§in
3. Model tÃ¼m ayarlarÄ±yla yÃ¼klenir
4. Hemen kullanÄ±ma hazÄ±r!
```

### Dosya YapÄ±sÄ±

```
models/
â””â”€â”€ deneme_model_20250102_143052/
    â”œâ”€â”€ model_weights.pth      # PyTorch aÄŸÄ±rlÄ±klarÄ±
    â”œâ”€â”€ full_model.pth         # Tam model
    â”œâ”€â”€ config.json            # Parametreler
    â””â”€â”€ model_info.json        # Meta bilgiler
```

### Model KarÅŸÄ±laÅŸtÄ±rma

```python
# FarklÄ± konfigÃ¼rasyonlarÄ± kaydedin
Model 1: d_model=64, num_heads=4
Model 2: d_model=128, num_heads=8
Model 3: d_model=64, num_heads=2

# SÄ±rayla yÃ¼kleyip sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rÄ±n
```

---

## ğŸ“ Proje YapÄ±sÄ±

```
04 Self-Attention/
â”‚
â”œâ”€â”€ main.py                      # Ana program
â”œâ”€â”€ self_attention_module.py     # Self-Attention implementasyonu
â”œâ”€â”€ visualization_module.py      # GÃ¶rselleÅŸtirme fonksiyonlarÄ±
â”œâ”€â”€ model_manager.py             # Model yÃ¶netimi
â”œâ”€â”€ help.html                    # HTML yardÄ±m dosyasÄ±
â”œâ”€â”€ README.md                    # Bu dosya
â”œâ”€â”€ requirements.txt             # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”‚
â”œâ”€â”€ outputs/                     # Ã‡Ä±ktÄ± dosyalarÄ±
â”‚   â”œâ”€â”€ attention_map.png
â”‚   â”œâ”€â”€ qkv_matrices.png
â”‚   â””â”€â”€ training_history.png
â”‚
â””â”€â”€ models/                      # KaydedilmiÅŸ modeller
    â””â”€â”€ model_name_timestamp/
        â”œâ”€â”€ model_weights.pth
        â”œâ”€â”€ full_model.pth
        â”œâ”€â”€ config.json
        â””â”€â”€ model_info.json
```

### Dosya AÃ§Ä±klamalarÄ±

| Dosya | AÃ§Ä±klama |
|-------|----------|
| `main.py` | Ana uygulama ve GUI |
| `self_attention_module.py` | Self-Attention katmanlarÄ± ve eÄŸitim |
| `visualization_module.py` | Matplotlib grafikleri |
| `model_manager.py` | Model kaydetme/yÃ¼kleme |
| `help.html` | DetaylÄ± kullanÄ±m kÄ±lavuzu |

---

## ğŸ› Sorun Giderme

### YaygÄ±n Hatalar

#### 1. "d_model must be divisible by num_heads"

```
Problem: d_model, num_heads'e tam bÃ¶lÃ¼nmÃ¼yor
Ã‡Ã¶zÃ¼m: d_model = num_heads Ã— k (k bir tam sayÄ±)
Ã–rnek: d_model=64, num_heads=4 âœ“
       d_model=64, num_heads=5 âœ—
```

#### 2. Loss NaN Oldu

```
Problem: Learning rate Ã§ok yÃ¼ksek
Ã‡Ã¶zÃ¼m: learning_rate'i dÃ¼ÅŸÃ¼rÃ¼n (Ã¶rn: 0.0001)
```

#### 3. Loss DÃ¼ÅŸmÃ¼yor

```
Problem: Learning rate Ã§ok dÃ¼ÅŸÃ¼k
Ã‡Ã¶zÃ¼m: learning_rate'i artÄ±rÄ±n (Ã¶rn: 0.001)
```

#### 4. Out of Memory

```
Problem: Batch size veya model Ã§ok bÃ¼yÃ¼k
Ã‡Ã¶zÃ¼m: 
- batch_size'Ä± kÃ¼Ã§Ã¼ltÃ¼n
- d_model'i azaltÄ±n
- num_heads'i azaltÄ±n
```

#### 5. Ã‡ok YavaÅŸ EÄŸitim

```
Problem: Parametreler Ã§ok bÃ¼yÃ¼k
Ã‡Ã¶zÃ¼m:
- d_model'i dÃ¼ÅŸÃ¼rÃ¼n (256 â†’ 128)
- num_heads'i azaltÄ±n (8 â†’ 4)
- epochs'u azaltÄ±n
```

### GPU KullanÄ±mÄ±

```python
# Program otomatik olarak GPU kullanÄ±r (varsa)
# Kontrol iÃ§in:
import torch
print(torch.cuda.is_available())  # True = GPU var
print(torch.cuda.get_device_name(0))  # GPU adÄ±
```

### Grafikler GÃ¶rÃ¼nmÃ¼yor

```
Problem: Matplotlib backend sorunu
Ã‡Ã¶zÃ¼m: Python'u yÃ¶netici olarak Ã§alÄ±ÅŸtÄ±rÄ±n
```

---

## ğŸ’¡ Ä°puÃ§larÄ± ve En Ä°yi Pratikler

### Ã–ÄŸrenme Stratejisi

1. **Hafta 1**: Temel kavramlarÄ± anlayÄ±n
   - Q, K, V nedir?
   - Attention nasÄ±l hesaplanÄ±r?

2. **Hafta 2**: Basit Ã¶rneklerle pratik
   - Ã–rnek veri setlerini deneyin
   - Attention map'leri inceleyin

3. **Hafta 3**: Parametre deneyleri
   - Tek tek parametreleri deÄŸiÅŸtirin
   - Etkileri gÃ¶zlemleyin

4. **Hafta 4**: Kendi verileriniz
   - Ã–zel veri setleri oluÅŸturun
   - GerÃ§ek problemlere uygulayÄ±n

### Deney Yapma Teknikleri

#### Tek DeÄŸiÅŸken KuralÄ±
```
Her seferinde sadece BÄ°R parametreyi deÄŸiÅŸtirin
Ã–rnek: Ä°lk num_heads=4, sonra num_heads=8
```

#### KayÄ±t Tutma
```
Her deneyden sonra:
1. Modeli kaydedin (aÃ§Ä±klayÄ±cÄ± isimle)
2. Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±n
3. SonuÃ§larÄ± not edin
```

#### KarÅŸÄ±laÅŸtÄ±rma
```
1. Baseline oluÅŸturun (standart parametreler)
2. Her deÄŸiÅŸikliÄŸi baseline ile karÅŸÄ±laÅŸtÄ±rÄ±n
3. En iyi sonucu not edin
```

### Parametre SeÃ§imi Rehberi

```python
# KÃ¼Ã§Ã¼k Veri (< 10 token)
d_model = 64
num_heads = 4
epochs = 50

# Orta Veri (10-50 token)
d_model = 128
num_heads = 8
epochs = 100

# BÃ¼yÃ¼k Veri (> 50 token)
d_model = 256
num_heads = 16
epochs = 200
```

---

## ğŸ“ Ek Kaynaklar

### Ã–nemli Makaleler

1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - Self-Attention'Ä±n tanÄ±tÄ±ldÄ±ÄŸÄ± orijinal makale

2. **"BERT: Pre-training of Deep Bidirectional Transformers"** (Devlin et al., 2018)
   - BERT modelinde Self-Attention kullanÄ±mÄ±

3. **"Language Models are Few-Shot Learners"** (Brown et al., 2020)
   - GPT-3 ve bÃ¼yÃ¼k dil modelleri

### Online Kaynaklar

- ğŸ“ **The Illustrated Transformer** - Jay Alammar
- ğŸ¥ **Attention Mechanism** - StatQuest
- ğŸ“š **CS224N** - Stanford NLP Dersleri
- ğŸŒ **Hugging Face Tutorials** - Transformer'lar

### Kitaplar

- ğŸ“– "Deep Learning" - Ian Goodfellow
- ğŸ“– "Natural Language Processing with Transformers" - Lewis Tunstall
- ğŸ“– "Dive into Deep Learning" - Aston Zhang

---

## ğŸ¤ KatkÄ±da Bulunma

Projeye katkÄ±da bulunmak isterseniz:

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

### KatkÄ± AlanlarÄ±

- ğŸ› Bug dÃ¼zeltmeleri
- âœ¨ Yeni Ã¶zellikler
- ğŸ“ DokÃ¼mantasyon iyileÅŸtirmeleri
- ğŸŒ Ã‡eviri (Ä°ngilizce, vb.)
- ğŸ¨ UI/UX iyileÅŸtirmeleri

---

## ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

---

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z veya Ã¶nerileriniz iÃ§in:

- ğŸ“§ Email: [your-email]
- ğŸ› Issues: [GitHub Issues]
- ğŸ’¬ Discussions: [GitHub Discussions]

---

## ğŸ™ TeÅŸekkÃ¼rler

Bu proje aÅŸaÄŸÄ±daki aÃ§Ä±k kaynak projeleri kullanÄ±r:

- **PyTorch** - Deep Learning framework
- **CustomTkinter** - Modern GUI toolkit
- **Matplotlib** - GÃ¶rselleÅŸtirme
- **Seaborn** - Ä°statistiksel grafikler
- **NumPy** - SayÄ±sal hesaplamalar

---


<div align="center">

### ğŸ“ Ä°yi Ã–ÄŸrenmeler!

**Self-Attention mekanizmasÄ±nÄ± anlamak, modern yapay zeka sistemlerini anlamanÄ±n temelidir.**

Bol bol deney yapÄ±n ve keÅŸfedin! ğŸš€

---

Made with â¤ï¸ for AI Education

</div>
