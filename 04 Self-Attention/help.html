<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Attention Ã–ÄŸrenme AracÄ± - YardÄ±m</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #e0e0e0;
            background: linear-gradient(135deg, #1e1e2e 0%, #2d2d44 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(30, 30, 46, 0.95);
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.5);
        }
        
        h1 {
            color: #61afef;
            text-align: center;
            margin-bottom: 10px;
            font-size: 2.5em;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
        }
        
        .subtitle {
            text-align: center;
            color: #98c379;
            margin-bottom: 40px;
            font-size: 1.2em;
        }
        
        h2 {
            color: #e5c07b;
            margin-top: 30px;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #61afef;
            font-size: 1.8em;
        }
        
        h3 {
            color: #c678dd;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.4em;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        .section {
            margin-bottom: 40px;
            padding: 20px;
            background: rgba(45, 45, 68, 0.5);
            border-radius: 10px;
            border-left: 4px solid #61afef;
        }
        
        .formula {
            background: #282c34;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            color: #98c379;
            border-left: 4px solid #98c379;
            overflow-x: auto;
        }
        
        .example {
            background: #2d3748;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            border-left: 4px solid #e5c07b;
        }
        
        .example-title {
            color: #e5c07b;
            font-weight: bold;
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        .highlight {
            background: rgba(97, 175, 239, 0.2);
            padding: 3px 8px;
            border-radius: 4px;
            color: #61afef;
            font-weight: bold;
        }
        
        .warning {
            background: rgba(229, 192, 123, 0.2);
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #e5c07b;
            margin: 15px 0;
        }
        
        .tip {
            background: rgba(152, 195, 121, 0.2);
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #98c379;
            margin: 15px 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: #282c34;
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: #61afef;
            color: #1e1e2e;
            padding: 12px;
            text-align: left;
            font-weight: bold;
        }
        
        td {
            padding: 12px;
            border-bottom: 1px solid #444;
        }
        
        tr:hover {
            background: rgba(97, 175, 239, 0.1);
        }
        
        .button-demo {
            display: inline-block;
            padding: 10px 20px;
            background: #61afef;
            color: #1e1e2e;
            border-radius: 5px;
            text-decoration: none;
            font-weight: bold;
            margin: 5px;
        }
        
        .toc {
            background: rgba(97, 175, 239, 0.1);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        
        .toc h3 {
            color: #61afef;
            margin-bottom: 15px;
        }
        
        .toc ul {
            list-style-type: none;
            margin-left: 0;
        }
        
        .toc a {
            color: #98c379;
            text-decoration: none;
            transition: color 0.3s;
        }
        
        .toc a:hover {
            color: #61afef;
        }
        
        .image-placeholder {
            background: #282c34;
            padding: 40px;
            text-align: center;
            border-radius: 8px;
            margin: 20px 0;
            border: 2px dashed #61afef;
        }
        
        code {
            background: #282c34;
            padding: 2px 6px;
            border-radius: 3px;
            color: #98c379;
            font-family: 'Courier New', monospace;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“ Self-Attention Ã–ÄŸrenme AracÄ±</h1>
        <p class="subtitle">Yapay Sinir AÄŸlarÄ±nda Self-Attention MekanizmasÄ±nÄ± Ã–ÄŸrenin</p>
        
        <div class="toc">
            <h3>ğŸ“‘ Ä°Ã§indekiler</h3>
            <ul>
                <li><a href="#giris">1. GiriÅŸ ve Temel Kavramlar</a></li>
                <li><a href="#nasil-calisir">2. Self-Attention NasÄ±l Ã‡alÄ±ÅŸÄ±r?</a></li>
                <li><a href="#kullanim">3. Program NasÄ±l KullanÄ±lÄ±r?</a></li>
                <li><a href="#parametreler">4. Parametreler ve Etkileri</a></li>
                <li><a href="#ornekler">5. Pratik Ã–rnekler</a></li>
                <li><a href="#gorsellestirme">6. GÃ¶rselleÅŸtirmeleri Anlama</a></li>
                <li><a href="#model-yonetimi">7. Model YÃ¶netimi</a></li>
                <li><a href="#ipuclari">8. Ä°puÃ§larÄ± ve En Ä°yi Pratikler</a></li>
            </ul>
        </div>
        
        <!-- BÃ¶lÃ¼m 1: GiriÅŸ -->
        <div class="section" id="giris">
            <h2>1. ğŸŒŸ GiriÅŸ ve Temel Kavramlar</h2>
            
            <h3>Self-Attention Nedir?</h3>
            <p>
                Self-Attention, bir dizideki her elemanÄ±n diÄŸer tÃ¼m elemanlarla iliÅŸkisini Ã¶ÄŸrenen 
                gÃ¼Ã§lÃ¼ bir mekanizmadÄ±r. Transformer mimarisinin temel taÅŸÄ±dÄ±r ve GPT, BERT gibi 
                modern dil modellerinin baÅŸarÄ±sÄ±nÄ±n arkasÄ±ndaki ana unsurdur.
            </p>
            
            <h3>ğŸ”‘ Temel Kavramlar</h3>
            
            <div class="example">
                <div class="example-title">Query (Q) - Sorgu</div>
                <p>
                    <strong>"Neyi arÄ±yorum?"</strong> sorusunun cevabÄ±dÄ±r. Her token, Query vektÃ¶rÃ¼ 
                    aracÄ±lÄ±ÄŸÄ±yla diÄŸer token'lardan ne tÃ¼r bilgi istediÄŸini belirtir.
                </p>
                <p>
                    <strong>Ã–rnek:</strong> "Kedi mat Ã¼zerinde oturdu" cÃ¼mlesinde "oturdu" kelimesi, 
                    Query'si aracÄ±lÄ±ÄŸÄ±yla "kim oturdu?" ve "nerede oturdu?" gibi sorulara cevap arar.
                </p>
            </div>
            
            <div class="example">
                <div class="example-title">Key (K) - Anahtar</div>
                <p>
                    <strong>"Ben neyim?"</strong> sorusunun cevabÄ±dÄ±r. Her token, Key vektÃ¶rÃ¼ ile 
                    kendini tanÄ±tÄ±r ve diÄŸer token'larÄ±n sorgularÄ±na cevap verir.
                </p>
                <p>
                    <strong>Ã–rnek:</strong> "Kedi" kelimesi, Key'i ile "ben bir Ã¶zneyim, bir hayvanyÄ±m" 
                    bilgisini taÅŸÄ±r.
                </p>
            </div>
            
            <div class="example">
                <div class="example-title">Value (V) - DeÄŸer</div>
                <p>
                    <strong>"Ne bilgi taÅŸÄ±yorum?"</strong> sorusunun cevabÄ±dÄ±r. Attention hesaplandÄ±ktan 
                    sonra aktarÄ±lacak gerÃ§ek bilgiyi iÃ§erir.
                </p>
                <p>
                    <strong>Ã–rnek:</strong> "Kedi" kelimesinin Value vektÃ¶rÃ¼, semantik anlamÄ±, gramatikal 
                    rolÃ¼ gibi zengin bilgileri kodlar.
                </p>
            </div>
            
            <div class="tip">
                <strong>ğŸ’¡ Ä°pucu:</strong> Q, K, V'yi bir kÃ¼tÃ¼phane sistemi olarak dÃ¼ÅŸÃ¼nÃ¼n:
                <ul>
                    <li><strong>Query:</strong> AradÄ±ÄŸÄ±nÄ±z konu (Ã¶rn: "kediler hakkÄ±nda kitap")</li>
                    <li><strong>Key:</strong> KitaplarÄ±n katalog bilgileri</li>
                    <li><strong>Value:</strong> KitaplarÄ±n iÃ§erikleri</li>
                </ul>
                Attention mekanizmasÄ±, query'nize en uygun kitaplarÄ± bulur ve iÃ§eriklerini size sunar!
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 2: NasÄ±l Ã‡alÄ±ÅŸÄ±r -->
        <div class="section" id="nasil-calisir">
            <h2>2. âš™ï¸ Self-Attention NasÄ±l Ã‡alÄ±ÅŸÄ±r?</h2>
            
            <h3>AdÄ±m AdÄ±m Hesaplama</h3>
            
            <div class="formula">
                <strong>AdÄ±m 1: Q, K, V Hesaplama</strong><br><br>
                Q = X Ã— W<sub>q</sub><br>
                K = X Ã— W<sub>k</sub><br>
                V = X Ã— W<sub>v</sub><br><br>
                <em>X: GiriÅŸ vektÃ¶rleri (token embedding'leri)</em><br>
                <em>W<sub>q</sub>, W<sub>k</sub>, W<sub>v</sub>: Ã–ÄŸrenilebilir aÄŸÄ±rlÄ±k matrisleri</em>
            </div>
            
            <div class="formula">
                <strong>AdÄ±m 2: Attention SkorlarÄ±</strong><br><br>
                Scores = (Q Ã— K<sup>T</sup>) / âˆšd<sub>k</sub><br><br>
                <em>d<sub>k</sub>: Key vektÃ¶rlerinin boyutu</em><br>
                <em>âˆšd<sub>k</sub>: Scaling faktÃ¶rÃ¼ (gradient'leri stabilize eder)</em>
            </div>
            
            <div class="formula">
                <strong>AdÄ±m 3: Softmax</strong><br><br>
                Attention_Weights = softmax(Scores)<br><br>
                <em>Softmax, skorlarÄ± 0-1 arasÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r</em><br>
                <em>Her satÄ±rÄ±n toplamÄ± 1'dir</em>
            </div>
            
            <div class="formula">
                <strong>AdÄ±m 4: Weighted Sum</strong><br><br>
                Output = Attention_Weights Ã— V<br><br>
                <em>Her token iÃ§in aÄŸÄ±rlÄ±klÄ± ortalama alÄ±nÄ±r</em><br>
                <em>SonuÃ§: BaÄŸlamsal bilgilerle zenginleÅŸtirilmiÅŸ vektÃ¶rler</em>
            </div>
            
            <h3>ğŸ¨ Multi-Head Attention</h3>
            <p>
                Tek bir attention mekanizmasÄ± yerine birden fazla "head" (kafa) kullanÄ±lÄ±r. 
                Her head farklÄ± bir iliÅŸki tÃ¼rÃ¼nÃ¼ Ã¶ÄŸrenebilir:
            </p>
            
            <ul>
                <li><strong>Head 1:</strong> SÃ¶zdizimsel iliÅŸkiler (Ã¶zne-yÃ¼klem)</li>
                <li><strong>Head 2:</strong> Anlamsal iliÅŸkiler (eÅŸanlamlÄ± kelimeler)</li>
                <li><strong>Head 3:</strong> Uzun mesafe baÄŸÄ±mlÄ±lÄ±klar</li>
                <li><strong>Head 4:</strong> KÄ±sa mesafe baÄŸÄ±mlÄ±lÄ±klar</li>
            </ul>
            
            <div class="formula">
                <strong>Multi-Head Attention FormÃ¼lÃ¼</strong><br><br>
                MultiHead(Q, K, V) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>) Ã— W<sub>O</sub><br><br>
                head<sub>i</sub> = Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 3: KullanÄ±m -->
        <div class="section" id="kullanim">
            <h2>3. ğŸš€ Program NasÄ±l KullanÄ±lÄ±r?</h2>
            
            <h3>BaÅŸlangÄ±Ã§ AdÄ±mlarÄ±</h3>
            
            <ol>
                <li>
                    <strong>Veri SeÃ§imi:</strong>
                    <ul>
                        <li>Ã–rnek veri setlerinden birini seÃ§in veya kendi verinizi girin</li>
                        <li>Her satÄ±r bir token'Ä± temsil eder</li>
                        <li>Minimum 3-4 token Ã¶nerilir</li>
                    </ul>
                </li>
                
                <li>
                    <strong>Parametre Ayarlama:</strong>
                    <ul>
                        <li><code>d_model</code>: Embedding boyutu (32-512 arasÄ±)</li>
                        <li><code>num_heads</code>: Head sayÄ±sÄ± (1-16 arasÄ±)</li>
                        <li><code>dropout</code>: Dropout oranÄ± (0.0-0.5 arasÄ±)</li>
                        <li><code>learning_rate</code>: Ã–ÄŸrenme hÄ±zÄ± (0.0001-0.01 arasÄ±)</li>
                    </ul>
                </li>
                
                <li>
                    <strong>EÄŸitim AyarlarÄ±:</strong>
                    <ul>
                        <li><code>Epoch SayÄ±sÄ±</code>: KaÃ§ kez eÄŸitim yapÄ±lacaÄŸÄ± (Ã¶nerilen: 50-200)</li>
                        <li><code>Batch Size</code>: Batch bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (Ã¶nerilen: 4-16)</li>
                    </ul>
                </li>
                
                <li>
                    <strong>EÄŸitimi BaÅŸlatma:</strong>
                    <ul>
                        <li>"ğŸš€ EÄŸitimi BaÅŸlat" butonuna tÄ±klayÄ±n</li>
                        <li>Ä°lerleme Ã§ubuÄŸunu takip edin</li>
                        <li>EÄŸitim tamamlandÄ±ÄŸÄ±nda gÃ¶rselleÅŸtirmeler otomatik gÃ¼ncellenir</li>
                    </ul>
                </li>
            </ol>
            
            <div class="example">
                <div class="example-title">ğŸ“ Ã–rnek Ã‡alÄ±ÅŸma AkÄ±ÅŸÄ±</div>
                <ol>
                    <li>Ã–rnek: "Kelime Dizisi" seÃ§in</li>
                    <li>Parametreler: d_model=64, num_heads=4, dropout=0.1, lr=0.001</li>
                    <li>EÄŸitim: epochs=50, batch_size=8</li>
                    <li>EÄŸitimi baÅŸlatÄ±n ve sonuÃ§larÄ± gÃ¶zlemleyin</li>
                    <li>num_heads'i 8'e Ã§Ä±karÄ±n ve farkÄ± gÃ¶zlemleyin</li>
                    <li>d_model'i 128'e Ã§Ä±karÄ±n ve yeniden eÄŸitin</li>
                </ol>
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 4: Parametreler -->
        <div class="section" id="parametreler">
            <h2>4. âš™ï¸ Parametreler ve Etkileri</h2>
            
            <table>
                <thead>
                    <tr>
                        <th>Parametre</th>
                        <th>AÃ§Ä±klama</th>
                        <th>Ã–nerilen DeÄŸerler</th>
                        <th>Etkisi</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>d_model</strong></td>
                        <td>Embedding vektÃ¶rlerinin boyutu</td>
                        <td>64, 128, 256, 512</td>
                        <td>
                            â†‘ Daha zengin temsil, daha fazla bellek<br>
                            â†“ Daha hÄ±zlÄ±, daha basit temsil
                        </td>
                    </tr>
                    <tr>
                        <td><strong>num_heads</strong></td>
                        <td>Attention head sayÄ±sÄ±</td>
                        <td>4, 8, 12, 16</td>
                        <td>
                            â†‘ Daha Ã§eÅŸitli iliÅŸkiler, daha fazla hesaplama<br>
                            â†“ Daha hÄ±zlÄ±, daha basit iliÅŸkiler<br>
                            <em>Not: d_model'e tam bÃ¶lÃ¼nmeli!</em>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>dropout</strong></td>
                        <td>Regularization iÃ§in dropout oranÄ±</td>
                        <td>0.1, 0.2, 0.3</td>
                        <td>
                            â†‘ Daha az overfitting, daha genel model<br>
                            â†“ Daha fazla kapasitÃ©, overfitting riski<br>
                            <em>0.0 = dropout yok</em>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>learning_rate</strong></td>
                        <td>Optimizer Ã¶ÄŸrenme hÄ±zÄ±</td>
                        <td>0.0001 - 0.01</td>
                        <td>
                            â†‘ Daha hÄ±zlÄ± Ã¶ÄŸrenme, kararsÄ±zlÄ±k riski<br>
                            â†“ Daha stabil, yavaÅŸ Ã¶ÄŸrenme
                        </td>
                    </tr>
                    <tr>
                        <td><strong>epochs</strong></td>
                        <td>EÄŸitim dÃ¶ngÃ¼sÃ¼ sayÄ±sÄ±</td>
                        <td>50 - 200</td>
                        <td>
                            â†‘ Daha iyi Ã¶ÄŸrenme, overfitting riski<br>
                            â†“ Daha hÄ±zlÄ±, eksik Ã¶ÄŸrenme riski
                        </td>
                    </tr>
                    <tr>
                        <td><strong>batch_size</strong></td>
                        <td>Her iterasyondaki Ã¶rnek sayÄ±sÄ±</td>
                        <td>4, 8, 16</td>
                        <td>
                            â†‘ Daha stabil gradientler, daha fazla bellek<br>
                            â†“ Daha gÃ¼rÃ¼ltÃ¼lÃ¼ gradientler, dÃ¼ÅŸÃ¼k bellek
                        </td>
                    </tr>
                </tbody>
            </table>
            
            <div class="warning">
                <strong>âš ï¸ Dikkat:</strong>
                <ul>
                    <li><code>num_heads</code> parametresi <code>d_model</code>'e tam bÃ¶lÃ¼nmelidir!</li>
                    <li>Ã‡ok yÃ¼ksek <code>learning_rate</code> eÄŸitimi kararsÄ±z yapabilir</li>
                    <li>Ã‡ok dÃ¼ÅŸÃ¼k <code>dropout</code> overfitting'e neden olabilir</li>
                    <li>Ã‡ok fazla <code>epoch</code> eÄŸitim sÃ¼resini uzatÄ±r</li>
                </ul>
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 5: Ã–rnekler -->
        <div class="section" id="ornekler">
            <h2>5. ğŸ“š Pratik Ã–rnekler</h2>
            
            <div class="example">
                <div class="example-title">Ã–rnek 1: Basit CÃ¼mle Analizi</div>
                <p><strong>Veri:</strong></p>
                <code>
                    Kedi<br>
                    Mat<br>
                    Ãœzerinde<br>
                    Oturdu
                </code>
                <p><strong>Parametreler:</strong></p>
                <ul>
                    <li>d_model: 64</li>
                    <li>num_heads: 4</li>
                    <li>epochs: 50</li>
                </ul>
                <p><strong>GÃ¶zlem:</strong> "Oturdu" kelimesinin "Kedi" ve "Mat" ile gÃ¼Ã§lÃ¼ attention baÄŸlantÄ±larÄ± oluÅŸtuÄŸunu gÃ¶receksiniz.</p>
            </div>
            
            <div class="example">
                <div class="example-title">Ã–rnek 2: Zaman Serisi</div>
                <p><strong>Veri:</strong></p>
                <code>
                    Pazartesi<br>
                    SalÄ±<br>
                    Ã‡arÅŸamba<br>
                    PerÅŸembe<br>
                    Cuma
                </code>
                <p><strong>Parametreler:</strong></p>
                <ul>
                    <li>d_model: 128</li>
                    <li>num_heads: 8</li>
                    <li>epochs: 100</li>
                </ul>
                <p><strong>GÃ¶zlem:</strong> ArdÄ±ÅŸÄ±k gÃ¼nlerin birbirleriyle gÃ¼Ã§lÃ¼ attention iliÅŸkisi kurduÄŸunu gÃ¶receksiniz.</p>
            </div>
            
            <div class="example">
                <div class="example-title">Ã–rnek 3: Parametre KarÅŸÄ±laÅŸtÄ±rmasÄ±</div>
                <p><strong>Deney:</strong> AynÄ± veriyle farklÄ± head sayÄ±larÄ±nÄ± deneyin</p>
                <ol>
                    <li>Ä°lk eÄŸitim: num_heads=2, modeli kaydedin</li>
                    <li>Ä°kinci eÄŸitim: num_heads=8, modeli kaydedin</li>
                    <li>Attention map'leri karÅŸÄ±laÅŸtÄ±rÄ±n</li>
                    <li>Daha fazla head'in daha detaylÄ± iliÅŸkiler Ã¶ÄŸrendiÄŸini gÃ¶zlemleyin</li>
                </ol>
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 6: GÃ¶rselleÅŸtirme -->
        <div class="section" id="gorsellestirme">
            <h2>6. ğŸ“Š GÃ¶rselleÅŸtirmeleri Anlama</h2>
            
            <h3>ğŸ” Attention Map (Dikkat HaritasÄ±)</h3>
            <div class="image-placeholder">
                <img src="outputs/attention_map.png" alt="Attention Map Ã–rneÄŸi" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.3);">
            </div>
            <ul>
                <li><strong>SatÄ±rlar:</strong> Query token'larÄ± (soran)</li>
                <li><strong>SÃ¼tunlar:</strong> Key token'larÄ± (cevap veren)</li>
                <li><strong>Renk YoÄŸunluÄŸu:</strong> Attention aÄŸÄ±rlÄ±ÄŸÄ± (0-1 arasÄ±)</li>
                <li><strong>Parlak HÃ¼creler:</strong> GÃ¼Ã§lÃ¼ iliÅŸki</li>
                <li><strong>Koyu HÃ¼creler:</strong> ZayÄ±f iliÅŸki</li>
            </ul>
            
            <div class="tip">
                <strong>ğŸ’¡ NasÄ±l Okunur:</strong>
                <ul>
                    <li>Bir satÄ±ra bakÄ±n (Ã¶rn: "Oturdu")</li>
                    <li>En parlak hÃ¼creleri bulun</li>
                    <li>Bu hÃ¼creler, "Oturdu"nun en Ã§ok dikkat ettiÄŸi kelimeleri gÃ¶sterir</li>
                    <li>Tipik olarak Ã¶zne ve nesne kelimeleri parlak olacaktÄ±r</li>
                </ul>
            </div>
            
            <h3>ğŸ“Š Q, K, V Matrisleri</h3>
            <div class="image-placeholder">
                <img src="outputs/qkv_matrices.png" alt="Q, K, V Matrisleri Ã–rneÄŸi" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.3);">
            </div>
            <ul>
                <li><strong>Her satÄ±r:</strong> Bir token'Ä±n vektÃ¶r temsili</li>
                <li><strong>Renkler:</strong> VektÃ¶r deÄŸerleri</li>
                <li><strong>KÄ±rmÄ±zÄ±:</strong> Pozitif deÄŸerler</li>
                <li><strong>Mavi:</strong> Negatif deÄŸerler</li>
                <li><strong>SarÄ±/Beyaz:</strong> SÄ±fÄ±ra yakÄ±n deÄŸerler</li>
            </ul>
            
            <h3>ğŸ“ˆ EÄŸitim GrafiÄŸi</h3>
            <div class="image-placeholder">
                <img src="outputs/training_history.png" alt="EÄŸitim GrafiÄŸi Ã–rneÄŸi" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.3);">
            </div>
            <ul>
                <li><strong>X Ekseni:</strong> Epoch sayÄ±sÄ±</li>
                <li><strong>Y Ekseni:</strong> Loss (kayÄ±p) deÄŸeri</li>
                <li><strong>Ä°deal Durum:</strong> SÃ¼rekli azalan eÄŸri</li>
                <li><strong>Plato:</strong> Ã–ÄŸrenmenin durduÄŸu bÃ¶lgeler</li>
                <li><strong>ArtÄ±ÅŸ:</strong> Overfitting sinyali olabilir</li>
            </ul>
            
            <div class="warning">
                <strong>âš ï¸ Dikkat Edilmesi Gerekenler:</strong>
                <ul>
                    <li>Loss Ã§ok hÄ±zlÄ± dÃ¼ÅŸÃ¼yorsa: learning_rate Ã§ok yÃ¼ksek olabilir</li>
                    <li>Loss hiÃ§ dÃ¼ÅŸmÃ¼yorsa: learning_rate Ã§ok dÃ¼ÅŸÃ¼k olabilir</li>
                    <li>Loss aÅŸÄ±rÄ± dalgalÄ±ysa: batch_size Ã§ok kÃ¼Ã§Ã¼k olabilir</li>
                    <li>Loss artmaya baÅŸladÄ±ysa: overfitting baÅŸlÄ±yor olabilir</li>
                </ul>
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 7: Model YÃ¶netimi -->
        <div class="section" id="model-yonetimi">
            <h2>7. ğŸ’¾ Model YÃ¶netimi</h2>
            
            <h3>Model Kaydetme</h3>
            <ol>
                <li>EÄŸitim tamamlandÄ±ktan sonra "Model AdÄ±" alanÄ±na anlamlÄ± bir isim girin</li>
                <li>"ğŸ’¾ Modeli Kaydet" butonuna tÄ±klayÄ±n</li>
                <li>Model, tÃ¼m parametreleri ve aÄŸÄ±rlÄ±klarÄ±yla birlikte kaydedilir</li>
                <li>Otomatik olarak zaman damgasÄ± eklenir</li>
            </ol>
            
            <div class="example">
                <div class="example-title">Model KayÄ±t Ã–rneÄŸi</div>
                <code>
                    Model AdÄ±: kelime_dizisi_model<br>
                    Kaydedilen: kelime_dizisi_model_20250102_143052<br>
                    Konum: models/kelime_dizisi_model_20250102_143052/
                </code>
            </div>
            
            <h3>Model YÃ¼kleme</h3>
            <ol>
                <li>"ğŸ“‚ Model YÃ¼kle" butonuna tÄ±klayÄ±n</li>
                <li>AÃ§Ä±lan listeden yÃ¼klemek istediÄŸiniz modeli seÃ§in</li>
                <li>Model tÃ¼m ayarlarÄ±yla birlikte yÃ¼klenir</li>
                <li>YÃ¼klenen modelle hemen deney yapabilirsiniz</li>
            </ol>
            
            <h3>Model Dosya YapÄ±sÄ±</h3>
            <div class="formula">
                models/<br>
                â””â”€â”€ model_adÄ±_20250102_143052/<br>
                    â”œâ”€â”€ model_weights.pth      (aÄŸÄ±rlÄ±klar)<br>
                    â”œâ”€â”€ full_model.pth         (tam model)<br>
                    â”œâ”€â”€ config.json            (konfigÃ¼rasyon)<br>
                    â””â”€â”€ model_info.json        (meta bilgiler)
            </div>
            
            <div class="tip">
                <strong>ğŸ’¡ Ä°puÃ§larÄ±:</strong>
                <ul>
                    <li>FarklÄ± parametre kombinasyonlarÄ± iÃ§in farklÄ± modeller kaydedin</li>
                    <li>Model adlarÄ±nÄ± aÃ§Ä±klayÄ±cÄ± seÃ§in (Ã¶rn: "yuksek_head_model", "dusuk_dropout_model")</li>
                    <li>Ã–nemli modelleri dÃ¼zenli olarak yedekleyin</li>
                    <li>Model karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in birden fazla versiyonu saklayÄ±n</li>
                </ul>
            </div>
        </div>
        
        <!-- BÃ¶lÃ¼m 8: Ä°puÃ§larÄ± -->
        <div class="section" id="ipuclari">
            <h2>8. ğŸ’¡ Ä°puÃ§larÄ± ve En Ä°yi Pratikler</h2>
            
            <h3>ğŸ¯ BaÅŸlangÄ±Ã§ Ä°Ã§in</h3>
            <ul>
                <li>KÃ¼Ã§Ã¼k veri setleriyle baÅŸlayÄ±n (4-6 token)</li>
                <li>Basit parametrelerle baÅŸlayÄ±n (d_model=64, num_heads=4)</li>
                <li>Ä°lk eÄŸitimlerde dÃ¼ÅŸÃ¼k epoch kullanÄ±n (20-50)</li>
                <li>Her deÄŸiÅŸiklikten sonra sonuÃ§larÄ± dikkatle gÃ¶zlemleyin</li>
            </ul>
            
            <h3>ğŸ”¬ Deneyler Ä°Ã§in</h3>
            <ul>
                <li><strong>Tek DeÄŸiÅŸken KuralÄ±:</strong> Her seferinde sadece bir parametreyi deÄŸiÅŸtirin</li>
                <li><strong>KayÄ±t Tutun:</strong> Her deneyden sonra modeli farklÄ± bir isimle kaydedin</li>
                <li><strong>KarÅŸÄ±laÅŸtÄ±rÄ±n:</strong> FarklÄ± modellerin attention map'lerini karÅŸÄ±laÅŸtÄ±rÄ±n</li>
                <li><strong>Extremeleri Test Edin:</strong> Ã‡ok dÃ¼ÅŸÃ¼k ve Ã§ok yÃ¼ksek deÄŸerleri deneyin</li>
            </ul>
            
            <h3>ğŸš€ Performans Ä°yileÅŸtirme</h3>
            <ul>
                <li>GPU varsa otomatik olarak kullanÄ±lÄ±r</li>
                <li>BÃ¼yÃ¼k d_model deÄŸerleri daha fazla bellek gerektirir</li>
                <li>Batch size'Ä± belleÄŸe gÃ¶re ayarlayÄ±n</li>
                <li>Ã‡ok fazla epoch kullanmaktan kaÃ§Ä±nÄ±n</li>
            </ul>
            
            <h3>ğŸ“– Ã–ÄŸrenme Stratejileri</h3>
            
            <div class="example">
                <div class="example-title">Ã–ÄŸrenme Yolu Ã–nerisi</div>
                <ol>
                    <li><strong>1. Hafta:</strong> Temel kavramlarÄ± anlayÄ±n (Q, K, V nedir?)</li>
                    <li><strong>2. Hafta:</strong> Basit Ã¶rneklerle attention map'leri inceleyin</li>
                    <li><strong>3. Hafta:</strong> Parametrelerin etkilerini deneyin</li>
                    <li><strong>4. Hafta:</strong> Kendi verilerinizle deneyler yapÄ±n</li>
                    <li><strong>5. Hafta:</strong> Multi-head attention'Ä±n avantajlarÄ±nÄ± keÅŸfedin</li>
                </ol>
            </div>
            
            <h3>ğŸ› Sorun Giderme</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Sorun</th>
                        <th>OlasÄ± Sebep</th>
                        <th>Ã‡Ã¶zÃ¼m</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Loss dÃ¼ÅŸmÃ¼yor</td>
                        <td>Learning rate Ã§ok dÃ¼ÅŸÃ¼k</td>
                        <td>Learning rate'i artÄ±rÄ±n (Ã¶rn: 0.001)</td>
                    </tr>
                    <tr>
                        <td>Loss patladÄ± (NaN)</td>
                        <td>Learning rate Ã§ok yÃ¼ksek</td>
                        <td>Learning rate'i dÃ¼ÅŸÃ¼rÃ¼n (Ã¶rn: 0.0001)</td>
                    </tr>
                    <tr>
                        <td>Hata: d_model bÃ¶lÃ¼nmÃ¼yor</td>
                        <td>num_heads uyumsuz</td>
                        <td>d_model'i num_heads'e tam bÃ¶lÃ¼nebilir yapÄ±n</td>
                    </tr>
                    <tr>
                        <td>Ã‡ok yavaÅŸ</td>
                        <td>Parametreler Ã§ok bÃ¼yÃ¼k</td>
                        <td>d_model veya num_heads'i azaltÄ±n</td>
                    </tr>
                    <tr>
                        <td>Bellek hatasÄ±</td>
                        <td>Batch size Ã§ok bÃ¼yÃ¼k</td>
                        <td>Batch size'Ä± kÃ¼Ã§Ã¼ltÃ¼n (Ã¶rn: 4)</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="tip">
                <strong>ğŸ“ Ä°leri Seviye Ä°puÃ§larÄ±:</strong>
                <ul>
                    <li>Positional Encoding'in etkisini anlamak iÃ§in farklÄ± dizilimler deneyin</li>
                    <li>Multi-head attention'da her head'in farklÄ± paternler Ã¶ÄŸrendiÄŸini gÃ¶zlemleyin</li>
                    <li>Uzun diziler iÃ§in attention pattern'lerinin nasÄ±l deÄŸiÅŸtiÄŸini inceleyin</li>
                    <li>Layer Normalization'Ä±n etkisini araÅŸtÄ±rÄ±n</li>
                </ul>
            </div>
            
            <h3>ğŸ“š Ek Kaynaklar</h3>
            <ul>
                <li><strong>Paper:</strong> "Attention Is All You Need" (Vaswani et al., 2017)</li>
                <li><strong>Blog:</strong> Jay Alammar's "The Illustrated Transformer"</li>
                <li><strong>Video:</strong> 3Blue1Brown - Deep Learning serisi</li>
                <li><strong>Kurs:</strong> Stanford CS224N - Natural Language Processing</li>
            </ul>
        </div>
        
        <!-- Footer -->
        <div style="text-align: center; margin-top: 50px; padding-top: 20px; border-top: 2px solid #61afef;">
            <p style="color: #98c379; font-size: 1.2em;">
                <strong>ğŸ“ Ä°yi Ã–ÄŸrenmeler!</strong>
            </p>
            <p style="color: #e0e0e0; margin-top: 10px;">
                Self-Attention mekanizmasÄ±nÄ± anlamak, modern yapay zeka sistemlerini 
                anlamanÄ±n temelidir. Bol bol deney yapÄ±n ve keÅŸfedin!
            </p>
        </div>
    </div>
</body>
</html>
