"""
Deep Learning EÄŸitim ProgramÄ± - Otomatik KÃ¼tÃ¼phane YÃ¼kleyici ile
HazÄ±rlayan: Dr. Mustafa AFYONLUOÄLU - EylÃ¼l 2025 (https://afyonluoglu.org/)

Bu program gerekli kÃ¼tÃ¼phaneleri otomatik olarak kontrol eder ve eksik olanlarÄ± yÃ¼kler.
Desteklenen kÃ¼tÃ¼phaneler:
- numpy: SayÄ±sal hesaplamalar iÃ§in
- pandas: Veri analizi iÃ§in
- tensorflow: Derin Ã¶ÄŸrenme modelleri iÃ§in
- matplotlib: Grafik Ã§izimleri iÃ§in
- scikit-learn: Makine Ã¶ÄŸrenmesi yardÄ±mcÄ±larÄ± iÃ§in
- pygame: Sistem bilgileri iÃ§in (opsiyonel)

Program Ã§alÄ±ÅŸmadan Ã¶nce tÃ¼m gerekli kÃ¼tÃ¼phaneleri kontrol edip eksikleri otomatik yÃ¼kler.
"""

import os
import subprocess
import sys
from stat import FILE_ATTRIBUTE_ARCHIVE
from time import time

# KÃ¼tÃ¼phane yÃ¼kleme fonksiyonu
def install_and_import(package_name, import_name=None):
    """
    KÃ¼tÃ¼phane yÃ¼klÃ¼ deÄŸilse otomatik olarak yÃ¼kler
    Args:
        package_name (str): pip ile yÃ¼klenecek paket adÄ±
        import_name (str): import ifadesinde kullanÄ±lacak modÃ¼l adÄ±
    """
    if import_name is None:
        import_name = package_name
    
    try:
        __import__(import_name)
        print(f"âœ… {package_name} kÃ¼tÃ¼phanesi zaten yÃ¼klÃ¼")
        return True
    except ImportError:
        print(f"âš ï¸  {package_name} kÃ¼tÃ¼phanesi bulunamadÄ±. YÃ¼kleniyor...")
        try:
            # pip gÃ¼ncellemesi ve yÃ¼kleme
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip"], 
                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            subprocess.check_call([sys.executable, "-m", "pip", "install", package_name])
            print(f"âœ… {package_name} baÅŸarÄ±yla yÃ¼klendi!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ {package_name} yÃ¼klenirken hata oluÅŸtu: {e}")
            return False
        except Exception as e:
            print(f"âŒ Beklenmedik hata: {e}")
            return False

# Gerekli kÃ¼tÃ¼phaneleri kontrol et ve yÃ¼kle
required_packages = [
    ("numpy", "numpy"),
    ("pandas", "pandas"), 
    ("tensorflow", "tensorflow"),
    ("matplotlib", "matplotlib"),
    ("scikit-learn", "sklearn"),
    ("pygame", "pygame")
]

print("ğŸ“¦ Gerekli kÃ¼tÃ¼phaneler kontrol ediliyor...")
print("-" * 50)

failed_packages = []
for package, import_name in required_packages:
    success = install_and_import(package, import_name)
    if not success:
        failed_packages.append(package)

if failed_packages:
    print(f"\nâŒ Åu kÃ¼tÃ¼phaneler yÃ¼klenemedi: {', '.join(failed_packages)}")
    print("LÃ¼tfen manuel olarak yÃ¼klemeyi deneyiniz:")
    for pkg in failed_packages:
        print(f"   pip install {pkg}")
    print("\nProgram devam edecek ancak bazÄ± Ã¶zellikler Ã§alÄ±ÅŸmayabilir.")
    input("Devam etmek iÃ§in ENTER tuÅŸuna basÄ±nÄ±z...")
else:
    print("ğŸš€ TÃ¼m kÃ¼tÃ¼phaneler hazÄ±r! Program baÅŸlatÄ±lÄ±yor...")
    
print("-" * 50, "\n")

# KÃ¼tÃ¼phaneleri import et
try:
    from pygame import ver
    os.environ["PYGAME_HIDE_SUPPORT_PROMPT"] = "1"  # pygame mesajlarÄ±nÄ± gizle
except ImportError:
    pass

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'        # tensorflow uyarÄ±larÄ±nÄ± gizle

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam, Ftrl
from tensorflow.keras.callbacks import EarlyStopping
import time

# HazÄ±rlayan: Dr. Mustafa AFYONLUOÄLU - EylÃ¼l 2025  (https://afyonluoglu.org/)


CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# ############################### California Housing veriseti ############################### 
# Seti internetten indirip CSV olarak kaydetmek iÃ§in aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:
# from sklearn.datasets import fetch_california_housing
# print("ğŸ“¥  California Housing veriseti indiriliyor...")
# data = fetch_california_housing()
# df = pd.DataFrame(data.data, columns=data.feature_names)
# df["MedHouseVal"]=data.target

# df.to_csv(os.path.join(CURRENT_DIR, "california_housing.csv"), index=False, encoding="utf-8")
# print("California Housing veriseti kaydedildi.")
# ############################### California Housing veriseti ############################### 

#  pip install scikit-learn
#  pip install tensorflow

NEURON_COUNT = 10
ACTIVATION = 'relu'
#############  DiÄŸer activation tÃ¼rleri:
# 'relu' (Rectified Linear Unit)
# 'sigmoid'
# 'tanh'
# 'softmax'
# 'softplus'
# 'softsign'
# 'selu'
# 'elu'
# 'exponential'
# 'linear'
EPOCHS = 130
HIDDEN_LAYERS = 3
OPTIMIZER = 'adam'  # 'adam', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'ftrl', 'nadam'
LOSS_ALGORITHM = 'sparse_categorical_crossentropy'
METRICS = 'accuracy'
VALIDATION_SPLIT = 0.5
TEST_SPLIT = 0.2
LEARNING_RATE = 0.001

OPTIMIZER = OPTIMIZER.lower()
optimizer_dict = {
    'sgd': SGD,
    'rmsprop': RMSprop,
    'adagrad': Adagrad,
    'adadelta': Adadelta,
    'adam': Adam,
    'adamax': Adamax,
    'nadam': Nadam,
    'ftrl': Ftrl
}

DATAFILE_CLASSIFICATION = "knn_purchase_history.csv"
DATAFILE_REGRESSION = "california_housing.csv"

train_duration = 0
Dataset_count = 0

TERMINAL_COLOR_GREEN = "\033[92m"
TERMINAL_COLOR_RED = "\033[91m"
TERMINAL_COLOR_YELLOW = "\033[93m"
TERMINAL_COLOR_BLUE = "\033[94m"
TERMINAL_COLOR_BOLD = "\033[1m"

TERMINAL_COLOR_RESET = "\033[0m"


def draw_graphs(history):
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("âš ï¸  matplotlib kÃ¼tÃ¼phanesi bulunamadÄ±. YÃ¼kleniyor...")
        install_and_import("matplotlib", "matplotlib")
        import matplotlib.pyplot as plt
    
    # TÃ¼m grafikleri aynÄ± ekranda gÃ¶ster
    plt.figure(figsize=(15, 6))
    plt.suptitle('Model EÄŸitim SÃ¼reci', fontsize=16, fontweight='bold', color='blue')
    plt.subplots_adjust(left=0.05, right=0.98, top=0.89, bottom=0.2)

    actual_epochs = len(history.history['loss'])
    early_stopped = actual_epochs < EPOCHS
    
    # Grafiklerin altÄ±na bu eÄŸitim sÃ¼recine iliÅŸkin parametreleri yazdÄ±r
    params_text = (
        f"{pd.Timestamp.now().strftime('%d-%m-%Y  (%H:%M)')}  Process Type: {process_type} | "
        f"Train Duration: {train_duration:.2f} sec | Dataset Rec #: {Dataset_count:,}\n"
        f"Hidden Layers: {HIDDEN_LAYERS} | Neuron Count: {NEURON_COUNT} | Activation: {ACTIVATION} | "
        f"Epochs: {actual_epochs}/{EPOCHS} {'(Early Stopped)' if early_stopped else ''} | Learning Rate: {LEARNING_RATE}\n"
        f"Test Split: {TEST_SPLIT} | Validation Split: {VALIDATION_SPLIT} | "
        f"Optimizer: {OPTIMIZER} | Loss: {LOSS_ALGORITHM} | Metrics: {METRICS}\n"
        f"Early Stopping: Patience={EARLY_STOPPING_PATIENCE}, Min Delta={EARLY_STOPPING_MIN_DELTA}"
    )
    
    plt.figtext(0.01, 0.02, params_text, ha='left', va='bottom', 
                fontsize=11, color='black', 
                bbox=dict(facecolor="#A9E5FF", alpha=0.9))

    ################# Ä°lk grafik - Metrik karÅŸÄ±laÅŸtÄ±rmasÄ±
    ax = plt.subplot(1, 3, 1)
    
    if is_regression:
        # Regresyon iÃ§in MAE grafiÄŸi
        if 'mean_absolute_error' in history.history:
            plt.plot(history.history['mean_absolute_error'], label='Train MAE')
            if 'val_mean_absolute_error' in history.history:
                plt.plot(history.history['val_mean_absolute_error'], label='Val MAE')
            plt.title('Model MAE - 1')
            plt.ylabel('Mean Absolute Error')
        elif 'mae' in history.history:
            plt.plot(history.history['mae'], label='Train MAE')
            if 'val_mae' in history.history:
                plt.plot(history.history['val_mae'], label='Val MAE')
            plt.title('Model MAE - 2')
            plt.ylabel('Mean Absolute Error')

    else:
        # SÄ±nÄ±flandÄ±rma iÃ§in accuracy grafiÄŸi
        plt.plot(history.history['accuracy'], label='Train Acc')
        plt.plot(history.history['val_accuracy'], label='Val Acc')
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
    
    plt.xlabel('Epoch')
    plt.legend()

    for spine in ax.spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor("#61AFF8")

    ################# Ä°kinci grafik - DetaylÄ± metrik karÅŸÄ±laÅŸtÄ±rmasÄ±
    bx = plt.subplot(1, 3, 2)
    
    if is_regression:
        # Regresyon iÃ§in detaylÄ± MAE/MSE grafiÄŸi
        if 'mean_squared_error' in history.history:
            plt.plot(history.history['mean_squared_error'], label='Train MSE')
            if 'val_mean_squared_error' in history.history:
                plt.plot(history.history['val_mean_squared_error'], label='Val MSE')
            plt.title('Model MSE')
            plt.ylabel('Mean Squared Error')
        elif 'mse' in history.history:
            plt.plot(history.history['mse'], label='Train MSE')
            if 'val_mse' in history.history:
                plt.plot(history.history['val_mse'], label='Val MSE')
            plt.title('Model MSE')
            plt.ylabel('Mean Squared Error')
        else:
            # MSE yoksa MAE'yi tekrar Ã§iz
            if 'mean_absolute_error' in history.history:
                plt.plot(history.history['mean_absolute_error'], label='Train MAE (EÄŸitim)')
                if 'val_mean_absolute_error' in history.history:
                    plt.plot(history.history['val_mean_absolute_error'], label='Val MAE (DoÄŸrulama)')
                plt.title('MAE Comparison')
                plt.ylabel('Mean Absolute Error')
            elif 'mae' in history.history:
                plt.plot(history.history['mae'], label='Train MAE (EÄŸitim)')
                if 'val_mae' in history.history:
                    plt.plot(history.history['val_mae'], label='Val MAE (DoÄŸrulama)')
                plt.title('MAE Comparison')
                plt.ylabel('Mean Absolute Error')
    else:
        # SÄ±nÄ±flandÄ±rma iÃ§in detaylÄ± accuracy grafiÄŸi
        plt.plot(history.history['accuracy'], label='Train Accuracy (EÄŸitim)')
        plt.plot(history.history['val_accuracy'], label='Val Accuracy (DoÄŸrulama)')
        plt.title('Accuracy Comparison')
        plt.ylabel('Accuracy')
    
    plt.xlabel('Epoch')
    plt.legend()

    for spine in bx.spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor("#186B0C")

    ################# ÃœÃ§Ã¼ncÃ¼ grafik - Loss karÅŸÄ±laÅŸtÄ±rmasÄ±
    cx = plt.subplot(1, 3, 3)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    for spine in cx.spines.values():
        spine.set_linewidth(2)
        spine.set_edgecolor("#C9801B")

    plt.show()

    # EÄŸitim notlarÄ±:
    # Train â†‘ ama Val â†“       â†’ overfit.
    # Train ve Val birlikte â†‘ â†’ saÄŸlÄ±klÄ± Ã¶ÄŸrenme

print(TERMINAL_COLOR_GREEN,"="*70)
print("          DEEP LEARNING EÄÄ°TÄ°M PROGRAMI  - TensorFlow Keras")
print("="*70,TERMINAL_COLOR_RESET)

print("\n", TERMINAL_COLOR_YELLOW,"TensorFlow sÃ¼rÃ¼mÃ¼:", tf.__version__,TERMINAL_COLOR_RESET)
if tf.__version__ > "2.11":
    print(TERMINAL_COLOR_RED,"âš ï¸  TensorFlow sÃ¼rÃ¼mÃ¼nÃ¼z GPU'yu desteklemiyor. 2.11 veya altÄ± bir sÃ¼rÃ¼m yÃ¼kleyin.",TERMINAL_COLOR_RESET)
sources = tf.config.list_logical_devices()
print("\n âœ¨  KullanÄ±labilir kaynaklar (CPU/GPU):", sources)
CPU_COUNT = os.cpu_count()
print("ğŸŸ¢ CPU Ã§ekirdek sayÄ±sÄ±:", CPU_COUNT)
if not tf.config.list_physical_devices('GPU'):
    print(TERMINAL_COLOR_RED,"âš ï¸  GPU'ya eriÅŸilemedi. ",TERMINAL_COLOR_RESET)

# Veri Setinin YÃ¼klenmesi

# internetten indirmek iÃ§in:
# dataset = pd.read_csv("https://raw.githubusercontent.com/futurexskill/projects/refs/heads/main/knn-classification/purchase_history.csv")

print("\n", TERMINAL_COLOR_BLUE,"--- Veri seti seÃ§enekleri:","-"*42,TERMINAL_COLOR_RESET)
print(f"   1 - SÄ±nÄ±flandÄ±rma - Classification      ({DATAFILE_CLASSIFICATION})")
print(f"   2 - Regresyon - Regression              ({DATAFILE_REGRESSION})")
print(f"Default: {DATAFILE_CLASSIFICATION} (1)")
print("="*70)
print("SeÃ§iminize gÃ¶re model oluÅŸturulacak ve eÄŸitilecektir.")
secim = input("SeÃ§iminiz (E: exit): ")

if secim == "e" or secim == "E":
    exit()
elif secim == "2":
    FILENAME = DATAFILE_REGRESSION
else:
    FILENAME = DATAFILE_CLASSIFICATION
is_regression = FILENAME == DATAFILE_REGRESSION
CSV_File_Path = os.path.join(CURRENT_DIR, FILENAME)

if is_regression:
    LOSS_ALGORITHM = 'mean_squared_error'  # mse
    METRICS = 'mean_absolute_error'  # mae
    EPOCHS = 80
    NEURON_COUNT = 64
    VALIDATION_SPLIT = 0.2
    TEST_SPLIT = 0.2
    ACTIVATION = 'relu'
    HIDDEN_LAYERS = 2

print("ğŸ“  Veriseti yolu:", CSV_File_Path)
try:
    dataset = pd.read_csv(CSV_File_Path)
except Exception as e:
    print("âŒ  Veriseti yÃ¼klenemedi. Hata:", e)

Dataset_count = len(dataset)
process_type = "Regression" if is_regression else "Classification"
print(f"Veriseti: (Ä°ÅŸlem TÃ¼rÃ¼: {process_type})", "-"*40,"\n", dataset)
# Verisetinin ilk 5 satÄ±rÄ±nÄ± gÃ¶ster
X = dataset.iloc[:, :-1].values     # son sÃ¼tun hariÃ§ tÃ¼m kolonlar   : Ana veri seti
y = dataset.iloc[:,-1].values       # sadece en son kolon            : SonuÃ§ veri seti (train sonrasÄ±nda AI'nÄ±n tahmin etmesi beklenen sÃ¼tun)

print(f"\n ğŸ”¢  X shape: {X.shape} ({list(dataset.columns[:-1])})\n ğŸ”¢  y shape: {y.shape} ({dataset.columns[-1]})")
print("="*50)
print(dataset.describe().T)
print("="*50)

# X ve Y verilerinin train ve test olarak ayrÄ±lmasÄ± (%80 train %20 test)
try:
    from sklearn.model_selection import train_test_split
except ImportError:
    print("âš ï¸  scikit-learn kÃ¼tÃ¼phanesi bulunamadÄ±. YÃ¼kleniyor...")
    install_and_import("scikit-learn", "sklearn")
    from sklearn.model_selection import train_test_split
    
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =TEST_SPLIT, random_state=42)

# Kategorik ve sayÄ±sal sÃ¼tunlarÄ± iÅŸle
try:
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer
except ImportError:
    print("âš ï¸  scikit-learn kÃ¼tÃ¼phanesi bulunamadÄ±. YÃ¼kleniyor...")
    install_and_import("scikit-learn", "sklearn")
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer

if FILENAME == DATAFILE_CLASSIFICATION:
    # 'Gender' ve 'Product ID' sÃ¼tunlarÄ±nÄ± one-hot encode et
    # 'Age', 'Salary', 'Price' sÃ¼tunlarÄ±nÄ± Ã¶lÃ§eklendir
    # 'Customer ID' sÃ¼tununu dikkate alma (ilk sÃ¼tun)
    ct = ColumnTransformer(transformers=[
        ('onehot', OneHotEncoder(), [1, 4]),      # 'Gender' (index 1) ve 'Product ID' (index 4)
        ('scaler', StandardScaler(), [2, 3, 5])   # 'Age' (index 2), 'Salary' (index 3), 'Price' (index 5)
    ], remainder='drop') # DiÄŸer sÃ¼tunlarÄ± (Customer ID gibi) bÄ±rak
else:
    # TÃ¼m sÃ¼tunlarÄ± Ã¶lÃ§eklendir
    # ct = ColumnTransformer(transformers=[
    #     ('scaler', StandardScaler(), list(range(X.shape[1]))) # tÃ¼m sÃ¼tunlar
    # ], remainder='drop') # DiÄŸer sÃ¼tunlarÄ± bÄ±rak
    ct = StandardScaler()

# DÃ¶nÃ¼ÅŸÃ¼mleri uygula
######## fit_transform: DÃ¶nÃ¼ÅŸÃ¼m parametrelerini Ã¶ÄŸrenir ve uygular
######## transform    : Ã–nceden Ã¶ÄŸrenilen parametreleri sadece uygular
X_train_transformed = ct.fit_transform(X_train)
X_test_transformed = ct.transform(X_test)

print(f"\n ğŸ”¢  X_train_transformed shape: {X_train_transformed.shape}")

# DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ veriyi NumPy dizisine Ã§evir (eÄŸer sparse formatta ise)
if not isinstance(X_train_transformed, np.ndarray):
    X_train_transformed = X_train_transformed.toarray()
    print("ğŸ”¢  X_train_transformed dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ (sparse â†’ dense)")
if not isinstance(X_test_transformed, np.ndarray):
    X_test_transformed = X_test_transformed.toarray()
    print("ğŸ”¢  X_test_transformed dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ (sparse â†’ dense)")


# Sinir AÄŸÄ± Modelinin OluÅŸturulmasÄ±

# Rastgelelik iÃ§in tohum belirleme (aynÄ± sonuÃ§larÄ± almak iÃ§in)
np.random.seed(42)
tf.random.set_seed(42)

# modelde input katmanÄ± Ã¶zellikle belirtilmez. 
# Sistem input katmanÄ±nÄ± train serisinden Ã¶ÄŸrenir ve otomatik olarak oluÅŸturur.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=(X_train_transformed.shape[1],)))
for layer in range(HIDDEN_LAYERS):
    print(f"ğŸ”¸  {layer+1}. katman nÃ¶ron sayÄ±sÄ±: {NEURON_COUNT}")
    if is_regression and layer > 0:
        neu = NEURON_COUNT // 2
        print(f"ğŸ”¸  Regression - {layer+1}. katman nÃ¶ron sayÄ±sÄ±: {neu}")
        model.add(tf.keras.layers.Dense(neu, activation=ACTIVATION))

    model.add(tf.keras.layers.Dense(NEURON_COUNT, activation=ACTIVATION))
    model.add(tf.keras.layers.Dropout(0.2))  

if is_regression:
    # regresyon iÃ§in tek nÃ¶ronlu Ã§Ä±kÄ±ÅŸ katmanÄ±
    print("ğŸ”¸  Regression - Ã‡Ä±kÄ±ÅŸ katmanÄ± nÃ¶ron sayÄ±sÄ±: 1")
    model.add(tf.keras.layers.Dense(1))  # Default olarak: activation='linear'
else:
    model.add(tf.keras.layers.Dense(2, activation='softmax'))

print(f"\n ğŸ§   {HIDDEN_LAYERS} katmanlÄ± model oluÅŸturuldu.")

# EARLY_STOP PARAAMETERS - EXPLANATIONS
# monitor='val_loss': Validation loss'u izler (regression iÃ§in ideal)
# patience=10: 10 epoch boyunca iyileÅŸme yoksa durur
# min_delta=0.001: En az 0.001 iyileÅŸme olmalÄ±, yoksa sayÄ±lmaz
# restore_best_weights=True: DurduÄŸunda en iyi epoch'taki aÄŸÄ±rlÄ±klarÄ± kullanÄ±r

# Early Stopping parametreleri 
EARLY_STOPPING_PATIENCE = 10           # KaÃ§ epoch boyunca iyileÅŸme yoksa durduracak
EARLY_STOPPING_MIN_DELTA = 0.001       # Minimum iyileÅŸme miktarÄ±
EARLY_STOPPING_RESTORE_BEST = True     # En iyi aÄŸÄ±rlÄ±klarÄ± geri yÃ¼kle

# Modelin EÄŸitilmesi
print(f"\n ğŸ’«  Model ve EÄŸitim Bilgileri:"+
      f"\n      ğŸ”¸   Dataset                    : {FILENAME} "+
      f"\n      ğŸ”¸   Record Count               : {Dataset_count:,}   â†’  Train: {len(X_train):,}  Test: {len(X_test):,} "+
      f"\n      ğŸ”¸   Process Type               : {process_type} "+
      f"\n      ğŸ”¸   Hidden Layers              : {HIDDEN_LAYERS} "+
      f"\n      ğŸ”¸   Neuron # in each Layer     : {NEURON_COUNT} "+
      f"\n      ğŸ”¸   Activation                 : {ACTIVATION} "+
      f"\n      ğŸ”¸   Epochs                     : {EPOCHS} " +
      f"\n      ğŸ”¸   Optimization               : {OPTIMIZER} " +
      f"\n      ğŸ”¸   Loss Function              : {LOSS_ALGORITHM} " +
      f"\n      ğŸ”¸   Metric                     : {METRICS} " +
      f"\n      ğŸ”¸   Test Split                 : {TEST_SPLIT} " +
      f"\n      ğŸ”¸   Validation Split           : {VALIDATION_SPLIT} "
    )

if OPTIMIZER in optimizer_dict:
    optimizer_class = optimizer_dict[OPTIMIZER]
    try:
        optimizer_instance = optimizer_class(learning_rate=LEARNING_RATE)
        print(f"      ğŸ”¸   Learning Rate              : {LEARNING_RATE} (atanan)")
    except TypeError:
        optimizer_instance = optimizer_class()
        print("      ğŸ”¸   SeÃ§ilen optimizer learning_rate parametresini desteklemiyor, varsayÄ±lan kullanÄ±ldÄ±.")
else:
    optimizer_instance = OPTIMIZER  # string olarak bÄ±rak (Keras default davranÄ±ÅŸÄ±)
    print("      ğŸ”¸   Bilinmeyen optimizer, string olarak kullanÄ±lacak.")

# Optimizer: Learning Rate'i adaptif ÅŸekilde ayarlayan algoritmalar
model.compile(optimizer=optimizer_instance, loss=LOSS_ALGORITHM, metrics=[METRICS])

# Early Stopping callback'ini oluÅŸtur
early_stopping = EarlyStopping(
    monitor='val_loss',                                # Ä°zlenecek metrik (validation loss)
    patience=EARLY_STOPPING_PATIENCE,                  # KaÃ§ epoch sabÄ±r gÃ¶sterecek
    min_delta=EARLY_STOPPING_MIN_DELTA,                # Minimum iyileÅŸme miktarÄ±
    restore_best_weights=EARLY_STOPPING_RESTORE_BEST,  # En iyi aÄŸÄ±rlÄ±klarÄ± geri yÃ¼kle
    verbose=1                                          # DurduÄŸunda bilgi ver
)

print(f"      ğŸ”¸   Early Stopping Patience    : {EARLY_STOPPING_PATIENCE} epochs")
print(f"      ğŸ”¸   Early Stopping Min Delta   : {EARLY_STOPPING_MIN_DELTA}")
print(f"      ğŸ”¸   Early Stopping Monitor     : val_loss")

# train_test_split ile verinin bir kÄ±smÄ±nÄ± test iÃ§in ayrÄ±lÄ±r (modelin hiÃ§ gÃ¶rmediÄŸi, nihai deÄŸerlendirme iÃ§in).
# validation_split, eÄŸitim verisinin bir kÄ±smÄ±nÄ± eÄŸitim sÄ±rasÄ±nda modelin doÄŸrulama (validation) performansÄ±nÄ± izlemek iÃ§in ayÄ±rÄ±r.
print(f"\nğŸ’«  Model eÄŸitiliyor... ")

if EPOCHS <= 20:
    verboseLevel = 1
else:
    verboseLevel = 0

basla = time.time()

# Batch_size= Stochastic Gradient Descent iÃ§in alÄ±nan "mini_batches size"
history = model.fit(
    X_train_transformed, 
    y_train, 
    epochs=EPOCHS, 
    validation_split=VALIDATION_SPLIT, 
    batch_size=32,
    verbose=verboseLevel,
    callbacks=[early_stopping]  # Early stopping callback'ini ekle
) # verbose=0: epoch baÅŸÄ±na ilerleme Ã§ubuÄŸu gÃ¶sterilmez
bitis = time.time()
train_duration += (bitis - basla)
print(f"â±ï¸  EÄŸitim sÃ¼resi: {train_duration:.2f} saniye")

if is_regression:
    val_metric = history.history['val_mean_absolute_error'][-1]
    print(f"ğŸš© {HIDDEN_LAYERS} katman â†’ Val MAE: {val_metric:.5f}")
else:
    val_acc = history.history['val_accuracy'][-1]
    print(f"ğŸš© {HIDDEN_LAYERS} katman â†’ DoÄŸruluk: {val_acc:.5f}")
    
print("âœ…  Model eÄŸitildi\n")

# Modelin test veri seti Ã¼zerinde deÄŸerlendirilmesi
print("â¡ï¸  Model test verisi Ã¼zerinde deÄŸerlendiriliyor...")
loss, metric = model.evaluate(X_test_transformed, y_test)

print(f"\nğŸ”´  Test Seti KaybÄ±    : {loss}")
if is_regression:
    print(f"ğŸŸ¢  Test Seti MAE      : {metric}\n")
else:
    print(f"ğŸŸ¢  Test Seti DoÄŸruluÄŸu: {metric}\n")

model.summary()
print("_"*50,"\n")

# EÄŸitim sonrasÄ± bilgi ekleyin
actual_epochs = len(history.history['loss'])
if actual_epochs < EPOCHS:
    print(f"ğŸ›‘  Early Stopping devreye girdi! EÄŸitim {actual_epochs}. epoch'ta durduruldu.")
    print(f"    (Planlanan: {EPOCHS}, GerÃ§ekleÅŸen: {actual_epochs})")
else:
    print(f"âœ…  TÃ¼m {EPOCHS} epoch tamamlandÄ± (Early Stopping devreye girmedi)")

# Ã–rnek veriyle test et

if is_regression:
    # "California Housing" iÃ§in Ã¶rnek veri
    # Fields: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude
    print("\nâ¡ï¸  Regression: Model ile tahmin yapÄ±lÄ±yor...")
    new_data = np.array([[8.3252, 25.0, 6.0, 1.2, 600, 2.75, 39.00, -122.23]])
elif FILENAME == DATAFILE_CLASSIFICATION:
    # "knn_purchase_history.csv" iÃ§in Ã¶rnek veri
    # Fields: Customer ID, Gender, Age, Salary, Product ID, Price
    print("\nâ¡ï¸  Classification: Model ile tahmin yapÄ±lÄ±yor...")
    new_data = np.array([[1001, 'Male', 42, 50000, 'P01', 3000]])
else:
    print("âŒ  Bilinmeyen veri seti, tahmin yapÄ±lamÄ±yor.")
    exit()

new_data_transformed = ct.transform(new_data)

print(f"{new_data=}")

if not isinstance(new_data_transformed, np.ndarray):
    new_data_transformed = new_data_transformed.toarray()

prediction = model.predict(new_data_transformed)

if is_regression:
    print(f"âœ¨ Tahmin Edilen Ev FiyatÄ±: ${prediction[0][0]*100000:,.0f}")
else:
    print(f"âœ¨ Tahmin Edilen SatÄ±n Alma OlasÄ±lÄ±ÄŸÄ±: % {(prediction[0][1]*100):.2f}")

print("_"*50,"\n")

draw_graphs(history)

# input()  # grafik ekranÄ± block=false ise aÃ§Ä±k kalmmasÄ± iÃ§in